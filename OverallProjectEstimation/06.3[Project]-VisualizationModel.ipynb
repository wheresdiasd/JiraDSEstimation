{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified visualization script to correctly handle feature categories\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Set the aesthetic style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Define custom colors\n",
    "colors = [\"#4287f5\", \"#42c5f5\", \"#42f5d1\", \"#42f56f\", \n",
    "          \"#9ef542\", \"#f5ee42\", \"#f5c242\", \"#f54242\"]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_viridis\", colors)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "viz_dir = 'model_visualizations'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Load the models and data\n",
    "try:\n",
    "    # Load the datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    data_path = 'task_estimation_results_improved/data_splits.pkl'\n",
    "    \n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        \n",
    "    X_train = data.get('X_train')\n",
    "    X_test = data.get('X_test')\n",
    "    y_train = data.get('y_train')\n",
    "    y_test = data.get('y_test')\n",
    "    feature_names = data.get('feature_names')\n",
    "    \n",
    "    # Load the models\n",
    "    models = {}\n",
    "    model_files = [f for f in os.listdir('task_estimation_results_improved') if f.endswith('_model.pkl')]\n",
    "    \n",
    "    for model_file in model_files:\n",
    "        model_name = model_file.replace('_model.pkl', '')\n",
    "        with open(f'task_estimation_results_improved/{model_file}', 'rb') as f:\n",
    "            models[model_name] = pickle.load(f)\n",
    "    \n",
    "    # Find the best model\n",
    "    with open('task_estimation_results_improved/best_model.pkl', 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "        \n",
    "    # Identify best model name\n",
    "    best_model_name = None\n",
    "    for name, model in models.items():\n",
    "        if model == best_model:\n",
    "            best_model_name = name\n",
    "            break\n",
    "    \n",
    "    print(f\"Loaded models: {list(models.keys())}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading models or data: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Generate predictions for all models\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    predictions[name] = model.predict(X_test)\n",
    "\n",
    "# Calculate ensemble prediction\n",
    "ensemble_pred = np.mean([predictions[name] for name in models.keys()], axis=0)\n",
    "\n",
    "# =============== 1. Feature Importance Visualization ===============\n",
    "print(\"\\nCreating feature importance visualizations...\")\n",
    "\n",
    "# Create a function to plot feature importance\n",
    "def plot_feature_importance(model, feature_names, title, filename, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance for a model with enhanced styling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : model object\n",
    "        The trained model with feature_importances_ attribute\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    title : str\n",
    "        Plot title\n",
    "    filename : str\n",
    "        Output filename\n",
    "    top_n : int\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(f\"Model {title} doesn't have feature_importances_ attribute\")\n",
    "        return None\n",
    "    \n",
    "    # Get feature importances and sort them\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create DataFrames for visualization and analysis\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Print the top features for debugging\n",
    "    print(f\"\\nTop 10 features for {title}:\")\n",
    "    for i, (feature, importance) in enumerate(zip(importance_df['Feature'].head(10), \n",
    "                                                importance_df['Importance'].head(10))):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Analyze feature prefixes to better understand what types we have\n",
    "    feature_prefixes = set()\n",
    "    for feature in importance_df['Feature'].head(top_n):\n",
    "        parts = feature.split('__')\n",
    "        if len(parts) > 1:\n",
    "            feature_prefixes.add(parts[0])\n",
    "    \n",
    "    print(f\"Feature prefixes found in top {top_n}: {feature_prefixes}\")\n",
    "    \n",
    "    # Check if any count features are in the top features (modified to use 'count' rather than 'count_std')\n",
    "    count_features = [f for f in importance_df['Feature'].head(top_n) if 'count' in f]\n",
    "    if count_features:\n",
    "        print(f\"Found count-related features in top {top_n}: {count_features}\")\n",
    "    \n",
    "    # Select top N features\n",
    "    n_features = min(top_n, len(feature_names))\n",
    "    top_indices = indices[:n_features]\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Plot horizontal bars for better readability with feature names\n",
    "    colors = custom_cmap(np.linspace(0, 1, n_features))\n",
    "    bars = ax.barh(range(n_features), importances[top_indices], color=colors, alpha=0.8)\n",
    "    \n",
    "    # Add feature names and importance values\n",
    "    for i, bar in enumerate(bars):\n",
    "        ax.text(\n",
    "            bar.get_width() * 1.01, \n",
    "            bar.get_y() + bar.get_height()/2, \n",
    "            f\"{importances[top_indices[i]]:.3f}\", \n",
    "            va='center', \n",
    "            fontsize=10, \n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_yticks(range(n_features))\n",
    "    ax.set_yticklabels([feature_names[i] for i in top_indices], fontsize=11)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add a grid for better readability\n",
    "    ax.xaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Highlight count features if present (modified to use 'count' rather than 'count_std')\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        feature_name = feature_names[idx]\n",
    "        if 'count' in feature_name:\n",
    "            # Add a highlight or outline to the bar\n",
    "            ax.get_yticklabels()[i].set_color('darkblue')\n",
    "            ax.get_yticklabels()[i].set_fontweight('bold')\n",
    "            # Add a subtle background highlight\n",
    "            ax.axhspan(i-0.4, i+0.4, color='lightblue', alpha=0.3)\n",
    "    \n",
    "    # Add styling\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/{filename}\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Plot feature importance for all models\n",
    "importance_dfs = {}\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = plot_feature_importance(\n",
    "            model, \n",
    "            feature_names,\n",
    "            f\"Feature Importance - {name}\",\n",
    "            f\"{name}_feature_importance.png\"\n",
    "        )\n",
    "        importance_dfs[name] = importance_df\n",
    "\n",
    "# Create a special plot for the best model\n",
    "if best_model_name and hasattr(models[best_model_name], 'feature_importances_'):\n",
    "    best_importance_df = plot_feature_importance(\n",
    "        models[best_model_name],\n",
    "        feature_names,\n",
    "        f\"Feature Importance - Best Model ({best_model_name})\",\n",
    "        \"best_model_feature_importance.png\",\n",
    "        top_n=20\n",
    "    )\n",
    "    \n",
    "    # =============== 2. Feature Category Importance ===============\n",
    "    print(\"Creating feature category importance visualization...\")\n",
    "    \n",
    "    # Define feature categories based on actual prefixes in the data\n",
    "    # Modify to match actual feature patterns in the dataset\n",
    "    categories = {\n",
    "        'Issue Type': [f for f in feature_names if 'type_' in f or 'is_type_' in f],\n",
    "        'Priority': [f for f in feature_names if 'priority' in f],\n",
    "        'Project Stats': [f for f in feature_names if ('count' in f and 'count_' not in f) or 'pct_minmax__' in f],\n",
    "        'Team Size': [f for f in feature_names if 'team_size' in f],\n",
    "        'Created Time': [f for f in feature_names if 'created_' in f],\n",
    "        'Robust Stats': [f for f in feature_names if 'stat_robust' in f],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    # Assign remaining features to 'Other'\n",
    "    for feature in feature_names:\n",
    "        if not any(feature in cat_features for cat_features in categories.values()):\n",
    "            categories['Other'].append(feature)\n",
    "            \n",
    "    # Print categorized features for verification\n",
    "    print(\"\\nFeatures by category:\")\n",
    "    for category, cat_features in categories.items():\n",
    "        if cat_features:\n",
    "            print(f\"{category} ({len(cat_features)}): {', '.join(cat_features[:5])}\" + \n",
    "                  (\"...\" if len(cat_features) > 5 else \"\"))\n",
    "    \n",
    "    # Calculate importance by category\n",
    "    category_importance = {}\n",
    "    for category, cat_features in categories.items():\n",
    "        if cat_features:  # Skip empty categories\n",
    "            total_importance = sum(best_importance_df.loc[best_importance_df['Feature'].isin(cat_features), 'Importance'])\n",
    "            category_importance[category] = total_importance\n",
    "            print(f\"{category}: Total importance = {total_importance:.4f}\")\n",
    "    \n",
    "    # Sort categories by importance\n",
    "    categories_sorted = sorted(category_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    categories_names = [item[0] for item in categories_sorted]\n",
    "    categories_values = [item[1] for item in categories_sorted]\n",
    "    \n",
    "    # Create a pie chart for category importance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    \n",
    "    # Pie chart\n",
    "    wedges, texts, autotexts = ax1.pie(\n",
    "        categories_values, \n",
    "        labels=categories_names,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=custom_cmap(np.linspace(0, 1, len(categories_values))),\n",
    "        wedgeprops={'edgecolor': 'w', 'linewidth': 1},\n",
    "        textprops={'fontsize': 11, 'fontweight': 'bold'}\n",
    "    )\n",
    "    \n",
    "    # Customize pie chart\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(9)\n",
    "        autotext.set_fontweight('bold')\n",
    "    ax1.set_title('Feature Importance by Category', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Bar chart\n",
    "    bars = ax2.bar(\n",
    "        categories_names, \n",
    "        categories_values,\n",
    "        color=custom_cmap(np.linspace(0, 1, len(categories_values))),\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height * 1.01,\n",
    "            f'{height:.3f}',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "    \n",
    "    # Customize bar chart\n",
    "    ax2.set_ylabel('Total Importance', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Feature Importance by Category', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax2.set_xticklabels(categories_names, rotation=45, ha='right', fontsize=11)\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f\"{viz_dir}/feature_importance_by_category.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =============== 3. Model Performance Comparison ===============\n",
    "print(\"Creating model performance comparison visualizations...\")\n",
    "\n",
    "# Define a function to calculate metrics for all models\n",
    "def calculate_metrics(y_true, predictions_dict):\n",
    "    \"\"\"Calculate MAE, RMSE, and R² for all models.\"\"\"\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    metrics = {}\n",
    "    for name, y_pred in predictions_dict.items():\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "        \n",
    "        metrics[name] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2,\n",
    "            'Spearman': spearman_corr\n",
    "        }\n",
    "    \n",
    "    # Add ensemble metrics\n",
    "    ensemble_pred = np.mean([predictions_dict[name] for name in predictions_dict.keys()], axis=0)\n",
    "    mae = mean_absolute_error(y_true, ensemble_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, ensemble_pred))\n",
    "    r2 = r2_score(y_true, ensemble_pred)\n",
    "    spearman_corr, _ = spearmanr(y_true, ensemble_pred)\n",
    "    \n",
    "    metrics['Ensemble'] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame for the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    model_name: {metric: value for metric, value in model_metrics.items()}\n",
    "    for model_name, model_metrics in metrics.items()\n",
    "})\n",
    "\n",
    "# Transpose to make models as rows\n",
    "metrics_df = metrics_df.T\n",
    "\n",
    "# Create a comprehensive model performance comparison visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# 1. MAE Comparison\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "metrics_df.sort_values('MAE').plot(\n",
    "    y='MAE', \n",
    "    kind='bar', \n",
    "    color=custom_cmap(np.linspace(0, 1, len(metrics_df))), \n",
    "    ax=ax1,\n",
    "    alpha=0.8\n",
    ")\n",
    "ax1.set_title('Mean Absolute Error (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('MAE (Hours)', fontsize=12)\n",
    "for i, v in enumerate(metrics_df.sort_values('MAE')['MAE']):\n",
    "    ax1.text(i, v + 0.1, f'{v:.2f}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2. R² Comparison\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "metrics_df.sort_values('R²', ascending=False).plot(\n",
    "    y='R²', \n",
    "    kind='bar', \n",
    "    color=custom_cmap(np.linspace(0, 1, len(metrics_df))), \n",
    "    ax=ax2,\n",
    "    alpha=0.8\n",
    ")\n",
    "ax2.set_title('R² Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('R²', fontsize=12)\n",
    "for i, v in enumerate(metrics_df.sort_values('R²', ascending=False)['R²']):\n",
    "    ax2.text(i, max(v - 0.05, 0.01), f'{v:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 3. Comprehensive Metric Comparison\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "metrics_to_plot = ['MAE', 'RMSE', 'R²', 'Spearman']\n",
    "# Normalize metrics for better visualization\n",
    "normalized_metrics = metrics_df.copy()\n",
    "for metric in metrics_to_plot:\n",
    "    if metric in ['MAE', 'RMSE']:  # Lower is better\n",
    "        min_val = normalized_metrics[metric].min()\n",
    "        max_val = normalized_metrics[metric].max()\n",
    "        if max_val > min_val:\n",
    "            normalized_metrics[metric] = 1 - (normalized_metrics[metric] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_metrics[metric] = 1\n",
    "    else:  # Higher is better\n",
    "        min_val = normalized_metrics[metric].min()\n",
    "        max_val = normalized_metrics[metric].max()\n",
    "        if max_val > min_val:\n",
    "            normalized_metrics[metric] = (normalized_metrics[metric] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_metrics[metric] = 1\n",
    "\n",
    "normalized_metrics[metrics_to_plot].plot(\n",
    "    kind='bar', \n",
    "    ax=ax3,\n",
    "    figsize=(12, 6),\n",
    "    colormap=custom_cmap,\n",
    "    alpha=0.8\n",
    ")\n",
    "ax3.set_title('Normalized Performance Metrics (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Normalized Score', fontsize=12)\n",
    "ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "ax3.set_ylim(0, 1.1)\n",
    "ax3.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "# General title for the figure\n",
    "fig.suptitle('Model Performance Comparison', fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(f\"{viz_dir}/model_performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save the original metrics to CSV for reference\n",
    "metrics_df.to_csv(f\"{viz_dir}/model_performance_metrics.csv\")\n",
    "print(f\"Saved performance metrics to {viz_dir}/model_performance_metrics.csv\")\n",
    "\n",
    "# =============== 4. Actual vs Predicted Visualizations ===============\n",
    "print(\"Creating actual vs predicted visualizations...\")\n",
    "\n",
    "# Define a function to create enhanced actual vs predicted plots\n",
    "def enhanced_actual_vs_predicted_plot(y_true, y_pred, title, filename):\n",
    "    \"\"\"Create an enhanced actual vs predicted plot with residuals and distribution.\"\"\"\n",
    "    # Calculate additional metrics\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    spearman, _ = spearmanr(y_true, y_pred)\n",
    "    \n",
    "    # Create figure with gridspec\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    gs = GridSpec(2, 3, figure=fig)\n",
    "    \n",
    "    # 1. Actual vs Predicted Scatter Plot\n",
    "    ax1 = fig.add_subplot(gs[0, 0:2])\n",
    "    scatter = ax1.scatter(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        alpha=0.5, \n",
    "        c=np.abs(y_true - y_pred), \n",
    "        cmap='viridis', \n",
    "        edgecolors='w', \n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax1.plot([0, y_true.max()], [0, y_true.max()], 'r--', linewidth=2)\n",
    "    ax1.set_xlabel('Actual Hours', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Predicted Hours', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Actual vs Predicted Hours', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add color bar to indicate prediction error\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Absolute Error', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add metrics to the plot\n",
    "    metrics_text = (\n",
    "        f\"MAE: {mae:.2f}\\n\"\n",
    "        f\"RMSE: {rmse:.2f}\\n\"\n",
    "        f\"R²: {r2:.4f}\\n\"\n",
    "        f\"Spearman ρ: {spearman:.4f}\"\n",
    "    )\n",
    "    ax1.text(\n",
    "        0.05, 0.95, metrics_text,\n",
    "        transform=ax1.transAxes,\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "    )\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 2. Residuals Plot\n",
    "    ax2 = fig.add_subplot(gs[1, 0:2])\n",
    "    residuals = y_true - y_pred\n",
    "    scatter = ax2.scatter(\n",
    "        y_pred, \n",
    "        residuals, \n",
    "        alpha=0.5, \n",
    "        c=np.abs(residuals), \n",
    "        cmap='coolwarm', \n",
    "        edgecolors='w', \n",
    "        linewidth=0.5\n",
    "    )\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('Predicted Hours', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Residuals (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add color bar for residuals\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Absolute Residual', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Find large residuals and add annotations\n",
    "    large_residuals = np.abs(residuals) > np.percentile(np.abs(residuals), 95)\n",
    "    if np.sum(large_residuals) > 0:\n",
    "        for i, (x, y) in enumerate(zip(y_pred[large_residuals], residuals[large_residuals])):\n",
    "            # Only label a few points to avoid cluttering\n",
    "            if i % max(1, int(np.sum(large_residuals) / 5)) == 0:\n",
    "                ax2.annotate(\n",
    "                    f\"{y:.0f}\",\n",
    "                    xy=(x, y),\n",
    "                    xytext=(x+2, y),\n",
    "                    fontsize=8,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='black', alpha=0.6)\n",
    "                )\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. Error Distribution\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    sns.histplot(residuals, kde=True, ax=ax3, color='skyblue', bins=20, alpha=0.7)\n",
    "    ax3.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "    ax3.set_xlabel('Residual Error', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add stats about the error distribution\n",
    "    error_stats = (\n",
    "        f\"Mean Error: {np.mean(residuals):.2f}\\n\"\n",
    "        f\"Median Error: {np.median(residuals):.2f}\\n\"\n",
    "        f\"Std Dev: {np.std(residuals):.2f}\"\n",
    "    )\n",
    "    ax3.text(\n",
    "        0.05, 0.95, error_stats,\n",
    "        transform=ax3.transAxes,\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "    )\n",
    "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. Percent Error Analysis\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    # Calculate percent error, handling division by zero\n",
    "    percent_error = np.zeros_like(y_true, dtype=float)\n",
    "    non_zero_mask = y_true != 0\n",
    "    percent_error[non_zero_mask] = 100 * np.abs(residuals[non_zero_mask]) / y_true[non_zero_mask]\n",
    "    \n",
    "    # Plot histogram of percent errors\n",
    "    sns.histplot(percent_error, kde=False, ax=ax4, color='lightgreen', bins=20, alpha=0.7)\n",
    "    ax4.set_xlabel('Percent Error (%)', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Percent Error Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Compute percent error stats\n",
    "    percent_stats = (\n",
    "        f\"Mean %Error: {np.mean(percent_error):.2f}%\\n\"\n",
    "        f\"Median %Error: {np.median(percent_error):.2f}%\\n\"\n",
    "        f\"% within 25% error: {100*np.sum(percent_error <= 25)/len(percent_error):.1f}%\"\n",
    "    )\n",
    "    ax4.text(\n",
    "        0.05, 0.95, percent_stats,\n",
    "        transform=ax4.transAxes,\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8)\n",
    "    )\n",
    "    ax4.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # General title for the figure\n",
    "    fig.suptitle(title, fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f\"{viz_dir}/{filename}\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (1575833, 63)\n",
      "Reduced dataset shape (5%): (787916, 63)\n",
      "\n",
      "Fields to exclude that exist in the dataframe: ['fields.status.name', 'fields.project.key', 'fields.priority.name', 'fields.project.name']\n",
      "Dropping these fields from the dataframe: ['fields.status.name', 'fields.project.key', 'fields.priority.name', 'fields.project.name']\n",
      "Dataframe shape after dropping: (787916, 59)\n",
      "Missing values in target: 0\n",
      "Dataset shape after dropping missing targets: (787916, 59)\n",
      "\n",
      "Using 57 numeric features for correlation calculation\n",
      "Top 10 correlated features with target:\n",
      "count_std__total_issues                 0.862686\n",
      "remainder__team_size_combined           0.756612\n",
      "remainder__team_size_creators           0.756612\n",
      "time_power__project_duration_days       0.431717\n",
      "stat_robust__bug_ratio                  0.294738\n",
      "pct_minmax__type_bug_pct                0.264221\n",
      "avg_resolution_hours                    0.258149\n",
      "stat_robust__weighted_priority_score    0.186916\n",
      "age_days                                0.147374\n",
      "pct_minmax__priority_blocker_pct        0.146859\n",
      "Name: total_resolution_hours, dtype: float64\n",
      "\n",
      "Bottom 10 correlated features with target:\n",
      "created_year                              -0.147150\n",
      "pct_minmax__type_story_pct                -0.168730\n",
      "pct_minmax__type_epic_pct                 -0.215495\n",
      "project_id                                -0.278568\n",
      "pct_minmax__type_task_pct                 -0.284693\n",
      "stat_robust__high_to_low_priority_ratio   -0.286896\n",
      "pct_minmax__type_new_feature_pct          -0.357542\n",
      "fields.created                                  NaN\n",
      "fields.updated                                  NaN\n",
      "remainder__team_size_assignees                  NaN\n",
      "Name: total_resolution_hours, dtype: float64\n",
      "\n",
      "Features with >0.8 correlation with target: ['count_std__total_issues']\n",
      "Keeping 'count_std__total_issues' despite high correlation with target\n",
      "'count_std__total_issues' marked for exclusion\n",
      "\n",
      "Removing suspicious features: ['avg_resolution_hours', 'median_resolution_hours', 'resolution_hours', 'log_resolution_hours', 'count_std__total_issues', 'fields.status.name', 'fields.project.key', 'fields.priority.name', 'fields.project.name']\n",
      "Added 'count_std__total_issues' to features list\n",
      "Available features: 42\n",
      "Missing features: 0\n",
      "\n",
      "Checking for fields that need to be dropped:\n",
      "Field 'fields.status.name' does not exist in the dataframe\n",
      "Field 'fields.project.key' does not exist in the dataframe\n",
      "Field 'fields.priority.name' does not exist in the dataframe\n",
      "Field 'fields.project.name' does not exist in the dataframe\n",
      "\n",
      "Confirming excluded fields:\n",
      "'fields.status.name' is NOT in the feature list\n",
      "'fields.project.key' is NOT in the feature list\n",
      "'fields.priority.name' is NOT in the feature list\n",
      "'fields.project.name' is NOT in the feature list\n",
      "\n",
      "Using 42 numeric features for multicollinearity check\n",
      "\n",
      "High correlation pairs (>0.95):\n",
      "priority_id - fields.priority.id: 1.0000\n",
      "issue_type_id - fields.issuetype.id: 1.0000\n",
      "is_type_bug - type_bug: 1.0000\n",
      "is_type_task - type_task: 1.0000\n",
      "is_type_sub-task - type_sub_task: 1.0000\n",
      "remainder__team_size_combined - remainder__team_size_creators: 1.0000\n",
      "stat_robust__bug_ratio - pct_minmax__type_bug_pct: 0.9603\n",
      "\n",
      "Dropping features due to multicollinearity: ['fields.priority.id', 'fields.issuetype.id', 'type_bug', 'type_task', 'type_sub_task', 'remainder__team_size_creators', 'pct_minmax__type_bug_pct']\n",
      "Final feature count: 35\n",
      "\n",
      "Final confirmation - excluded fields:\n",
      "'fields.status.name' is NOT in the final feature list\n",
      "'fields.project.key' is NOT in the final feature list\n",
      "'fields.priority.name' is NOT in the final feature list\n",
      "'fields.project.name' is NOT in the final feature list\n",
      "\n",
      "'count_std__total_issues' is not in the final feature list\n",
      "CAUTION: 'count_std__total_issues' exists in the dataset but was excluded from final features\n",
      "\n",
      "Final verification before model training:\n",
      "Target variable: total_resolution_hours\n",
      "Number of features: 35\n",
      "Verified: Target is not being used as a feature\n",
      "\n",
      "Training original models (before hyperparameter tuning)...\n",
      "\n",
      "Training original Random_Forest...\n",
      "Original Random_Forest - MAE: 696252.80, RMSE: 1011883.18, R2: 0.9545, Spearman: 0.9303\n",
      "\n",
      "Training original Gradient_Boosting...\n",
      "Original Gradient_Boosting - MAE: 493179.56, RMSE: 793477.57, R2: 0.9721, Spearman: 0.9326\n",
      "\n",
      "Training original XGBoost...\n",
      "Original XGBoost - MAE: 540165.17, RMSE: 853762.87, R2: 0.9676, Spearman: 0.9312\n",
      "Original Ensemble - MAE: 561397.47, RMSE: 846977.97, R2: 0.9682, Spearman: 0.9325\n",
      "\n",
      "Creating plots for original Random_Forest model...\n",
      "\n",
      "Creating plots for original Gradient_Boosting model...\n",
      "\n",
      "Creating plots for original XGBoost model...\n",
      "\n",
      "Generated prediction and residual plots for original models and ensemble.\n",
      "\n",
      "Performing hyperparameter tuning...\n",
      "\n",
      "Tuning Random_Forest...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Random_Forest:\n",
      "{'n_estimators': 50, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None}\n",
      "Best score: 1161.7015 (MAE)\n",
      "\n",
      "Tuning Gradient_Boosting...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Gradient_Boosting:\n",
      "{'subsample': 1.0, 'n_estimators': 200, 'min_samples_leaf': 3, 'max_depth': 7, 'learning_rate': 0.05}\n",
      "Best score: 52283.8783 (MAE)\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for XGBoost:\n",
      "{'subsample': 1.0, 'n_estimators': 200, 'min_child_weight': 3, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "Best score: 109323.0280 (MAE)\n",
      "\n",
      "Performing 5-fold cross-validation with tuned models...\n",
      "Random_Forest CV R² scores: [0.99999367 0.99999354 0.99999275 0.99999204 0.99999314]\n",
      "Random_Forest CV R² mean: 1.0000, std: 0.0000\n",
      "Gradient_Boosting CV R² scores: [0.99945004 0.99956325 0.99952288 0.99955293 0.99955478]\n",
      "Gradient_Boosting CV R² mean: 0.9995, std: 0.0000\n",
      "XGBoost CV R² scores: [0.99789563 0.99805688 0.99817304 0.99791918 0.99795273]\n",
      "XGBoost CV R² mean: 0.9980, std: 0.0001\n",
      "\n",
      "Training tuned Random_Forest...\n",
      "Tuned Random_Forest - MAE: 795.44, RMSE: 10810.08, R2: 1.0000, Spearman: 0.9420\n",
      "\n",
      "Training tuned Gradient_Boosting...\n",
      "Tuned Gradient_Boosting - MAE: 55036.77, RMSE: 104439.29, R2: 0.9995, Spearman: 0.9416\n",
      "\n",
      "Training tuned XGBoost...\n",
      "Tuned XGBoost - MAE: 109830.73, RMSE: 213102.39, R2: 0.9980, Spearman: 0.9405\n",
      "Tuned Ensemble - MAE: 52590.85, RMSE: 101449.20, R2: 0.9995, Spearman: 0.9416\n",
      "Best tuned model based on MAE: Random_Forest\n",
      "Top 10 most important features:\n",
      "                                    Feature  Importance\n",
      "30            remainder__team_size_combined    0.362451\n",
      "31     stat_robust__weighted_priority_score    0.083505\n",
      "33  stat_robust__high_to_low_priority_ratio    0.081334\n",
      "34                   stat_robust__bug_ratio    0.072468\n",
      "20         pct_minmax__type_new_feature_pct    0.064973\n",
      "19                pct_minmax__type_task_pct    0.058105\n",
      "26         pct_minmax__priority_blocker_pct    0.051732\n",
      "32          stat_robust__issue_type_entropy    0.047089\n",
      "25        pct_minmax__priority_critical_pct    0.045079\n",
      "22         pct_minmax__type_improvement_pct    0.033961\n",
      "Calculating correlations for top 10 numeric features\n",
      "\n",
      "Creating model comparison visualizations...\n",
      "Best original model: Gradient_Boosting\n",
      "Best tuned model: Random_Forest\n",
      "\n",
      "Model training, tuning, and comparison complete. Results saved to task_estimation_hypertuned/\n",
      "Generated model comparison visualizations in task_estimation_hypertuned/comparison/\n",
      "See task_estimation_hypertuned/model_summary.txt for a detailed report\n",
      "\n",
      "NOTE: The 'count_std__total_issues' feature was not found in the final feature list.\n",
      "Possible reasons:\n",
      "1. It doesn't exist in the original dataset\n",
      "2. It was removed during preprocessing\n",
      "3. It was highly correlated with another feature and removed during multicollinearity check\n",
      "\n",
      "Analysis complete! The hypertuned model is ready for use.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'task_estimation_hypertuned_100_without_count'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'merged_task_data/merged_project_task_data.csv'\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Convert string columns that should be numeric\n",
    "for col in df.columns:\n",
    "    if col.startswith('fields.issuetype') or col.startswith('fields.priority'):\n",
    "        # Keep these as string/categorical\n",
    "        continue\n",
    "    try:\n",
    "        # Try converting to numeric, coerce errors to NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    except:\n",
    "        # If conversion fails completely, leave as is\n",
    "        pass\n",
    "\n",
    "# Use only 5% of the data for faster processing\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "print(f\"Reduced dataset shape (5%): {df.shape}\")\n",
    "\n",
    "# Define target variable - to be used only as the target, never as a feature\n",
    "target_variable = 'total_resolution_hours'\n",
    "\n",
    "# Define fields that must be excluded\n",
    "fields_to_exclude = ['fields.status.name', 'fields.project.key', 'fields.priority.name', 'fields.project.name']\n",
    "\n",
    "# Check which of these fields actually exist in the dataframe\n",
    "existing_fields_to_exclude = [field for field in fields_to_exclude if field in df.columns]\n",
    "print(f\"\\nFields to exclude that exist in the dataframe: {existing_fields_to_exclude}\")\n",
    "\n",
    "# Remove the fields from the dataframe itself to avoid correlation errors\n",
    "if existing_fields_to_exclude:\n",
    "    print(f\"Dropping these fields from the dataframe: {existing_fields_to_exclude}\")\n",
    "    df = df.drop(columns=existing_fields_to_exclude)\n",
    "    print(f\"Dataframe shape after dropping: {df.shape}\")\n",
    "\n",
    "# Check for missing values in the target variable\n",
    "print(f\"Missing values in target: {df[target_variable].isna().sum()}\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df.dropna(subset=[target_variable])\n",
    "print(f\"Dataset shape after dropping missing targets: {df.shape}\")\n",
    "\n",
    "# Plot distribution of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[target_variable].clip(0, 500), bins=50)\n",
    "plt.title('Distribution of Total Resolution Hours (clipped at 500 for visibility)')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'{output_dir}/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Identify numeric columns for correlation analysis\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# Exclude target from potential features\n",
    "numeric_features = [col for col in numeric_columns if col != target_variable]\n",
    "print(f\"\\nUsing {len(numeric_features)} numeric features for correlation calculation\")\n",
    "\n",
    "# Calculate correlations between features and target\n",
    "if target_variable in df.columns and numeric_features:\n",
    "    # Create a dataframe with just the features and target for correlation\n",
    "    corr_df = df[numeric_features + [target_variable]]\n",
    "    target_correlations = corr_df.corr()[target_variable].drop(target_variable).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 10 correlated features with target:\")\n",
    "    print(target_correlations.head(10))\n",
    "    print(\"\\nBottom 10 correlated features with target:\")\n",
    "    print(target_correlations.tail(10))\n",
    "    \n",
    "    # Print all features that have correlation > 0.8 with target\n",
    "    high_corr_features = target_correlations[target_correlations > 0.8].index.tolist()\n",
    "    print(f\"\\nFeatures with >0.8 correlation with target: {high_corr_features}\")\n",
    "else:\n",
    "    print(\"Cannot calculate target correlations - either target or features missing\")\n",
    "    high_corr_features = []\n",
    "    \n",
    "# Define suspicious features (features that may lead to data leakage)\n",
    "suspicious_features = [\n",
    "    # Resolution-related features that would leak the target information\n",
    "    'avg_resolution_hours', 'median_resolution_hours', \n",
    "    'resolution_hours', 'log_resolution_hours'\n",
    "]\n",
    "\n",
    "# Add highly correlated features to suspicious list\n",
    "suspicious_features.extend(high_corr_features)\n",
    "\n",
    "# # Keep count_std__total_issues despite high correlation (if it exists)\n",
    "# if 'count_std__total_issues' in suspicious_features:\n",
    "#     suspicious_features.remove('count_std__total_issues')\n",
    "#     print(\"Keeping 'count_std__total_issues' despite high correlation with target\")\n",
    "\n",
    "# Remove count_std__total_issues\n",
    "if 'count_std__total_issues' in suspicious_features:\n",
    "    # Do nothing, it's already excluded\n",
    "    print(\"Removing 'count_std__total_issues' from features\")\n",
    "else:\n",
    "    suspicious_features.append('count_std__total_issues')\n",
    "    print(\"'count_std__total_issues' marked for exclusion\")\n",
    "\n",
    "# Add excluded fields to suspicious list\n",
    "suspicious_features.extend(fields_to_exclude)\n",
    "\n",
    "# Make sure target variable is not in suspicious features list (it's our target, not a feature)\n",
    "if target_variable in suspicious_features:\n",
    "    suspicious_features.remove(target_variable)\n",
    "    \n",
    "print(f\"\\nRemoving suspicious features: {suspicious_features}\")\n",
    "\n",
    "# Original features list, excluding suspicious features\n",
    "features = [\n",
    "    'fields.issuetype.id', 'fields.priority.id', 'priority_id', 'issue_type_id',\n",
    "    'type_task', 'type_bug', 'inward_count', 'outward_count', 'type_sub_task',\n",
    "    'created_hour', 'created_month', 'created_year',\n",
    "    'is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement',\n",
    "    'is_type_new_feature', 'is_type_epic', 'is_type_sub-task',\n",
    "    'is_priority_blocker', 'is_priority_critical', 'is_priority_major',\n",
    "    'is_priority_minor', 'is_priority_trivial',\n",
    "    'pct_minmax__type_bug_pct', 'pct_minmax__type_task_pct',\n",
    "    'pct_minmax__type_new_feature_pct', 'pct_minmax__type_epic_pct',\n",
    "    'pct_minmax__type_improvement_pct', 'pct_minmax__type_story_pct',\n",
    "    'pct_minmax__type_documentation_pct', 'pct_minmax__priority_critical_pct',\n",
    "    'pct_minmax__priority_blocker_pct', 'pct_minmax__priority_high_pct',\n",
    "    'pct_minmax__priority_low_pct',\n",
    "    'remainder__team_size_creators', 'remainder__team_size_assignees',\n",
    "    'remainder__team_size_combined',\n",
    "    'stat_robust__weighted_priority_score', 'stat_robust__issue_type_entropy',\n",
    "    'stat_robust__high_to_low_priority_ratio', 'stat_robust__bug_ratio'\n",
    "]\n",
    "\n",
    "# If count_std__total_issues exists in the dataframe, add it to our features list\n",
    "# if 'count_std__total_issues' in df.columns and 'count_std__total_issues' not in features:\n",
    "#     features.append('count_std__total_issues')\n",
    "#     print(\"Added 'count_std__total_issues' to features list\")\n",
    "\n",
    "# Ensure we exclude the additional fields by checking if they exist in the dataset\n",
    "for field in fields_to_exclude:\n",
    "    if field in df.columns and field not in suspicious_features:\n",
    "        suspicious_features.append(field)\n",
    "        print(f\"Added {field} to suspicious features list\")\n",
    "\n",
    "# Filter dataset to include only features available in our dataset and not in suspicious list\n",
    "available_features = [f for f in features if f in df.columns and f not in suspicious_features]\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "print(f\"Missing features: {len(missing_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing feature list: {missing_features}\")\n",
    "    \n",
    "# Make absolutely sure we're not using the target as a feature\n",
    "if target_variable in available_features:\n",
    "    available_features.remove(target_variable)\n",
    "    print(f\"WARNING: Removed target variable '{target_variable}' from features list\")\n",
    "    \n",
    "# Explicitly check all fields that need to be dropped\n",
    "print(\"\\nChecking for fields that need to be dropped:\")\n",
    "for field in fields_to_exclude:\n",
    "    if field in df.columns:\n",
    "        print(f\"Field '{field}' exists in the dataframe\")\n",
    "        if field in available_features:\n",
    "            available_features.remove(field)\n",
    "            print(f\"-> Removed '{field}' from available features\")\n",
    "    else:\n",
    "        print(f\"Field '{field}' does not exist in the dataframe\")\n",
    "\n",
    "# Double-check if any of these fields might be in the final feature list\n",
    "for field in fields_to_exclude:\n",
    "    if field in available_features:\n",
    "        available_features.remove(field)\n",
    "        print(f\"-> Explicitly removed '{field}' from available features\")\n",
    "        \n",
    "# Print final confirmation\n",
    "print(\"\\nConfirming excluded fields:\")\n",
    "for field in fields_to_exclude:\n",
    "    print(f\"'{field}' is {'NOT' if field not in available_features else 'STILL'} in the feature list\")\n",
    "\n",
    "# Check for multicollinearity\n",
    "# Use only numeric features for correlation calculation\n",
    "numeric_available_features = [f for f in available_features if f in numeric_features]\n",
    "print(f\"\\nUsing {len(numeric_available_features)} numeric features for multicollinearity check\")\n",
    "\n",
    "if len(numeric_available_features) > 0:\n",
    "    # Verify one more time we don't have the target as a feature\n",
    "    if target_variable in numeric_available_features:\n",
    "        numeric_available_features.remove(target_variable)\n",
    "        print(f\"WARNING: Removed target variable from multicollinearity calculation\")\n",
    "    \n",
    "    correlation_matrix = df[numeric_available_features].corr().abs()\n",
    "    upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool_))\n",
    "    high_corr_pairs = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > 0.95]\n",
    "\n",
    "    print(f\"\\nHigh correlation pairs (>0.95):\")\n",
    "    for col1, col2 in high_corr_pairs:\n",
    "        print(f\"{col1} - {col2}: {upper_tri.loc[col2, col1]:.4f}\")\n",
    "    \n",
    "    # Optionally remove one of each highly correlated pair\n",
    "    features_to_drop = []\n",
    "    for col1, col2 in high_corr_pairs:\n",
    "        # # Keep count_std__total_issues even if highly correlated with another feature\n",
    "        # if col2 == 'count_std__total_issues':\n",
    "        #     # Swap the pair to keep count_std__total_issues\n",
    "        #     features_to_drop.append(col1)\n",
    "        # elif col1 == 'count_std__total_issues':\n",
    "        #     # Already in the order we want\n",
    "        #     features_to_drop.append(col2)\n",
    "        # else:\n",
    "            # Default: Keep the first one, drop the second one\n",
    "            if col2 not in features_to_drop:\n",
    "                features_to_drop.append(col2)\n",
    "\n",
    "    print(f\"\\nDropping features due to multicollinearity: {features_to_drop}\")\n",
    "else:\n",
    "    print(\"No numeric features available for multicollinearity check\")\n",
    "    features_to_drop = []\n",
    "\n",
    "final_features = [f for f in available_features if f not in features_to_drop]\n",
    "print(f\"Final feature count: {len(final_features)}\")\n",
    "\n",
    "# Final check to ensure we've dropped the specified fields\n",
    "for field in fields_to_exclude:\n",
    "    if field in final_features:\n",
    "        final_features.remove(field)\n",
    "        print(f\"Final check: Removed '{field}' from final_features\")\n",
    "        \n",
    "# Print confirmation of fields not in final features\n",
    "print(\"\\nFinal confirmation - excluded fields:\")\n",
    "for field in fields_to_exclude:\n",
    "    present = field in final_features\n",
    "    print(f\"'{field}' is {'STILL in' if present else 'NOT in'} the final feature list\")\n",
    "\n",
    "# Check if count_std__total_issues made it to the final list\n",
    "if 'count_std__total_issues' in final_features:\n",
    "    print(\"\\nSuccessfully kept 'count_std__total_issues' in the final feature list\")\n",
    "else:\n",
    "    print(\"\\n'count_std__total_issues' is not in the final feature list\")\n",
    "    # If it's in the dataframe but not in final features, this is concerning\n",
    "    if 'count_std__total_issues' in df.columns:\n",
    "        print(\"CAUTION: 'count_std__total_issues' exists in the dataset but was excluded from final features\")\n",
    "\n",
    "# Prepare the data\n",
    "X = df[final_features].copy()\n",
    "y = df[target_variable]  # The target variable\n",
    "\n",
    "# Fill any remaining NaN values with median\n",
    "for col in X.columns:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Split the data with stratification on binned target to ensure similar distributions\n",
    "y_binned = pd.qcut(y, q=10, duplicates='drop')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "\n",
    "# Final verification: ensure target is not used as a feature\n",
    "print(\"\\nFinal verification before model training:\")\n",
    "print(f\"Target variable: {target_variable}\")\n",
    "print(f\"Number of features: {len(final_features)}\")\n",
    "if target_variable in final_features:\n",
    "    print(f\"ERROR: Target variable found in feature list!\")\n",
    "    final_features.remove(target_variable)\n",
    "    X = df[final_features].copy()  # Recreate X without the target\n",
    "    print(f\"Removed target from features. New feature count: {len(final_features)}\")\n",
    "    \n",
    "    # Re-split with corrected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "else:\n",
    "    print(\"Verified: Target is not being used as a feature\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save data splits for later reference\n",
    "with open(f'{output_dir}/data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': final_features,\n",
    "        'scaler': scaler\n",
    "    }, f)\n",
    "\n",
    "# Define the original models with more conservative hyperparameters to prevent overfitting\n",
    "original_models = {\n",
    "    'Random_Forest': RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient_Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Copy for tuning later\n",
    "models = original_models.copy()\n",
    "\n",
    "# Save original model hyperparameters for reference\n",
    "original_params = {name: model.get_params() for name, model in original_models.items()}\n",
    "with open(f'{output_dir}/original_hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(original_params, f)\n",
    "\n",
    "# First, train and evaluate original models\n",
    "original_results = {}\n",
    "original_predictions = {}\n",
    "\n",
    "print(\"\\nTraining original models (before hyperparameter tuning)...\")\n",
    "for name, model in original_models.items():\n",
    "    print(f\"\\nTraining original {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    original_predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)  # Rank correlation, less affected by outliers\n",
    "    \n",
    "    original_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    print(f\"Original {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}, Spearman: {spearman_corr:.4f}\")\n",
    "    \n",
    "    # Save the original model\n",
    "    with open(f'{output_dir}/original_{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Create original ensemble prediction\n",
    "original_ensemble_pred = np.mean([original_predictions[name] for name in original_models.keys()], axis=0)\n",
    "\n",
    "# Calculate metrics for original ensemble\n",
    "original_ensemble_mae = mean_absolute_error(y_test, original_ensemble_pred)\n",
    "original_ensemble_rmse = np.sqrt(mean_squared_error(y_test, original_ensemble_pred))\n",
    "original_ensemble_r2 = r2_score(y_test, original_ensemble_pred)\n",
    "original_ensemble_spearman, _ = spearmanr(y_test, original_ensemble_pred)\n",
    "\n",
    "original_results['Ensemble'] = {\n",
    "    'MAE': original_ensemble_mae,\n",
    "    'RMSE': original_ensemble_rmse,\n",
    "    'R2': original_ensemble_r2,\n",
    "    'Spearman': original_ensemble_spearman\n",
    "}\n",
    "\n",
    "print(f\"Original Ensemble - MAE: {original_ensemble_mae:.2f}, RMSE: {original_ensemble_rmse:.2f}, R2: {original_ensemble_r2:.4f}, Spearman: {original_ensemble_spearman:.4f}\")\n",
    "\n",
    "# Save original results\n",
    "with open(f'{output_dir}/original_results.pkl', 'wb') as f:\n",
    "    pickle.dump(original_results, f)\n",
    "\n",
    "# Create prediction and residual plots for original models\n",
    "for name, y_pred in original_predictions.items():\n",
    "    print(f\"\\nCreating plots for original {name} model...\")\n",
    "    \n",
    "    # Actual vs Predicted Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Hours')\n",
    "    plt.ylabel('Predicted Hours')\n",
    "    plt.title(f'Original {name} - Actual vs Predicted')\n",
    "    plt.savefig(f'{output_dir}/original_{name}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Residual Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    residuals = y_test - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Hours')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.title(f'Original {name} - Residual Plot')\n",
    "    plt.savefig(f'{output_dir}/original_{name}_residuals.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create a plot for the original ensemble predictions\n",
    "# Actual vs Predicted Plot for Original Ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, original_ensemble_pred, alpha=0.3)\n",
    "plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Hours')\n",
    "plt.ylabel('Predicted Hours')\n",
    "plt.title('Original Ensemble - Actual vs Predicted')\n",
    "plt.savefig(f'{output_dir}/original_ensemble_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Residual Plot for Original Ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "original_ensemble_residuals = y_test - original_ensemble_pred\n",
    "plt.scatter(original_ensemble_pred, original_ensemble_residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Hours')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Original Ensemble - Residual Plot')\n",
    "plt.savefig(f'{output_dir}/original_ensemble_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nGenerated prediction and residual plots for original models and ensemble.\")\n",
    "\n",
    "#########################\n",
    "# Hyperparameter Tuning #\n",
    "#########################\n",
    "\n",
    "print(\"\\nPerforming hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_leaf': [1, 3, 5],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_samples_leaf': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "tuned_models = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster execution\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=10,  # Try 10 random combinations\n",
    "        cv=3,        # 3-fold cross-validation for speed\n",
    "        scoring='neg_mean_absolute_error',  # Optimize for MAE\n",
    "        n_jobs=-1,   # Use all available processors\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(f\"Best parameters for {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best score: {-grid_search.best_score_:.4f} (MAE)\")\n",
    "    \n",
    "    # Store best parameters and scores\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    best_scores[name] = -grid_search.best_score_\n",
    "    \n",
    "    # Update the model with the best estimator\n",
    "    tuned_models[name] = grid_search.best_estimator_\n",
    "\n",
    "# Save hyperparameter tuning results\n",
    "with open(f'{output_dir}/hyperparameter_tuning_results.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'best_params': best_params,\n",
    "        'best_scores': best_scores\n",
    "    }, f)\n",
    "\n",
    "# Replace original models with tuned models for future steps\n",
    "models = tuned_models\n",
    "\n",
    "# Perform cross-validation on tuned models\n",
    "print(\"\\nPerforming 5-fold cross-validation with tuned models...\")\n",
    "tuned_cv_results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    tuned_cv_results[name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} CV R² scores: {cv_scores}\")\n",
    "    print(f\"{name} CV R² mean: {cv_scores.mean():.4f}, std: {cv_scores.std():.4f}\")\n",
    "\n",
    "# Save cross-validation results\n",
    "with open(f'{output_dir}/tuned_cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_cv_results, f)\n",
    "\n",
    "# Train and evaluate tuned models\n",
    "tuned_results = {}\n",
    "tuned_predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining tuned {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    tuned_predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "    \n",
    "    tuned_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    print(f\"Tuned {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}, Spearman: {spearman_corr:.4f}\")\n",
    "    \n",
    "    # Save the tuned model\n",
    "    with open(f'{output_dir}/tuned_{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Hours')\n",
    "    plt.ylabel('Predicted Hours')\n",
    "    plt.title(f'Tuned {name} - Actual vs Predicted')\n",
    "    plt.savefig(f'{output_dir}/tuned_{name}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Add residual subplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    residuals = y_test - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Hours')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.title(f'Tuned {name} - Residual Plot')\n",
    "    plt.savefig(f'{output_dir}/tuned_{name}_residuals.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create a tuned ensemble prediction\n",
    "tuned_ensemble_pred = np.mean([tuned_predictions[name] for name in models.keys()], axis=0)\n",
    "\n",
    "# Calculate metrics for tuned ensemble\n",
    "tuned_ensemble_mae = mean_absolute_error(y_test, tuned_ensemble_pred)\n",
    "tuned_ensemble_rmse = np.sqrt(mean_squared_error(y_test, tuned_ensemble_pred))\n",
    "tuned_ensemble_r2 = r2_score(y_test, tuned_ensemble_pred)\n",
    "tuned_ensemble_spearman, _ = spearmanr(y_test, tuned_ensemble_pred)\n",
    "\n",
    "tuned_results['Ensemble'] = {\n",
    "    'MAE': tuned_ensemble_mae,\n",
    "    'RMSE': tuned_ensemble_rmse,\n",
    "    'R2': tuned_ensemble_r2,\n",
    "    'Spearman': tuned_ensemble_spearman\n",
    "}\n",
    "\n",
    "print(f\"Tuned Ensemble - MAE: {tuned_ensemble_mae:.2f}, RMSE: {tuned_ensemble_rmse:.2f}, R2: {tuned_ensemble_r2:.4f}, Spearman: {tuned_ensemble_spearman:.4f}\")\n",
    "\n",
    "# Save tuned results\n",
    "with open(f'{output_dir}/tuned_results.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_results, f)\n",
    "\n",
    "# Plot actual vs predicted for tuned ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, tuned_ensemble_pred, alpha=0.3)\n",
    "plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Hours')\n",
    "plt.ylabel('Predicted Hours')\n",
    "plt.title('Tuned Ensemble - Actual vs Predicted')\n",
    "plt.savefig(f'{output_dir}/tuned_ensemble_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot residuals for tuned ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "tuned_ensemble_residuals = y_test - tuned_ensemble_pred\n",
    "plt.scatter(tuned_ensemble_pred, tuned_ensemble_residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Hours')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Tuned Ensemble - Residual Plot')\n",
    "plt.savefig(f'{output_dir}/tuned_ensemble_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Identify the best tuned model\n",
    "best_tuned_model_name = min(tuned_results, key=lambda x: tuned_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "best_tuned_model = models[best_tuned_model_name]\n",
    "print(f\"Best tuned model based on MAE: {best_tuned_model_name}\")\n",
    "\n",
    "# Save the best tuned model\n",
    "with open(f'{output_dir}/best_tuned_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_tuned_model, f)\n",
    "\n",
    "# Plot feature importance for the best tuned model\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = best_tuned_model.feature_importances_\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f'Feature Importance - Best Tuned Model ({best_tuned_model_name})')\n",
    "    plt.bar(range(min(20, len(final_features))), \n",
    "            importances[indices[:20]], \n",
    "            align='center')\n",
    "    plt.xticks(range(min(20, len(final_features))), \n",
    "               [final_features[i] for i in indices[:20]], \n",
    "               rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/best_tuned_model_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame for easier analysis\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance to CSV\n",
    "    feature_importance_df.to_csv(f'{output_dir}/feature_importance.csv', index=False)\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "\n",
    "    # Calculate and plot correlations between top features and the target\n",
    "    top_features = [f for f in feature_importance_df['Feature'].head(10).tolist() if f in numeric_features]\n",
    "    if top_features:\n",
    "        # Final check that we don't include the target in the correlation matrix\n",
    "        if target_variable in top_features:\n",
    "            top_features.remove(target_variable)\n",
    "            print(f\"WARNING: Removed target from top features correlation\")\n",
    "            \n",
    "        print(f\"Calculating correlations for top {len(top_features)} numeric features\")\n",
    "        if top_features:\n",
    "            corr_columns = top_features + [target_variable]\n",
    "            corr_data = df[corr_columns].copy()\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "            plt.title('Correlation Between Top Features and Target')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/top_features_correlation.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"No valid top features remain for correlation analysis\")\n",
    "    else:\n",
    "        print(\"No numeric top features available for correlation analysis\")\n",
    "\n",
    "    # Group feature importance by categories\n",
    "    # Define categories based on feature name prefixes\n",
    "    categories = {\n",
    "        'Issue Type': [f for f in final_features if 'type_' in f or 'is_type_' in f],\n",
    "        'Priority': [f for f in final_features if 'priority' in f],\n",
    "        'Project Stats': [f for f in final_features if 'count_' in f or 'pct_' in f],\n",
    "        'Team Size': [f for f in final_features if 'team_size' in f],\n",
    "        'Created Time': [f for f in final_features if 'created_' in f],\n",
    "        'Robust Stats': [f for f in final_features if 'stat_robust' in f],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    # Assign remaining features to 'Other'\n",
    "    for feature in final_features:\n",
    "        if not any(feature in cat_features for cat_features in categories.values()):\n",
    "            categories['Other'].append(feature)\n",
    "    \n",
    "    # Calculate importance by category\n",
    "    category_importance = {}\n",
    "    for category, cat_features in categories.items():\n",
    "        if cat_features:  # Skip empty categories\n",
    "            total_importance = sum(feature_importance_df.loc[feature_importance_df['Feature'].isin(cat_features), 'Importance'])\n",
    "            category_importance[category] = total_importance\n",
    "    \n",
    "    # Plot category importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    categories_sorted = sorted(category_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    categories_names = [item[0] for item in categories_sorted]\n",
    "    categories_values = [item[1] for item in categories_sorted]\n",
    "    \n",
    "    plt.bar(categories_names, categories_values)\n",
    "    plt.xlabel('Feature Category')\n",
    "    plt.ylabel('Total Importance')\n",
    "    plt.title('Feature Importance by Category')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/best_tuned_model_feature_importance_by_category.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations to compare original vs tuned models\n",
    "comparison_dir = f'{output_dir}/comparison'\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nCreating model comparison visualizations...\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name in original_results.keys():\n",
    "    if model_name != 'Ensemble' or (model_name == 'Ensemble' and model_name in tuned_results):\n",
    "        for metric_name in ['MAE', 'RMSE', 'R2', 'Spearman']:\n",
    "            original_value = original_results[model_name].get(metric_name, None)\n",
    "            tuned_value = tuned_results.get(model_name, {}).get(metric_name, None)\n",
    "            \n",
    "            if original_value is not None and tuned_value is not None:\n",
    "                # Calculate improvement\n",
    "                if metric_name in ['MAE', 'RMSE']:  # Lower is better\n",
    "                    improvement = ((original_value - tuned_value) / original_value) * 100\n",
    "                else:  # Higher is better\n",
    "                    improvement = ((tuned_value - original_value) / abs(original_value)) * 100\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Metric': metric_name,\n",
    "                    'Original': original_value,\n",
    "                    'Tuned': tuned_value,\n",
    "                    'Improvement (%)': improvement\n",
    "                })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(f'{comparison_dir}/model_comparison.csv', index=False)\n",
    "\n",
    "# Create a performance comparison bar chart for each metric\n",
    "for metric in ['MAE', 'RMSE', 'R2', 'Spearman']:\n",
    "    metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Adjust these values based on your metric\n",
    "    better_direction = 'lower' if metric in ['MAE', 'RMSE'] else 'higher'\n",
    "    \n",
    "    # Setup the bar chart\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(metric_data['Model'].unique()))\n",
    "    \n",
    "    # Plot bars\n",
    "    plt.bar(x - bar_width/2, metric_data['Original'], width=bar_width, label='Original', color='lightblue')\n",
    "    plt.bar(x + bar_width/2, metric_data['Tuned'], width=bar_width, label='Hyperparameter Tuned', color='darkblue')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, original in enumerate(metric_data['Original']):\n",
    "        plt.text(i - bar_width/2, original * 1.02, f'{original:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "    for i, tuned in enumerate(metric_data['Tuned']):\n",
    "        plt.text(i + bar_width/2, tuned * 1.02, f'{tuned:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'{metric} Comparison: Original vs Tuned Models ({better_direction} is better)')\n",
    "    plt.xticks(x, metric_data['Model'])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{comparison_dir}/{metric}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create an improvement percentage chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Pivot the data for easier plotting\n",
    "pivot_df = pd.pivot_table(\n",
    "    comparison_df, \n",
    "    values='Improvement (%)', \n",
    "    index='Model', \n",
    "    columns='Metric'\n",
    ")\n",
    "\n",
    "# Plot the improvement heatmap\n",
    "sns.heatmap(\n",
    "    pivot_df, \n",
    "    annot=True, \n",
    "    cmap='RdYlGn', \n",
    "    fmt='.1f',\n",
    "    linewidths=.5, \n",
    "    center=0\n",
    ")\n",
    "plt.title('Percentage Improvement from Hyperparameter Tuning')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{comparison_dir}/improvement_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create side-by-side comparison of best models\n",
    "# Identify best original and tuned models\n",
    "best_original_model_name = min(original_results, key=lambda x: original_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "best_tuned_model_name = min(tuned_results, key=lambda x: tuned_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "\n",
    "print(f\"Best original model: {best_original_model_name}\")\n",
    "print(f\"Best tuned model: {best_tuned_model_name}\")\n",
    "\n",
    "# Create side-by-side comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Original model plot\n",
    "ax1.scatter(y_test, original_predictions[best_original_model_name], alpha=0.3)\n",
    "ax1.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "ax1.set_xlabel('Actual Hours')\n",
    "ax1.set_ylabel('Predicted Hours')\n",
    "ax1.set_title(f'Best Original Model: {best_original_model_name}')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add metrics annotation\n",
    "orig_metrics = original_results[best_original_model_name]\n",
    "orig_metrics_text = f\"MAE: {orig_metrics['MAE']:.2f}\\nRMSE: {orig_metrics['RMSE']:.2f}\\nR²: {orig_metrics['R2']:.4f}\"\n",
    "ax1.annotate(\n",
    "    orig_metrics_text, \n",
    "    xy=(0.05, 0.95), \n",
    "    xycoords='axes fraction', \n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "# Tuned model plot\n",
    "ax2.scatter(y_test, tuned_predictions[best_tuned_model_name], alpha=0.3)\n",
    "ax2.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "ax2.set_xlabel('Actual Hours')\n",
    "ax2.set_ylabel('Predicted Hours')\n",
    "ax2.set_title(f'Best Tuned Model: {best_tuned_model_name}')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add metrics annotation\n",
    "tuned_metrics = tuned_results[best_tuned_model_name]\n",
    "tuned_metrics_text = f\"MAE: {tuned_metrics['MAE']:.2f}\\nRMSE: {tuned_metrics['RMSE']:.2f}\\nR²: {tuned_metrics['R2']:.4f}\"\n",
    "ax2.annotate(\n",
    "    tuned_metrics_text, \n",
    "    xy=(0.05, 0.95), \n",
    "    xycoords='axes fraction', \n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "plt.suptitle('Best Models Comparison: Original vs Tuned', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig(f'{comparison_dir}/best_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Generate summary report\n",
    "with open(f'{output_dir}/model_summary.txt', 'w') as f:\n",
    "    f.write(\"# Resolution Hours Ensemble Model with Hyperparameter Tuning\\n\\n\")\n",
    "    f.write(\"## Performance Comparison\\n\\n\")\n",
    "    \n",
    "    # Write performance table\n",
    "    f.write(\"| Model | Metric | Original | Tuned | Improvement (%) |\\n\")\n",
    "    f.write(\"|-------|--------|----------|-------|----------------|\\n\")\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        f.write(f\"| {row['Model']} | {row['Metric']} | {row['Original']:.4f} | {row['Tuned']:.4f} | {row['Improvement (%)']:.2f} |\\n\")\n",
    "    \n",
    "    # Write hyperparameter differences\n",
    "    f.write(\"\\n## Hyperparameter Changes\\n\\n\")\n",
    "    for model_name in best_params:\n",
    "        f.write(f\"### {model_name}\\n\\n\")\n",
    "        f.write(\"| Parameter | Original | Tuned |\\n\")\n",
    "        f.write(\"|-----------|----------|-------|\\n\")\n",
    "        \n",
    "        orig_params = original_params[model_name]\n",
    "        tuned_params = best_params[model_name]\n",
    "        \n",
    "        for param, tuned_value in tuned_params.items():\n",
    "            orig_value = orig_params.get(param, \"N/A\")\n",
    "            f.write(f\"| {param} | {orig_value} | {tuned_value} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Write conclusion\n",
    "    f.write(\"\\n## Conclusion\\n\\n\")\n",
    "    \n",
    "    # Calculate average improvement\n",
    "    mae_improvement = comparison_df[comparison_df['Metric'] == 'MAE']['Improvement (%)'].mean()\n",
    "    r2_improvement = comparison_df[comparison_df['Metric'] == 'R2']['Improvement (%)'].mean()\n",
    "    \n",
    "    f.write(f\"Hyperparameter tuning resulted in an average MAE improvement of {mae_improvement:.2f}% \")\n",
    "    f.write(f\"and an average R² improvement of {r2_improvement:.2f}%.\\n\\n\")\n",
    "    \n",
    "    if best_original_model_name == best_tuned_model_name:\n",
    "        f.write(f\"Both before and after tuning, {best_original_model_name} was the best performing model.\\n\")\n",
    "    else:\n",
    "        f.write(f\"Before tuning, {best_original_model_name} was the best performing model. \")\n",
    "        f.write(f\"After tuning, {best_tuned_model_name} became the best performing model.\\n\")\n",
    "\n",
    "print(f\"\\nModel training, tuning, and comparison complete. Results saved to {output_dir}/\")\n",
    "print(f\"Generated model comparison visualizations in {comparison_dir}/\")\n",
    "print(f\"See {output_dir}/model_summary.txt for a detailed report\")\n",
    "\n",
    "# If count_std__total_issues was kept, check its importance\n",
    "if 'count_std__total_issues' in final_features and hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    count_feature_idx = final_features.index('count_std__total_issues')\n",
    "    count_feature_importance = importances[count_feature_idx]\n",
    "    count_feature_rank = list(indices).index(count_feature_idx) + 1\n",
    "    \n",
    "    print(f\"\\nImportance of 'count_std__total_issues':\")\n",
    "    print(f\"- Importance value: {count_feature_importance:.6f}\")\n",
    "    print(f\"- Rank among all features: {count_feature_rank} out of {len(final_features)}\")\n",
    "    print(f\"- Percentile: {100 - (count_feature_rank / len(final_features) * 100):.1f}%\")\n",
    "    \n",
    "    # Highlight this in the summary as well\n",
    "    with open(f'{output_dir}/model_summary.txt', 'a') as f:\n",
    "        f.write(\"\\n## Special Feature Analysis: count_std__total_issues\\n\\n\")\n",
    "        f.write(f\"Importance value: {count_feature_importance:.6f}\\n\")\n",
    "        f.write(f\"Rank among all features: {count_feature_rank} out of {len(final_features)}\\n\")\n",
    "        f.write(f\"Percentile: {100 - (count_feature_rank / len(final_features) * 100):.1f}%\\n\")\n",
    "        \n",
    "        if count_feature_rank <= 10:\n",
    "            f.write(\"\\nThis feature is among the top 10 most important features for the model.\\n\")\n",
    "        elif count_feature_rank <= len(final_features) // 4:\n",
    "            f.write(\"\\nThis feature is in the top 25% of all features by importance.\\n\")\n",
    "        else:\n",
    "            f.write(\"\\nThis feature has moderate importance in the model.\\n\")\n",
    "\n",
    "# Print final message about count_std__total_issues to guide further analysis\n",
    "if 'count_std__total_issues' in final_features:\n",
    "    print(\"\\nNOTE: The 'count_std__total_issues' feature was successfully included in the model.\")\n",
    "    print(\"Check the feature importance section in the summary report to see its impact.\")\n",
    "else:\n",
    "    print(\"\\nNOTE: The 'count_std__total_issues' feature was not found in the final feature list.\")\n",
    "    print(\"Possible reasons:\")\n",
    "    print(\"1. It doesn't exist in the original dataset\")\n",
    "    print(\"2. It was removed during preprocessing\")\n",
    "    print(\"3. It was highly correlated with another feature and removed during multicollinearity check\")\n",
    "    \n",
    "print(\"\\nAnalysis complete! The hypertuned model is ready for use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
