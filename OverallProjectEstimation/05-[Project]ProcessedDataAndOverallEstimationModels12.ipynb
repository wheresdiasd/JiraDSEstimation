{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (971, 185)\n",
      "Missing values: 71310\n",
      "\n",
      "=== DATA TYPE ANALYSIS ===\n",
      "float64    169\n",
      "int64        9\n",
      "object       7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Non-numeric columns (7):\n",
      "  - project_key: object, 958 unique values\n",
      "  - project_name: object, 970 unique values\n",
      "  - project_start_date: object, 970 unique values\n",
      "  - project_latest_resolved_date: object, 969 unique values\n",
      "  - project_latest_update_date: object, 962 unique values\n",
      "  - repository: object, 7 unique values\n",
      "    Values: ['MariaDB' 'Hyperledger' 'MongoDB' 'Jira' 'Mojang' 'RedHat' 'Apache']\n",
      "  - source_file: object, 971 unique values\n",
      "\n",
      "=== HANDLING NON-NUMERIC COLUMNS ===\n",
      "Dropping identifier columns: ['project_key', 'project_name', 'project_start_date', 'project_latest_resolved_date', 'project_latest_update_date', 'repository', 'source_file']\n",
      "\n",
      "=== IDENTIFYING TARGET VARIABLES ===\n",
      "Target variables: ['avg_resolution_hours', 'median_resolution_hours', 'min_resolution_hours', 'max_resolution_hours', 'resolution_hours_std', 'total_resolution_hours']\n",
      "\n",
      "=== CATEGORIZING FEATURES ===\n",
      "\n",
      "=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\n",
      "Capped 181 lower and 3 upper outliers in project_id\n",
      "Capped 0 lower and 117 upper outliers in total_issues\n",
      "Capped 0 lower and 1 upper outliers in project_duration_days\n",
      "Capped 0 lower and 78 upper outliers in avg_resolution_hours\n",
      "Capped 0 lower and 126 upper outliers in median_resolution_hours\n",
      "Capped 3 lower and 169 upper outliers in min_resolution_hours\n",
      "Capped 0 lower and 2 upper outliers in max_resolution_hours\n",
      "Filled 18 missing values in resolution_hours_std with median: 5566.3916\n",
      "Capped 0 lower and 21 upper outliers in resolution_hours_std\n",
      "Capped 0 lower and 136 upper outliers in total_resolution_hours\n",
      "Filled 37 missing values in resolution_time_skewness with median: 2.8143\n",
      "Capped 0 lower and 23 upper outliers in resolution_time_skewness\n",
      "Filled 37 missing values in resolution_time_kurtosis with median: 8.5848\n",
      "Capped 0 lower and 69 upper outliers in resolution_time_kurtosis\n",
      "Capped 0 lower and 128 upper outliers in resolution_time_p25\n",
      "Capped 0 lower and 133 upper outliers in resolution_time_p75\n",
      "Capped 0 lower and 90 upper outliers in resolution_time_p90\n",
      "Capped 0 lower and 128 upper outliers in resolution_time_iqr\n",
      "Capped 0 lower and 32 upper outliers in pct_resolved_within_24h\n",
      "Capped 0 lower and 25 upper outliers in pct_resolved_within_week\n",
      "Capped 23 lower and 0 upper outliers in pct_resolved_within_month\n",
      "Capped 0 lower and 30 upper outliers in pct_issues_created_on_weekend\n",
      "Capped 0 lower and 40 upper outliers in pct_issues_resolved_on_weekend\n",
      "Capped 0 lower and 85 upper outliers in max_issues_per_month\n",
      "Capped 0 lower and 96 upper outliers in avg_issues_per_month\n",
      "Capped 0 lower and 6 upper outliers in months_with_activity\n",
      "Filled 7 missing values in issue_creation_volatility with median: 0.8586\n",
      "Capped 13 lower and 30 upper outliers in issue_creation_volatility\n",
      "Filled 156 missing values in priority_major_count with median: 258.0000\n",
      "Capped 0 lower and 92 upper outliers in priority_major_count\n",
      "Filled 156 missing values in priority_major_pct with median: 72.7273\n",
      "Capped 31 lower and 0 upper outliers in priority_major_pct\n",
      "Filled 196 missing values in priority_minor_count with median: 62.0000\n",
      "Capped 0 lower and 94 upper outliers in priority_minor_count\n",
      "Filled 196 missing values in priority_minor_pct with median: 16.1622\n",
      "Capped 0 lower and 21 upper outliers in priority_minor_pct\n",
      "Filled 264 missing values in priority_critical_count with median: 16.0000\n",
      "Capped 0 lower and 96 upper outliers in priority_critical_count\n",
      "Filled 264 missing values in priority_critical_pct with median: 3.5452\n",
      "Capped 0 lower and 42 upper outliers in priority_critical_pct\n",
      "Filled 336 missing values in priority_trivial_count with median: 12.0000\n",
      "Capped 0 lower and 87 upper outliers in priority_trivial_count\n",
      "Filled 336 missing values in priority_trivial_pct with median: 2.4590\n",
      "Capped 0 lower and 37 upper outliers in priority_trivial_pct\n",
      "Filled 34 missing values in type_bug_count with median: 153.0000\n",
      "Capped 0 lower and 131 upper outliers in type_bug_count\n",
      "Filled 34 missing values in type_bug_pct with median: 43.1579\n",
      "Filled 96 missing values in type_task_count with median: 30.0000\n",
      "Capped 0 lower and 123 upper outliers in type_task_count\n",
      "Filled 96 missing values in type_task_pct with median: 8.6022\n",
      "Capped 0 lower and 62 upper outliers in type_task_pct\n",
      "Filled 362 missing values in type_new_feature_count with median: 35.0000\n",
      "Capped 0 lower and 59 upper outliers in type_new_feature_count\n",
      "Filled 362 missing values in type_new_feature_pct with median: 8.2696\n",
      "Capped 0 lower and 31 upper outliers in type_new_feature_pct\n",
      "Filled 251 missing values in priority_major_type_task_count with median: 20.0000\n",
      "Capped 0 lower and 96 upper outliers in priority_major_type_task_count\n",
      "Filled 251 missing values in priority_major_type_task_avg_resolution_hours with median: 2001.7835\n",
      "Capped 0 lower and 68 upper outliers in priority_major_type_task_avg_resolution_hours\n",
      "Filled 206 missing values in priority_major_type_bug_count with median: 100.0000\n",
      "Capped 0 lower and 101 upper outliers in priority_major_type_bug_count\n",
      "Filled 206 missing values in priority_major_type_bug_avg_resolution_hours with median: 2162.1244\n",
      "Capped 0 lower and 60 upper outliers in priority_major_type_bug_avg_resolution_hours\n",
      "Filled 451 missing values in priority_major_type_new_feature_count with median: 19.0000\n",
      "Capped 0 lower and 58 upper outliers in priority_major_type_new_feature_count\n",
      "Filled 451 missing values in priority_major_type_new_feature_avg_resolution_hours with median: 4221.5146\n",
      "Capped 0 lower and 49 upper outliers in priority_major_type_new_feature_avg_resolution_hours\n",
      "Filled 432 missing values in priority_minor_type_task_count with median: 5.0000\n",
      "Capped 0 lower and 69 upper outliers in priority_minor_type_task_count\n",
      "Filled 432 missing values in priority_minor_type_task_avg_resolution_hours with median: 2301.8082\n",
      "Capped 0 lower and 48 upper outliers in priority_minor_type_task_avg_resolution_hours\n",
      "Filled 289 missing values in priority_minor_type_bug_count with median: 23.5000\n",
      "Capped 0 lower and 93 upper outliers in priority_minor_type_bug_count\n",
      "Filled 289 missing values in priority_minor_type_bug_avg_resolution_hours with median: 3192.1343\n",
      "Capped 0 lower and 41 upper outliers in priority_minor_type_bug_avg_resolution_hours\n",
      "Filled 331 missing values in priority_critical_type_bug_count with median: 12.0000\n",
      "Capped 0 lower and 82 upper outliers in priority_critical_type_bug_count\n",
      "Filled 331 missing values in priority_critical_type_bug_avg_resolution_hours with median: 2145.9509\n",
      "Capped 0 lower and 66 upper outliers in priority_critical_type_bug_avg_resolution_hours\n",
      "Filled 449 missing values in priority_trivial_type_bug_count with median: 6.0000\n",
      "Capped 0 lower and 67 upper outliers in priority_trivial_type_bug_count\n",
      "Filled 449 missing values in priority_trivial_type_bug_avg_resolution_hours with median: 1586.5446\n",
      "Capped 0 lower and 45 upper outliers in priority_trivial_type_bug_avg_resolution_hours\n",
      "Capped 0 lower and 39 upper outliers in avg_inward_links\n",
      "Capped 0 lower and 33 upper outliers in avg_outward_links\n",
      "Capped 0 lower and 37 upper outliers in avg_total_links\n",
      "Capped 0 lower and 159 upper outliers in total_inward_links\n",
      "Capped 0 lower and 160 upper outliers in total_outward_links\n",
      "Capped 0 lower and 163 upper outliers in total_links\n",
      "Capped 0 lower and 1 upper outliers in pct_issues_with_high_dependencies\n",
      "Capped 0 lower and 37 upper outliers in link_density\n",
      "Capped 0 lower and 113 upper outliers in num_resolved_issues\n",
      "Capped 55 lower and 0 upper outliers in pct_resolved_issues\n",
      "Filled 1 missing values in resolution_rate_per_day with median: 0.1182\n",
      "Capped 0 lower and 99 upper outliers in resolution_rate_per_day\n",
      "Filled 96 missing values in type_task_resolution_rate with median: 86.9565\n",
      "Capped 49 lower and 0 upper outliers in type_task_resolution_rate\n",
      "Filled 34 missing values in type_bug_resolution_rate with median: 87.1486\n",
      "Capped 67 lower and 0 upper outliers in type_bug_resolution_rate\n",
      "Filled 362 missing values in type_new_feature_resolution_rate with median: 75.0000\n",
      "Capped 38 lower and 0 upper outliers in type_new_feature_resolution_rate\n",
      "Capped 0 lower and 114 upper outliers in weekly_efficiency_ratio\n",
      "Capped 0 lower and 83 upper outliers in complexity_weighted_resolution_time\n",
      "Filled 165 missing values in high_to_low_priority_ratio with median: 4.4334\n",
      "Capped 0 lower and 69 upper outliers in high_to_low_priority_ratio\n",
      "Filled 34 missing values in bug_ratio with median: 0.7692\n",
      "Capped 0 lower and 5 upper outliers in bug_ratio\n",
      "Filled 1 missing values in creation_resolution_balance with median: 0.5387\n",
      "Filled 86 missing values in weighted_priority_score with median: 5.1793\n",
      "Capped 82 lower and 27 upper outliers in weighted_priority_score\n",
      "Capped 25 lower and 0 upper outliers in issue_type_entropy\n",
      "Filled 7 missing values in monthly_velocity with median: 5.1989\n",
      "Capped 0 lower and 101 upper outliers in monthly_velocity\n",
      "Filled 105 missing values in team_size_creators with median: 71.0000\n",
      "Capped 0 lower and 116 upper outliers in team_size_creators\n",
      "Filled 105 missing values in team_size_assignees with median: 0.0000\n",
      "Filled 105 missing values in team_size_combined with median: 71.0000\n",
      "Capped 0 lower and 116 upper outliers in team_size_combined\n",
      "Filled 105 missing values in core_team_ratio with median: 0.7327\n",
      "Capped 3 lower and 0 upper outliers in core_team_ratio\n",
      "Filled 105 missing values in creator_workload_gini with median: 0.6596\n",
      "Capped 19 lower and 0 upper outliers in creator_workload_gini\n",
      "Filled 105 missing values in creator_diversity with median: 4.2505\n",
      "Capped 0 lower and 11 upper outliers in creator_diversity\n",
      "Filled 105 missing values in avg_issues_per_creator with median: 5.4148\n",
      "Capped 0 lower and 57 upper outliers in avg_issues_per_creator\n",
      "Filled 105 missing values in top_creator_contribution with median: 0.2308\n",
      "Capped 0 lower and 24 upper outliers in top_creator_contribution\n",
      "Filled 112 missing values in creator_activity_variance with median: 209.6838\n",
      "Capped 0 lower and 110 upper outliers in creator_activity_variance\n",
      "Filled 112 missing values in creator_activity_std with median: 14.4805\n",
      "Capped 0 lower and 64 upper outliers in creator_activity_std\n",
      "Filled 124 missing values in team_type_specialization_index with median: 0.8873\n",
      "Capped 31 lower and 0 upper outliers in team_type_specialization_index\n",
      "Filled 105 missing values in bug_creation_ratio with median: 0.4379\n",
      "Filled 105 missing values in feature_request_ratio with median: 0.2941\n",
      "Capped 0 lower and 4 upper outliers in feature_request_ratio\n",
      "Filled 135 missing values in bug_creator_concentration with median: 0.5041\n",
      "Filled 135 missing values in bug_developer_ratio with median: 1.9115\n",
      "Capped 0 lower and 82 upper outliers in bug_developer_ratio\n",
      "Filled 245 missing values in feature_developer_ratio with median: 1.5074\n",
      "Capped 0 lower and 51 upper outliers in feature_developer_ratio\n",
      "Filled 105 missing values in weekend_activity_ratio with median: 0.2241\n",
      "Capped 0 lower and 40 upper outliers in weekend_activity_ratio\n",
      "Filled 110 missing values in creation_rate_stability with median: 0.1431\n",
      "Capped 26 lower and 12 upper outliers in creation_rate_stability\n",
      "Filled 110 missing values in avg_new_creators_per_month with median: 1.7791\n",
      "Capped 0 lower and 113 upper outliers in avg_new_creators_per_month\n",
      "Filled 110 missing values in creator_onboarding_volatility with median: 0.5612\n",
      "Capped 69 lower and 24 upper outliers in creator_onboarding_volatility\n",
      "Filled 105 missing values in creator_link_density_mean with median: 0.1969\n",
      "Capped 0 lower and 41 upper outliers in creator_link_density_mean\n",
      "Filled 105 missing values in creator_link_density_std with median: 0.4140\n",
      "Capped 0 lower and 24 upper outliers in creator_link_density_std\n",
      "Filled 198 missing values in complex_issue_distribution with median: 0.4167\n",
      "Filled 198 missing values in team_complexity_capacity with median: 70.4000\n",
      "Capped 0 lower and 101 upper outliers in team_complexity_capacity\n",
      "Filled 105 missing values in work_hour_creation_ratio with median: 0.4425\n",
      "Capped 8 lower and 14 upper outliers in work_hour_creation_ratio\n",
      "Filled 105 missing values in creation_hour_entropy with median: 4.1876\n",
      "Capped 79 lower and 0 upper outliers in creation_hour_entropy\n",
      "Filled 105 missing values in creator_resolution_time_variability with median: 1.4155\n",
      "Capped 28 lower and 20 upper outliers in creator_resolution_time_variability\n",
      "Filled 105 missing values in team_resolution_predictability with median: -0.8716\n",
      "Capped 22 lower and 0 upper outliers in team_resolution_predictability\n",
      "Filled 105 missing values in avg_creator_experience_days with median: 308.0861\n",
      "Capped 0 lower and 40 upper outliers in avg_creator_experience_days\n",
      "Filled 105 missing values in avg_creator_issue_count with median: 21.5082\n",
      "Capped 0 lower and 85 upper outliers in avg_creator_issue_count\n",
      "Filled 105 missing values in avg_creator_specialization with median: 0.7458\n",
      "Filled 268 missing values in priority_blocker_count with median: 10.0000\n",
      "Capped 0 lower and 97 upper outliers in priority_blocker_count\n",
      "Filled 268 missing values in priority_blocker_pct with median: 2.4450\n",
      "Capped 0 lower and 42 upper outliers in priority_blocker_pct\n",
      "Filled 746 missing values in type_epic_count with median: 19.0000\n",
      "Capped 0 lower and 15 upper outliers in type_epic_count\n",
      "Filled 746 missing values in type_epic_pct with median: 2.9007\n",
      "Capped 0 lower and 20 upper outliers in type_epic_pct\n",
      "Filled 871 missing values in priority_major_type_epic_count with median: 9.0000\n",
      "Capped 0 lower and 13 upper outliers in priority_major_type_epic_count\n",
      "Filled 871 missing values in priority_major_type_epic_avg_resolution_hours with median: 6215.5621\n",
      "Capped 0 lower and 5 upper outliers in priority_major_type_epic_avg_resolution_hours\n",
      "Filled 934 missing values in priority_minor_type_epic_count with median: 1.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_minor_type_epic_count\n",
      "Filled 934 missing values in priority_minor_type_epic_avg_resolution_hours with median: 6831.7895\n",
      "Capped 0 lower and 2 upper outliers in priority_minor_type_epic_avg_resolution_hours\n",
      "Filled 604 missing values in priority_trivial_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 46 upper outliers in priority_trivial_type_task_count\n",
      "Filled 604 missing values in priority_trivial_type_task_avg_resolution_hours with median: 861.1744\n",
      "Capped 0 lower and 30 upper outliers in priority_trivial_type_task_avg_resolution_hours\n",
      "Filled 346 missing values in priority_blocker_type_bug_count with median: 9.0000\n",
      "Capped 0 lower and 84 upper outliers in priority_blocker_type_bug_count\n",
      "Filled 346 missing values in priority_blocker_type_bug_avg_resolution_hours with median: 995.3753\n",
      "Capped 0 lower and 83 upper outliers in priority_blocker_type_bug_avg_resolution_hours\n",
      "Filled 640 missing values in priority_critical_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 46 upper outliers in priority_critical_type_task_count\n",
      "Filled 640 missing values in priority_critical_type_task_avg_resolution_hours with median: 1703.8983\n",
      "Capped 0 lower and 28 upper outliers in priority_critical_type_task_avg_resolution_hours\n",
      "Filled 746 missing values in type_epic_resolution_rate with median: 44.8276\n",
      "Filled 948 missing values in type_technical_task_count with median: 6.0000\n",
      "Capped 0 lower and 4 upper outliers in type_technical_task_count\n",
      "Filled 948 missing values in type_technical_task_pct with median: 0.4292\n",
      "Capped 0 lower and 2 upper outliers in type_technical_task_pct\n",
      "Filled 952 missing values in priority_major_type_technical_task_count with median: 6.0000\n",
      "Capped 0 lower and 4 upper outliers in priority_major_type_technical_task_count\n",
      "Filled 952 missing values in priority_major_type_technical_task_avg_resolution_hours with median: 712.1675\n",
      "Capped 0 lower and 2 upper outliers in priority_major_type_technical_task_avg_resolution_hours\n",
      "Filled 640 missing values in priority_blocker_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 48 upper outliers in priority_blocker_type_task_count\n",
      "Filled 640 missing values in priority_blocker_type_task_avg_resolution_hours with median: 984.4468\n",
      "Capped 0 lower and 41 upper outliers in priority_blocker_type_task_avg_resolution_hours\n",
      "Filled 948 missing values in type_technical_task_resolution_rate with median: 100.0000\n",
      "Capped 4 lower and 0 upper outliers in type_technical_task_resolution_rate\n",
      "Filled 330 missing values in type_improvement_count with median: 92.0000\n",
      "Capped 0 lower and 80 upper outliers in type_improvement_count\n",
      "Filled 330 missing values in type_improvement_pct with median: 26.2782\n",
      "Capped 0 lower and 12 upper outliers in type_improvement_pct\n",
      "Filled 930 missing values in type_release_count with median: 4.0000\n",
      "Capped 0 lower and 5 upper outliers in type_release_count\n",
      "Filled 930 missing values in type_release_pct with median: 0.3648\n",
      "Capped 0 lower and 4 upper outliers in type_release_pct\n",
      "Filled 794 missing values in type_story_count with median: 36.0000\n",
      "Capped 0 lower and 12 upper outliers in type_story_count\n",
      "Filled 794 missing values in type_story_pct with median: 8.6747\n",
      "Filled 890 missing values in priority_major_type_story_count with median: 10.0000\n",
      "Capped 0 lower and 6 upper outliers in priority_major_type_story_count\n",
      "Filled 890 missing values in priority_major_type_story_avg_resolution_hours with median: 2647.0180\n",
      "Capped 0 lower and 6 upper outliers in priority_major_type_story_avg_resolution_hours\n",
      "Filled 930 missing values in type_release_resolution_rate with median: 100.0000\n",
      "Capped 7 lower and 0 upper outliers in type_release_resolution_rate\n",
      "Filled 794 missing values in type_story_resolution_rate with median: 71.4286\n",
      "Filled 330 missing values in type_improvement_resolution_rate with median: 80.0000\n",
      "Capped 39 lower and 0 upper outliers in type_improvement_resolution_rate\n",
      "Filled 564 missing values in priority_minor_type_new_feature_count with median: 6.0000\n",
      "Capped 0 lower and 58 upper outliers in priority_minor_type_new_feature_count\n",
      "Filled 564 missing values in priority_minor_type_new_feature_avg_resolution_hours with median: 5050.6199\n",
      "Capped 0 lower and 34 upper outliers in priority_minor_type_new_feature_avg_resolution_hours\n",
      "Filled 786 missing values in priority_critical_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 18 upper outliers in priority_critical_type_new_feature_count\n",
      "Filled 786 missing values in priority_critical_type_new_feature_avg_resolution_hours with median: 3163.5037\n",
      "Capped 0 lower and 17 upper outliers in priority_critical_type_new_feature_avg_resolution_hours\n",
      "Filled 862 missing values in priority_medium_count with median: 83.0000\n",
      "Capped 0 lower and 13 upper outliers in priority_medium_count\n",
      "Filled 862 missing values in priority_medium_pct with median: 20.9091\n",
      "Filled 853 missing values in priority_high_count with median: 48.5000\n",
      "Capped 0 lower and 14 upper outliers in priority_high_count\n",
      "Filled 853 missing values in priority_high_pct with median: 9.1847\n",
      "Capped 0 lower and 4 upper outliers in priority_high_pct\n",
      "Filled 861 missing values in priority_low_count with median: 23.0000\n",
      "Capped 0 lower and 24 upper outliers in priority_low_count\n",
      "Filled 861 missing values in priority_low_pct with median: 5.8824\n",
      "Capped 0 lower and 2 upper outliers in priority_low_pct\n",
      "Filled 879 missing values in type_documentation_count with median: 6.5000\n",
      "Capped 0 lower and 12 upper outliers in type_documentation_count\n",
      "Filled 879 missing values in type_documentation_pct with median: 0.8067\n",
      "Capped 0 lower and 6 upper outliers in type_documentation_pct\n",
      "Filled 889 missing values in priority_medium_type_bug_count with median: 31.5000\n",
      "Capped 0 lower and 14 upper outliers in priority_medium_type_bug_count\n",
      "Filled 889 missing values in priority_medium_type_bug_avg_resolution_hours with median: 1549.7276\n",
      "Capped 0 lower and 4 upper outliers in priority_medium_type_bug_avg_resolution_hours\n",
      "Filled 915 missing values in priority_medium_type_task_count with median: 10.0000\n",
      "Capped 0 lower and 6 upper outliers in priority_medium_type_task_count\n",
      "Filled 915 missing values in priority_medium_type_task_avg_resolution_hours with median: 1238.7529\n",
      "Capped 0 lower and 2 upper outliers in priority_medium_type_task_avg_resolution_hours\n",
      "Filled 892 missing values in priority_high_type_bug_count with median: 14.0000\n",
      "Capped 0 lower and 10 upper outliers in priority_high_type_bug_count\n",
      "Filled 892 missing values in priority_high_type_bug_avg_resolution_hours with median: 1072.9892\n",
      "Capped 0 lower and 2 upper outliers in priority_high_type_bug_avg_resolution_hours\n",
      "Filled 919 missing values in priority_high_type_task_count with median: 7.5000\n",
      "Capped 0 lower and 5 upper outliers in priority_high_type_task_count\n",
      "Filled 919 missing values in priority_high_type_task_avg_resolution_hours with median: 1167.7849\n",
      "Capped 0 lower and 3 upper outliers in priority_high_type_task_avg_resolution_hours\n",
      "Filled 908 missing values in priority_low_type_bug_count with median: 24.0000\n",
      "Capped 0 lower and 11 upper outliers in priority_low_type_bug_count\n",
      "Filled 908 missing values in priority_low_type_bug_avg_resolution_hours with median: 2982.1633\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_bug_avg_resolution_hours\n",
      "Filled 933 missing values in priority_low_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 5 upper outliers in priority_low_type_task_count\n",
      "Filled 933 missing values in priority_low_type_task_avg_resolution_hours with median: 1652.2908\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_task_avg_resolution_hours\n",
      "Filled 879 missing values in type_documentation_resolution_rate with median: 77.7971\n",
      "Capped 5 lower and 0 upper outliers in type_documentation_resolution_rate\n",
      "Filled 956 missing values in priority_high_type_improvement_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_improvement_count\n",
      "Filled 956 missing values in priority_high_type_improvement_avg_resolution_hours with median: 330.3733\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_improvement_avg_resolution_hours\n",
      "Filled 960 missing values in priority_high_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_new_feature_count\n",
      "Filled 960 missing values in priority_high_type_new_feature_avg_resolution_hours with median: 2542.1649\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_new_feature_avg_resolution_hours\n",
      "Filled 961 missing values in priority_low_type_improvement_count with median: 10.0000\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_improvement_count\n",
      "Filled 961 missing values in priority_low_type_improvement_avg_resolution_hours with median: 2729.6553\n",
      "Filled 960 missing values in priority_low_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_low_type_new_feature_count\n",
      "Filled 960 missing values in priority_low_type_new_feature_avg_resolution_hours with median: 3787.2931\n",
      "Filled 873 missing values in type_question_count with median: 5.0000\n",
      "Capped 0 lower and 13 upper outliers in type_question_count\n",
      "Filled 873 missing values in type_question_pct with median: 0.8336\n",
      "Capped 0 lower and 10 upper outliers in type_question_pct\n",
      "Filled 873 missing values in type_question_resolution_rate with median: 77.7778\n",
      "Filled 946 missing values in priority_normal_count with median: 18.0000\n",
      "Capped 0 lower and 4 upper outliers in priority_normal_count\n",
      "Filled 946 missing values in priority_normal_pct with median: 12.4189\n",
      "Filled 955 missing values in priority_normal_type_bug_count with median: 5.0000\n",
      "Capped 0 lower and 3 upper outliers in priority_normal_type_bug_count\n",
      "Filled 955 missing values in priority_normal_type_bug_avg_resolution_hours with median: 520.7106\n",
      "Capped 0 lower and 1 upper outliers in priority_normal_type_bug_avg_resolution_hours\n",
      "\n",
      "=== SEPARATING FEATURES FROM TARGETS ===\n",
      "\n",
      "=== CREATING FEATURE PREPROCESSING PIPELINE ===\n",
      "Scaling features (target variables will remain unchanged)...\n",
      "\n",
      "=== SAVING RESULTS ===\n",
      "1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned.csv\n",
      "2. Saved feature preprocessor: jira_feature_preprocessor.pkl\n",
      "3. Saved scaled features with original targets: common_features_scaled_with_original_targets.csv\n",
      "\n",
      "=== TARGET VARIABLES (PRESERVED IN ORIGINAL UNITS) ===\n",
      "avg_resolution_hours: Mean = 3930.69, Median = 2614.45, Min = 0.01, Max = 11742.86\n",
      "median_resolution_hours: Mean = 668.63, Median = 402.17, Min = 0.00, Max = 2116.74\n",
      "min_resolution_hours: Mean = 0.02, Median = 0.01, Min = -0.04, Max = 0.07\n",
      "max_resolution_hours: Mean = 45387.55, Median = 36556.54, Min = 0.01, Max = 154639.88\n",
      "resolution_hours_std: Mean = 7299.05, Median = 5566.39, Min = 0.00, Max = 23406.80\n",
      "total_resolution_hours: Mean = 3205638.91, Median = 880454.55, Min = 0.01, Max = 11880816.27\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "results_dir = 'prepared_processed_data'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "\n",
    "# 2. Basic exploration\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "# 3. Check data types and identify non-numeric columns\n",
    "print(\"\\n=== DATA TYPE ANALYSIS ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "non_numeric_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nNon-numeric columns ({len(non_numeric_columns)}):\")\n",
    "for col in non_numeric_columns:\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  - {col}: {df[col].dtype}, {unique_values} unique values\")\n",
    "    if unique_values < 10:  # Show examples if not too many\n",
    "        print(f\"    Values: {df[col].unique()}\")\n",
    "\n",
    "# 4. Handle non-numeric columns\n",
    "print(\"\\n=== HANDLING NON-NUMERIC COLUMNS ===\")\n",
    "# Identify columns to drop (identifiers) and columns to encode (categorical)\n",
    "cols_to_drop = []\n",
    "cols_to_encode = []\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    # Check if it's an identifier column\n",
    "    if any(keyword in col.lower() for keyword in ['project', 'key', 'name', 'id', 'source', 'repository', 'file']):\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        # Must be a categorical column\n",
    "        cols_to_encode.append(col)\n",
    "\n",
    "# Drop identifier columns\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping identifier columns: {cols_to_drop}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "if cols_to_encode:\n",
    "    print(f\"Encoding categorical columns: {cols_to_encode}\")\n",
    "    # Use pandas get_dummies for one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)\n",
    "    print(f\"Expanded to {df.shape[1]} columns after encoding\")\n",
    "\n",
    "# 5. Function for interquartile-based imputation and outlier capping\n",
    "def impute_and_cap_using_iqr(df, columns=None, fill_missing=True, cap_outliers=True, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Impute missing values and cap outliers using interquartile range method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    columns : list or None\n",
    "        Columns to process. If None, will process all numeric columns.\n",
    "    fill_missing : bool\n",
    "        Whether to fill missing values with median\n",
    "    cap_outliers : bool\n",
    "        Whether to cap outliers based on IQR\n",
    "    iqr_multiplier : float\n",
    "        Multiplier for IQR to define outlier boundaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate quartiles and IQR\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define lower and upper bounds\n",
    "        lower_bound = q1 - iqr_multiplier * iqr\n",
    "        upper_bound = q3 + iqr_multiplier * iqr\n",
    "        \n",
    "        # Fill missing values with median if requested\n",
    "        if fill_missing and df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"Filled {df[col].isna().sum()} missing values in {col} with median: {median_val:.4f}\")\n",
    "        \n",
    "        # Cap outliers if requested\n",
    "        if cap_outliers:\n",
    "            # Count outliers before capping\n",
    "            n_lower_outliers = (df_clean[col] < lower_bound).sum()\n",
    "            n_upper_outliers = (df_clean[col] > upper_bound).sum()\n",
    "            \n",
    "            if n_lower_outliers > 0 or n_upper_outliers > 0:\n",
    "                # Apply capping\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"Capped {n_lower_outliers} lower and {n_upper_outliers} upper outliers in {col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 6. Identify potential target variables\n",
    "print(\"\\n=== IDENTIFYING TARGET VARIABLES ===\")\n",
    "potential_target_variables = [\n",
    "    'avg_resolution_hours',\n",
    "    'median_resolution_hours',\n",
    "    'min_resolution_hours',\n",
    "    'max_resolution_hours',\n",
    "    'resolution_hours_std',\n",
    "    'total_resolution_hours'\n",
    "]\n",
    "\n",
    "# Check if all target variables exist\n",
    "missing_targets = [var for var in potential_target_variables if var not in df.columns]\n",
    "if missing_targets:\n",
    "    print(f\"Warning: Some target variables are missing: {missing_targets}\")\n",
    "    # Update the list to only include existing columns\n",
    "    potential_target_variables = [var for var in potential_target_variables if var in df.columns]\n",
    "\n",
    "print(f\"Target variables: {potential_target_variables}\")\n",
    "\n",
    "# 7. Categorize features based on their type and expected distribution\n",
    "print(\"\\n=== CATEGORIZING FEATURES ===\")\n",
    "\n",
    "# Time-based features (excluding target variables)\n",
    "time_features = [\n",
    "    'resolution_time_p25', \n",
    "    'resolution_time_p75', \n",
    "    'resolution_time_p90', \n",
    "    'resolution_time_iqr', \n",
    "    'project_duration_days'\n",
    "]\n",
    "\n",
    "# Percentage features\n",
    "pct_features = [\n",
    "    'pct_resolved_within_24h', 'pct_resolved_within_week', 'pct_resolved_within_month',\n",
    "    'pct_issues_created_on_weekend', 'pct_issues_resolved_on_weekend',\n",
    "    'pct_resolved_issues', 'pct_issues_with_high_dependencies',\n",
    "    'priority_critical_pct', 'priority_blocker_pct', 'type_bug_pct', \n",
    "    'type_task_pct', 'type_new_feature_pct', 'type_epic_pct', \n",
    "    'type_improvement_pct', 'type_story_pct', 'type_documentation_pct',\n",
    "    'priority_high_pct', 'priority_low_pct'\n",
    "]\n",
    "\n",
    "# Count features\n",
    "count_features = [\n",
    "    'total_issues', 'max_issues_per_month', 'months_with_activity',\n",
    "    'priority_critical_count', 'type_bug_count', 'type_task_count', \n",
    "    'type_new_feature_count', 'priority_critical_type_bug_count',\n",
    "    'total_inward_links', 'total_outward_links', 'total_links',\n",
    "    'num_resolved_issues', 'priority_blocker_count', 'type_epic_count',\n",
    "    'priority_blocker_type_bug_count', 'type_improvement_count',\n",
    "    'type_story_count', 'priority_high_count', 'priority_low_count',\n",
    "    'type_documentation_count', 'priority_high_type_bug_count',\n",
    "    'priority_low_type_bug_count'\n",
    "]\n",
    "\n",
    "# Statistical/rate features\n",
    "stat_features = [\n",
    "    'resolution_time_skewness', 'resolution_time_kurtosis',\n",
    "    'avg_issues_per_month', 'issue_creation_volatility',\n",
    "    'resolution_rate_per_day', 'type_task_resolution_rate', \n",
    "    'type_bug_resolution_rate', 'type_new_feature_resolution_rate',\n",
    "    'weekly_efficiency_ratio', 'complexity_weighted_resolution_time',\n",
    "    'high_to_low_priority_ratio', 'bug_ratio', 'creation_resolution_balance',\n",
    "    'weighted_priority_score', 'issue_type_entropy', 'monthly_velocity',\n",
    "    'type_epic_resolution_rate', 'type_story_resolution_rate',\n",
    "    'type_improvement_resolution_rate', 'type_documentation_resolution_rate'\n",
    "]\n",
    "\n",
    "# Link-related features\n",
    "link_features = [\n",
    "    'avg_inward_links', 'avg_outward_links', 'avg_total_links',\n",
    "    'link_density'\n",
    "]\n",
    "\n",
    "# Resolution hour features - these will be excluded from scaling as they're related to target variables\n",
    "avg_resolution_features = [\n",
    "    'priority_critical_type_bug_avg_resolution_hours',\n",
    "    'priority_blocker_type_bug_avg_resolution_hours',\n",
    "    'priority_high_type_bug_avg_resolution_hours',\n",
    "    'priority_low_type_bug_avg_resolution_hours'\n",
    "]\n",
    "\n",
    "# Combine all feature categories\n",
    "feature_categories = {\n",
    "    'time_features': time_features,\n",
    "    'pct_features': pct_features,\n",
    "    'count_features': count_features,\n",
    "    'stat_features': stat_features,\n",
    "    'link_features': link_features,\n",
    "    'avg_resolution_features': avg_resolution_features\n",
    "}\n",
    "\n",
    "# Filter to keep only features that exist in the dataframe\n",
    "for category, features in feature_categories.items():\n",
    "    filtered_features = [f for f in features if f in df.columns]\n",
    "    if len(filtered_features) != len(features):\n",
    "        print(f\"Warning: Some features in {category} are missing from the dataset\")\n",
    "        print(f\"  Missing: {set(features) - set(filtered_features)}\")\n",
    "    feature_categories[category] = filtered_features\n",
    "\n",
    "# Get a flat list of all features being processed\n",
    "all_features_to_process = []\n",
    "for category, features in feature_categories.items():\n",
    "    all_features_to_process.extend(features)\n",
    "\n",
    "# 8. Apply interquartile-based imputation to all numeric features\n",
    "print(\"\\n=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\")\n",
    "numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "df_clean = impute_and_cap_using_iqr(df, columns=numeric_columns, fill_missing=True, cap_outliers=True)\n",
    "\n",
    "# 9. Separate features from targets\n",
    "print(\"\\n=== SEPARATING FEATURES FROM TARGETS ===\")\n",
    "# Include all columns except the target variables\n",
    "feature_columns = [col for col in df_clean.columns if col not in potential_target_variables]\n",
    "feature_df = df_clean[feature_columns]\n",
    "target_df = df_clean[potential_target_variables]\n",
    "\n",
    "# 10. Create the column transformer for feature scaling\n",
    "print(\"\\n=== CREATING FEATURE PREPROCESSING PIPELINE ===\")\n",
    "# Get the actual feature lists based on what's available in the dataframe\n",
    "time_feats = feature_categories['time_features']\n",
    "pct_feats = feature_categories['pct_features']\n",
    "count_feats = feature_categories['count_features']\n",
    "stat_feats = feature_categories['stat_features']\n",
    "link_feats = feature_categories['link_features']\n",
    "\n",
    "# Check if any feature categories are empty\n",
    "for category, features in {'time': time_feats, 'pct': pct_feats, 'count': count_feats, \n",
    "                          'stat': stat_feats, 'link': link_feats}.items():\n",
    "    if not features:\n",
    "        print(f\"Warning: No {category} features available for scaling\")\n",
    "\n",
    "# Create transformers only for non-empty feature lists\n",
    "transformers = []\n",
    "if time_feats:\n",
    "    transformers.append(('time_power', PowerTransformer(method='yeo-johnson'), time_feats))\n",
    "if pct_feats:\n",
    "    transformers.append(('pct_minmax', MinMaxScaler(), pct_feats))\n",
    "if count_feats:\n",
    "    transformers.append(('count_std', StandardScaler(), count_feats))\n",
    "if stat_feats:\n",
    "    transformers.append(('stat_robust', RobustScaler(), stat_feats))\n",
    "if link_feats:\n",
    "    transformers.append(('link_std', StandardScaler(), link_feats))\n",
    "\n",
    "feature_preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough'  # Non-numeric columns will pass through\n",
    ")\n",
    "\n",
    "# 11. Apply the preprocessing to features only\n",
    "print(\"Scaling features (target variables will remain unchanged)...\")\n",
    "# Check if feature_df has any data\n",
    "if feature_df.empty:\n",
    "    print(\"Error: No features available for scaling!\")\n",
    "else:\n",
    "    scaled_features = feature_preprocessor.fit_transform(feature_df)\n",
    "\n",
    "    # 12. Create DataFrame with scaled features\n",
    "    feature_names = feature_preprocessor.get_feature_names_out()\n",
    "    df_scaled_features = pd.DataFrame(scaled_features, columns=feature_names, index=df_clean.index)\n",
    "\n",
    "    # 13. Check if we've introduced any NaN values during scaling\n",
    "    nan_count = df_scaled_features.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: {nan_count} NaN values introduced during scaling\")\n",
    "        # Replace NaNs with 0\n",
    "        df_scaled_features = df_scaled_features.fillna(0)\n",
    "        print(\"NaN values have been replaced with 0\")\n",
    "\n",
    "    # 14. Combine scaled features with original target variables\n",
    "    final_df = pd.concat([df_scaled_features, target_df], axis=1)\n",
    "\n",
    "    # 15. Save the results\n",
    "    print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "    # Save the clean but unscaled dataset (after IQR imputation and outlier capping)\n",
    "    df_clean.to_csv(f'{results_dir}/common_features_iqr_cleaned.csv', index=False)\n",
    "    print(\"1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned.csv\")\n",
    "\n",
    "    # Save the preprocessor for later use\n",
    "    with open(f'{results_dir}/jira_feature_preprocessor.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_preprocessor, f)\n",
    "    print(\"2. Saved feature preprocessor: jira_feature_preprocessor.pkl\")\n",
    "\n",
    "    # Save the dataset with scaled features and original target variables\n",
    "    final_df.to_csv(f'{results_dir}/common_features_scaled_with_original_targets.csv', index=False)\n",
    "    print(\"3. Saved scaled features with original targets: common_features_scaled_with_original_targets.csv\")\n",
    "\n",
    "    # 16. Print information about target variables (not scaled)\n",
    "    print(\"\\n=== TARGET VARIABLES (PRESERVED IN ORIGINAL UNITS) ===\")\n",
    "    for target in potential_target_variables:\n",
    "        print(f\"{target}: Mean = {df_clean[target].mean():.2f}, Median = {df_clean[target].median():.2f}, Min = {df_clean[target].min():.2f}, Max = {df_clean[target].max():.2f}\")\n",
    "\n",
    "print(\"\\n=== PROCESSING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./prepared_processed_data/common_features_scaled_with_original_targets.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = 'resolution_hours_analysis'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. TARGET VARIABLE VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "target = 'total_resolution_hours'\n",
    "df['log_total_resolution_hours'] = np.log1p(df[target])\n",
    "\n",
    "# Create distribution plots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[target], kde=True, color='steelblue')\n",
    "plt.title('Distribution of Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['log_total_resolution_hours'], kde=True, color='forestgreen')\n",
    "plt.title('Distribution of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_distribution.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create boxplots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[target], color='steelblue')\n",
    "plt.title('Boxplot of Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['log_total_resolution_hours'], color='forestgreen')\n",
    "plt.title('Boxplot of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_boxplots.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. PLANNING-TIME FEATURES ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define features available at planning time\n",
    "planning_features = [\n",
    "    # Project scope indicators\n",
    "    'total_issues',                    # Estimated during planning\n",
    "    \n",
    "    # Project composition estimates\n",
    "    'priority_critical_pct',           # Expected critical issues\n",
    "    'priority_high_pct',               # Expected high priority issues\n",
    "    'priority_medium_pct',             # Expected medium priority issues\n",
    "    'priority_low_pct',                # Expected low priority issues\n",
    "    'priority_blocker_pct',            # Expected blocker issues\n",
    "    \n",
    "    # Issue type distribution (estimated from similar projects)\n",
    "    'type_bug_pct',                    # Expected bug percentage\n",
    "    'type_task_pct',                   # Expected task percentage\n",
    "    'type_new_feature_pct',            # Expected feature work\n",
    "    'type_improvement_pct',            # Expected improvements\n",
    "    'type_documentation_pct',          # Expected documentation work\n",
    "    \n",
    "    # Team composition\n",
    "    'team_size_creators',              # Planned team size\n",
    "    'team_size_assignees',             # Planned assignees\n",
    "    'team_size_combined',              # Overall team size\n",
    "    \n",
    "    # Complexity indicators\n",
    "    'weighted_priority_score',         # Expected priority complexity\n",
    "    'issue_type_entropy',              # Expected variety of issues\n",
    "    \n",
    "    # Historical indicators that could be estimated\n",
    "    'high_to_low_priority_ratio',      # Expected priority distribution\n",
    "    'bug_ratio',                       # Expected bug ratio\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataframe\n",
    "planning_features = [f for f in planning_features if f in df.columns]\n",
    "\n",
    "# Create a correlation matrix of planning features with the target\n",
    "planning_correlations = df[planning_features].corrwith(df['log_total_resolution_hours'])\n",
    "planning_correlations = planning_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=planning_correlations.values, y=planning_correlations.index, palette='viridis')\n",
    "plt.title('Planning-Time Features: Correlation with Log-Transformed Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/planning_feature_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plots for top features\n",
    "top_features = planning_correlations.abs().sort_values(ascending=False).head(6).index\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "               scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = valid_data[feature].corr(valid_data['log_total_resolution_hours'])\n",
    "    \n",
    "    plt.title(f'{feature} vs. Log-Transformed Resolution Hours', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=10)\n",
    "    plt.annotate(f'r = {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/top_feature_relationships.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. FEATURE CATEGORY ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Project Scope': ['total_issues', 'project_duration_days'],\n",
    "    'Priority Distribution': [col for col in planning_features if 'priority_' in col and '_pct' in col],\n",
    "    'Issue Types': [col for col in planning_features if 'type_' in col and '_pct' in col],\n",
    "    'Team Composition': [col for col in planning_features if 'team_' in col],\n",
    "    'Complexity Metrics': ['weighted_priority_score', 'issue_type_entropy', 'high_to_low_priority_ratio', 'bug_ratio']\n",
    "}\n",
    "\n",
    "# Calculate average correlation by category\n",
    "category_correlations = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Get features that exist in the dataframe\n",
    "    existing_features = [f for f in features if f in df.columns]\n",
    "    if existing_features:\n",
    "        # Get absolute correlations\n",
    "        abs_corrs = df[existing_features].corrwith(df['log_total_resolution_hours']).abs()\n",
    "        category_correlations[category] = abs_corrs.mean()\n",
    "\n",
    "# Visualize category correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = list(category_correlations.keys())\n",
    "correlations = list(category_correlations.values())\n",
    "\n",
    "# Sort by correlation\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "categories = [categories[i] for i in sorted_indices]\n",
    "correlations = [correlations[i] for i in sorted_indices]\n",
    "\n",
    "sns.barplot(x=categories, y=correlations, palette='viridis')\n",
    "plt.title('Average Correlation by Feature Category', fontsize=14)\n",
    "plt.xlabel('Feature Category', fontsize=12)\n",
    "plt.ylabel('Average Absolute Correlation', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/category_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. PRIORITY & ISSUE TYPE IMPACT VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create visualizations for priority distribution impact\n",
    "priority_features = [col for col in df.columns if 'priority_' in col and '_pct' in col]\n",
    "priority_features = [f for f in priority_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if priority_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(priority_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/priority_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations for issue type impact\n",
    "issue_type_features = [col for col in df.columns if 'type_' in col and '_pct' in col]\n",
    "issue_type_features = [f for f in issue_type_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if issue_type_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(issue_type_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/issue_type_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. MODEL TRAINING & FEATURE IMPORTANCE\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Prepare the data for modeling\n",
    "df_planning = df[planning_features + ['log_total_resolution_hours']].copy()\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_planning[planning_features] = imputer.fit_transform(df_planning[planning_features])\n",
    "\n",
    "# Check for and replace infinite values\n",
    "for col in planning_features:\n",
    "    mask = np.isinf(df_planning[col])\n",
    "    if mask.any():\n",
    "        print(f\"Replacing {mask.sum()} infinite values in {col}\")\n",
    "        df_planning.loc[mask, col] = df_planning[col].median()\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_planning[planning_features]\n",
    "y = df_planning['log_total_resolution_hours']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.4f}, R²: {rf_r2:.4f}\")\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': planning_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
    "plt.title('Feature Importance for Predicting Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/feature_importance.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. ACTUAL VS PREDICTED VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Log scale comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, rf_pred, alpha=0.6, c='steelblue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Actual vs Predicted: Log Scale', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Predicted Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.annotate(f'R² = {rf_r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "            fontsize=12, fontweight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Original scale comparison \n",
    "plt.subplot(1, 2, 2)\n",
    "y_test_original = np.expm1(y_test)\n",
    "rf_pred_original = np.expm1(rf_pred)\n",
    "\n",
    "plt.scatter(y_test_original, rf_pred_original, alpha=0.6, c='forestgreen')\n",
    "plt.title('Actual vs Predicted: Original Scale', fontsize=14)\n",
    "plt.xlabel('Actual Total Resolution Hours', fontsize=12) \n",
    "plt.ylabel('Predicted Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Draw reference line\n",
    "max_val = max(y_test_original.max(), rf_pred_original.max())\n",
    "plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/actual_vs_predicted.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. ERROR ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = y_test - rf_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "# Create error distribution plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(errors, kde=True, color='steelblue')\n",
    "plt.title('Distribution of Prediction Errors (Log Scale)', fontsize=14)\n",
    "plt.xlabel('Prediction Error', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=y_test, y=abs_errors, alpha=0.6, color='forestgreen')\n",
    "plt.title('Error Magnitude vs Actual Value', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/error_analysis.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. SUMMARY HEATMAP\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a correlation heatmap of top planning features\n",
    "top_planning_features = feature_importances.head(10)['Feature'].tolist()\n",
    "correlation_matrix = df[top_planning_features + ['log_total_resolution_hours']].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f', \n",
    "           linewidths=0.5, mask=mask)\n",
    "plt.title('Correlation Matrix of Top Planning Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/correlation_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations saved to {viz_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (971, 185)\n",
      "\n",
      "Missing values per feature:\n",
      "total_issues                    0\n",
      "priority_critical_pct         264\n",
      "priority_high_pct             853\n",
      "priority_medium_pct           862\n",
      "priority_low_pct              861\n",
      "priority_blocker_pct          268\n",
      "type_bug_pct                   34\n",
      "type_task_pct                  96\n",
      "type_new_feature_pct          362\n",
      "type_improvement_pct          330\n",
      "type_documentation_pct        879\n",
      "team_size_creators            520\n",
      "team_size_assignees           520\n",
      "team_size_combined            520\n",
      "weighted_priority_score        86\n",
      "issue_type_entropy              0\n",
      "high_to_low_priority_ratio    165\n",
      "bug_ratio                      34\n",
      "log_total_resolution_hours      0\n",
      "total_resolution_hours          0\n",
      "dtype: int64\n",
      "\n",
      "Ridge Regression Performance:\n",
      "Log Space - RMSE: 2.0177, MAE: 1.4385, R²: 0.5136\n",
      "Error Distribution - Mean: -0.1481, Std: 2.0123\n",
      "Error Distribution - Skew: -1.1559, Kurtosis: 2.7661\n",
      "Original Scale - RMSE: 43125715753.6040, MAE: 3093375527.3487\n",
      "Percentage Errors - MAPE: 6592.04%, SMAPE: 98.01%\n",
      "\n",
      "Random Forest Performance:\n",
      "Log Space - RMSE: 1.2955, MAE: 0.8492, R²: 0.7995\n",
      "Error Distribution - Mean: 0.2597, Std: 1.2692\n",
      "Error Distribution - Skew: 0.5130, Kurtosis: 6.6187\n",
      "Original Scale - RMSE: 9580605.5934, MAE: 2921367.4735\n",
      "Percentage Errors - MAPE: 285.37%, SMAPE: 66.69%\n",
      "\n",
      "RMSE/MAE Ratio Analysis:\n",
      "Ridge - Log Space: 1.40, Original Space: 13.94\n",
      "RF - Log Space: 1.53, Original Space: 3.28\n",
      "Note: Ratio closer to 1 indicates more uniform error distribution\n",
      "      Higher ratio indicates presence of larger errors (RMSE more sensitive)\n",
      "\n",
      "=== METRIC SELECTION RECOMMENDATIONS ===\n",
      "Based on the error distribution analysis and the paper by Chai & Draxler (2014):\n",
      "\n",
      "Normality Test (Shapiro-Wilk):\n",
      "Ridge Model: p-value = 0.000000 (Non-normal distribution)\n",
      "Random Forest Model: p-value = 0.000000 (Non-normal distribution)\n",
      "\n",
      "Recommended Metrics:\n",
      "- For Ridge Model: MAE may be more appropriate (errors don't follow normal distribution)\n",
      "- For Random Forest Model: MAE may be more appropriate (errors don't follow normal distribution)\n",
      "\n",
      "Overall Recommendation:\n",
      "- Use multiple metrics (RMSE, MAE, and R²) to provide a complete picture\n",
      "- Report RMSE/MAE ratio to give insight into error distribution\n",
      "- Consider log-space metrics for model comparison (more stable distribution)\n",
      "- Use original-space metrics for practical interpretation of results\n",
      "\n",
      "Analysis complete! All visualizations saved to enhanced_model_evaluation/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 08:23:51,359 - INFO     - Executing shutdown due to inactivity...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = 'resolution_hours_analysis_enhanced'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. TARGET VARIABLE VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "target = 'total_resolution_hours'\n",
    "df['log_total_resolution_hours'] = np.log1p(df[target])\n",
    "\n",
    "# Create distribution plots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[target], kde=True, color='steelblue')\n",
    "plt.title('Distribution of Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['log_total_resolution_hours'], kde=True, color='forestgreen')\n",
    "plt.title('Distribution of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_distribution.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create boxplots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[target], color='steelblue')\n",
    "plt.title('Boxplot of Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['log_total_resolution_hours'], color='forestgreen')\n",
    "plt.title('Boxplot of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_boxplots.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. PLANNING-TIME FEATURES ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define features available at planning time\n",
    "planning_features = [\n",
    "    # Project scope indicators\n",
    "    'total_issues',                    # Estimated during planning\n",
    "    \n",
    "    # Project composition estimates\n",
    "    'priority_critical_pct',           # Expected critical issues\n",
    "    'priority_high_pct',               # Expected high priority issues\n",
    "    'priority_medium_pct',             # Expected medium priority issues\n",
    "    'priority_low_pct',                # Expected low priority issues\n",
    "    'priority_blocker_pct',            # Expected blocker issues\n",
    "    \n",
    "    # Issue type distribution (estimated from similar projects)\n",
    "    'type_bug_pct',                    # Expected bug percentage\n",
    "    'type_task_pct',                   # Expected task percentage\n",
    "    'type_new_feature_pct',            # Expected feature work\n",
    "    'type_improvement_pct',            # Expected improvements\n",
    "    'type_documentation_pct',          # Expected documentation work\n",
    "    \n",
    "    # Team composition\n",
    "    'team_size_creators',              # Planned team size\n",
    "    'team_size_assignees',             # Planned assignees\n",
    "    'team_size_combined',              # Overall team size\n",
    "    \n",
    "    # Complexity indicators\n",
    "    'weighted_priority_score',         # Expected priority complexity\n",
    "    'issue_type_entropy',              # Expected variety of issues\n",
    "    \n",
    "    # Historical indicators that could be estimated\n",
    "    'high_to_low_priority_ratio',      # Expected priority distribution\n",
    "    'bug_ratio',                       # Expected bug ratio\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataframe\n",
    "planning_features = [f for f in planning_features if f in df.columns]\n",
    "\n",
    "# Create a correlation matrix of planning features with the target\n",
    "planning_correlations = df[planning_features].corrwith(df['log_total_resolution_hours'])\n",
    "planning_correlations = planning_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=planning_correlations.values, y=planning_correlations.index, palette='viridis')\n",
    "plt.title('Planning-Time Features: Correlation with Log-Transformed Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/planning_feature_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plots for top features\n",
    "top_features = planning_correlations.abs().sort_values(ascending=False).head(6).index\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "               scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = valid_data[feature].corr(valid_data['log_total_resolution_hours'])\n",
    "    \n",
    "    plt.title(f'{feature} vs. Log-Transformed Resolution Hours', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=10)\n",
    "    plt.annotate(f'r = {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/top_feature_relationships.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. FEATURE CATEGORY ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Project Scope': ['total_issues', 'project_duration_days'],\n",
    "    'Priority Distribution': [col for col in planning_features if 'priority_' in col and '_pct' in col],\n",
    "    'Issue Types': [col for col in planning_features if 'type_' in col and '_pct' in col],\n",
    "    'Team Composition': [col for col in planning_features if 'team_' in col],\n",
    "    'Complexity Metrics': ['weighted_priority_score', 'issue_type_entropy', 'high_to_low_priority_ratio', 'bug_ratio']\n",
    "}\n",
    "\n",
    "# Calculate average correlation by category\n",
    "category_correlations = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Get features that exist in the dataframe\n",
    "    existing_features = [f for f in features if f in df.columns]\n",
    "    if existing_features:\n",
    "        # Get absolute correlations\n",
    "        abs_corrs = df[existing_features].corrwith(df['log_total_resolution_hours']).abs()\n",
    "        category_correlations[category] = abs_corrs.mean()\n",
    "\n",
    "# Visualize category correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = list(category_correlations.keys())\n",
    "correlations = list(category_correlations.values())\n",
    "\n",
    "# Sort by correlation\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "categories = [categories[i] for i in sorted_indices]\n",
    "correlations = [correlations[i] for i in sorted_indices]\n",
    "\n",
    "sns.barplot(x=categories, y=correlations, palette='viridis')\n",
    "plt.title('Average Correlation by Feature Category', fontsize=14)\n",
    "plt.xlabel('Feature Category', fontsize=12)\n",
    "plt.ylabel('Average Absolute Correlation', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/category_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. PRIORITY & ISSUE TYPE IMPACT VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create visualizations for priority distribution impact\n",
    "priority_features = [col for col in df.columns if 'priority_' in col and '_pct' in col]\n",
    "priority_features = [f for f in priority_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if priority_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(priority_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/priority_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations for issue type impact\n",
    "issue_type_features = [col for col in df.columns if 'type_' in col and '_pct' in col]\n",
    "issue_type_features = [f for f in issue_type_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if issue_type_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(issue_type_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/issue_type_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. MODEL TRAINING WITH HYPERPARAMETER TUNING\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Prepare the data for modeling\n",
    "df_planning = df[planning_features + ['log_total_resolution_hours']].copy()\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_planning[planning_features] = imputer.fit_transform(df_planning[planning_features])\n",
    "\n",
    "# Check for and replace infinite values\n",
    "for col in planning_features:\n",
    "    mask = np.isinf(df_planning[col])\n",
    "    if mask.any():\n",
    "        print(f\"Replacing {mask.sum()} infinite values in {col}\")\n",
    "        df_planning.loc[mask, col] = df_planning[col].median()\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_planning[planning_features]\n",
    "y = df_planning['log_total_resolution_hours']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create Random Forest base model\n",
    "rf_base = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "print(\"Starting hyperparameter tuning with GridSearchCV...\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Use all available cores\n",
    "    verbose=1,                 # Show progress\n",
    "    scoring='neg_root_mean_squared_error'  # Optimize for RMSE\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "rf_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Tuned Random Forest - RMSE: {rf_rmse:.4f}, R²: {rf_r2:.4f}\")\n",
    "\n",
    "# Compare with baseline model (original implementation)\n",
    "baseline_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "baseline_pred = baseline_rf.predict(X_test)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
    "baseline_r2 = r2_score(y_test, baseline_pred)\n",
    "\n",
    "print(f\"Baseline Random Forest - RMSE: {baseline_rmse:.4f}, R²: {baseline_r2:.4f}\")\n",
    "print(f\"Improvement in RMSE: {baseline_rmse - rf_rmse:.4f} ({(baseline_rmse - rf_rmse) / baseline_rmse * 100:.2f}%)\")\n",
    "\n",
    "# Get feature importances from the tuned model\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': planning_features,\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. ACTUAL VS PREDICTED VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Log scale comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, rf_pred, alpha=0.6, c='steelblue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Actual vs Predicted: Log Scale', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Predicted Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.annotate(f'R² = {rf_r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "            fontsize=12, fontweight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Original scale comparison \n",
    "plt.subplot(1, 2, 2)\n",
    "y_test_original = np.expm1(y_test)\n",
    "rf_pred_original = np.expm1(rf_pred)\n",
    "\n",
    "plt.scatter(y_test_original, rf_pred_original, alpha=0.6, c='forestgreen')\n",
    "plt.title('Actual vs Predicted: Original Scale', fontsize=14)\n",
    "plt.xlabel('Actual Total Resolution Hours', fontsize=12) \n",
    "plt.ylabel('Predicted Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Draw reference line\n",
    "max_val = max(y_test_original.max(), rf_pred_original.max())\n",
    "plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/actual_vs_predicted.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. ERROR ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = y_test - rf_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "# Create error distribution plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(errors, kde=True, color='steelblue')\n",
    "plt.title('Distribution of Prediction Errors (Log Scale)', fontsize=14)\n",
    "plt.xlabel('Prediction Error', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=y_test, y=abs_errors, alpha=0.6, color='forestgreen')\n",
    "plt.title('Error Magnitude vs Actual Value', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/error_analysis.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. SUMMARY HEATMAP\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a correlation heatmap of top planning features\n",
    "top_planning_features = feature_importances.head(10)['Feature'].tolist()\n",
    "correlation_matrix = df[top_planning_features + ['log_total_resolution_hours']].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f', \n",
    "           linewidths=0.5, mask=mask)\n",
    "plt.title('Correlation Matrix of Top Planning Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/correlation_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations saved to {viz_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
