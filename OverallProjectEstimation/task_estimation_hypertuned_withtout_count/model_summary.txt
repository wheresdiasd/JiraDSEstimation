# Resolution Hours Ensemble Model with Hyperparameter Tuning

## Performance Comparison

| Model | Metric | Original | Tuned | Improvement (%) |
|-------|--------|----------|-------|----------------|
| Random_Forest | MAE | 699979.6846 | 9635.5945 | 98.62 |
| Random_Forest | RMSE | 1018064.3325 | 54589.0042 | 94.64 |
| Random_Forest | R2 | 0.9539 | 0.9999 | 4.81 |
| Random_Forest | Spearman | 0.9301 | 0.9415 | 1.22 |
| Gradient_Boosting | MAE | 488155.8726 | 54871.5652 | 88.76 |
| Gradient_Boosting | RMSE | 780393.4770 | 112715.7566 | 85.56 |
| Gradient_Boosting | R2 | 0.9729 | 0.9994 | 2.72 |
| Gradient_Boosting | Spearman | 0.9328 | 0.9410 | 0.88 |
| XGBoost | MAE | 536858.7490 | 117453.3826 | 78.12 |
| XGBoost | RMSE | 843225.5996 | 231777.5926 | 72.51 |
| XGBoost | R2 | 0.9684 | 0.9976 | 3.02 |
| XGBoost | Spearman | 0.9306 | 0.9399 | 1.01 |
| Ensemble | MAE | 559290.8744 | 56795.6036 | 89.85 |
| Ensemble | RMSE | 845587.2583 | 113766.2377 | 86.55 |
| Ensemble | R2 | 0.9682 | 0.9994 | 3.22 |
| Ensemble | Spearman | 0.9322 | 0.9410 | 0.94 |

## Hyperparameter Changes

### Random_Forest

| Parameter | Original | Tuned |
|-----------|----------|-------|
| n_estimators | 100 | 50 |
| min_samples_leaf | 5 | 1 |
| max_features | sqrt | sqrt |
| max_depth | 10 | None |

### Gradient_Boosting

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 1.0 | 0.8 |
| n_estimators | 100 | 100 |
| min_samples_leaf | 5 | 5 |
| max_depth | 5 | 7 |
| learning_rate | 0.05 | 0.1 |

### XGBoost

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 0.8 | 1.0 |
| n_estimators | 100 | 200 |
| min_child_weight | 5 | 3 |
| max_depth | 5 | 5 |
| learning_rate | 0.05 | 0.1 |
| colsample_bytree | 0.8 | 0.8 |


## Conclusion

Hyperparameter tuning resulted in an average MAE improvement of 88.84% and an average RÂ² improvement of 3.44%.

Before tuning, Gradient_Boosting was the best performing model. After tuning, Random_Forest became the best performing model.
