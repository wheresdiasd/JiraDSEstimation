{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset...\n",
      "Dataset shape: (711, 87)\n",
      "Warning: Found 7 non-numeric columns that need handling: ['remainder__project_key', 'remainder__project_name', 'remainder__project_start_date', 'remainder__project_latest_resolved_date', 'remainder__project_latest_update_date', 'remainder__repository', 'remainder__source_file']\n",
      "Selected primary target variable: avg_resolution_hours\n",
      "Removing 4 features to prevent data leakage:\n",
      "  - remainder__priority_critical_type_bug_avg_resolution_hours\n",
      "  - remainder__priority_blocker_type_bug_avg_resolution_hours\n",
      "  - remainder__priority_high_type_bug_avg_resolution_hours\n",
      "  - remainder__priority_low_type_bug_avg_resolution_hours\n",
      "Features shape: (711, 77)\n",
      "Target shape: (711,)\n",
      "Data partitioning complete:\n",
      "  Training set: 355 samples (49.9%)\n",
      "  Validation set: 178 samples (25.0%)\n",
      "  Test set: 178 samples (25.0%)\n",
      "\n",
      "Target variable statistics:\n",
      "  Training: mean=3924.86, median=2846.57, min=0.63, max=10937.65\n",
      "  Validation: mean=3885.21, median=2713.98, min=1.37, max=10937.65\n",
      "  Test: mean=3588.97, median=2499.94, min=0.14, max=10937.65\n",
      "\n",
      "Initial setup complete. Ready for feature selection and model training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directory for results\n",
    "results_dir = 'model_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load the preprocessed dataset with scaled features and original targets\n",
    "print(\"Loading preprocessed dataset...\")\n",
    "df = pd.read_csv('../../common_features_scaled_with_original_targets.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# 2. Check for any remaining non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"Warning: Found {len(non_numeric_cols)} non-numeric columns that need handling: {non_numeric_cols}\")\n",
    "else:\n",
    "    print(\"All columns are numeric - good to proceed.\")\n",
    "\n",
    "# 3. Define potential target variables\n",
    "target_variables = [\n",
    "    'avg_resolution_hours',\n",
    "    'median_resolution_hours',\n",
    "    'min_resolution_hours',\n",
    "    'max_resolution_hours', \n",
    "    'resolution_hours_std',\n",
    "    'total_resolution_hours'\n",
    "]\n",
    "\n",
    "# Check which target variables exist in our dataset\n",
    "available_targets = [target for target in target_variables if target in df.columns]\n",
    "if len(available_targets) < len(target_variables):\n",
    "    print(f\"Note: Some target variables are not in the dataset. Available targets: {available_targets}\")\n",
    "\n",
    "# 4. Select primary target variable for prediction\n",
    "primary_target = 'avg_resolution_hours'\n",
    "if primary_target not in df.columns:\n",
    "    print(f\"Warning: Primary target '{primary_target}' not found in dataset!\")\n",
    "    if available_targets:\n",
    "        primary_target = available_targets[0]\n",
    "        print(f\"Using '{primary_target}' as alternative target.\")\n",
    "    else:\n",
    "        raise ValueError(\"No suitable target variable found in dataset!\")\n",
    "\n",
    "print(f\"Selected primary target variable: {primary_target}\")\n",
    "\n",
    "# 5. Check for potential data leakage features more thoroughly\n",
    "leakage_features = []\n",
    "\n",
    "# Look for features containing terms related to what we're trying to predict\n",
    "leakage_terms = ['resolution_hours', 'resolution_time', 'hours_', 'time_spent']\n",
    "\n",
    "for col in X.columns:  # Check only in features, not in target_variables\n",
    "    # Check if the column contains any of the leakage terms\n",
    "    if any(term in col.lower() for term in leakage_terms):\n",
    "        leakage_features.append(col)\n",
    "    \n",
    "    # Special check for features that are highly likely to leak information\n",
    "    if 'avg_resolution' in col or 'median_resolution' in col or 'total_resolution' in col:\n",
    "        if col not in leakage_features:  # Avoid duplicates\n",
    "            leakage_features.append(col)\n",
    "\n",
    "if leakage_features:\n",
    "    print(f\"Removing {len(leakage_features)} features to prevent data leakage:\")\n",
    "    for feature in leakage_features:\n",
    "        print(f\"  - {feature}\")\n",
    "    \n",
    "    # Remove these features\n",
    "    X = X.drop(columns=leakage_features)\n",
    "    \n",
    "    # Now redefine train/val/test with the updated X\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Features shape after removing leakage: {X.shape}\")\n",
    "else:\n",
    "    print(\"No potential data leakage features detected.\")\n",
    "\n",
    "# 6. Separate features and target\n",
    "X = df.drop(columns=[col for col in target_variables if col in df.columns])\n",
    "y = df[primary_target]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# 7. Data partitioning: 50% training, 25% validation, 25% testing\n",
    "# First split: 50% training, 50% remaining\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Second split: divide the remaining 50% into equal parts for validation and testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Data partitioning complete:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# 8. Basic exploratory analysis of the target variable\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Training set distribution\n",
    "plt.subplot(131)\n",
    "plt.hist(y_train, bins=30, alpha=0.7)\n",
    "plt.title('Training Target Distribution')\n",
    "plt.xlabel(primary_target)\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Validation set distribution\n",
    "plt.subplot(132)\n",
    "plt.hist(y_val, bins=30, alpha=0.7)\n",
    "plt.title('Validation Target Distribution')\n",
    "plt.xlabel(primary_target)\n",
    "\n",
    "# Test set distribution\n",
    "plt.subplot(133)\n",
    "plt.hist(y_test, bins=30, alpha=0.7)\n",
    "plt.title('Test Target Distribution')\n",
    "plt.xlabel(primary_target)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/target_distributions.png')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"  Training: mean={y_train.mean():.2f}, median={y_train.median():.2f}, min={y_train.min():.2f}, max={y_train.max():.2f}\")\n",
    "print(f\"  Validation: mean={y_val.mean():.2f}, median={y_val.median():.2f}, min={y_val.min():.2f}, max={y_val.max():.2f}\")\n",
    "print(f\"  Test: mean={y_test.mean():.2f}, median={y_test.median():.2f}, min={y_test.min():.2f}, max={y_test.max():.2f}\")\n",
    "\n",
    "# Save the splits for reproducibility\n",
    "splits = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(splits, f)\n",
    "\n",
    "print(\"\\nInitial setup complete. Ready for feature selection and model training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
