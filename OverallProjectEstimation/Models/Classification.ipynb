{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== PROJECT CLASSIFICATION PIPELINE ==========\n",
      "\n",
      "Loaded ../DataSets/data_export_1741772203780.csv: 190 rows, 100 columns\n",
      "Loaded ../DataSets/data_export_1741699774916.csv: 159 rows, 100 columns\n",
      "Combined dataset: 349 rows, 165 columns\n",
      "Starting data cleaning...\n",
      "Filling 159 missing values in issuetype.name_RTC with median\n",
      "Filling 159 missing values in issuetype.name_Outage with median\n",
      "Filling 159 missing values in issuetype.name_SVN->GIT Mirroring with median\n",
      "Filling 159 missing values in issuetype.name_Umbrella with median\n",
      "Filling 159 missing values in issuetype.name_Wish with median\n",
      "Filling 159 missing values in issuetype.name_Planned Work with median\n",
      "Filling 159 missing values in issuetype.name_New JIRA Project with median\n",
      "Filling 159 missing values in issuetype.name_New Git Repo with median\n",
      "Filling 159 missing values in issuetype.name_New Confluence Wiki with median\n",
      "Filling 159 missing values in issuetype.name_Temp with median\n",
      "Filling 159 missing values in issuetype.name_New TLP with median\n",
      "Filling 159 missing values in issuetype.name_New TLP - Common Tasks with median\n",
      "Filling 159 missing values in issuetype.name_Project with median\n",
      "Filling 159 missing values in issuetype.name_Dependency with median\n",
      "Filling 159 missing values in issuetype.name_Test with median\n",
      "Filling 159 missing values in issuetype.name_Comment with median\n",
      "Filling 159 missing values in issuetype.name_Pending Review with median\n",
      "Filling 159 missing values in issuetype.name_Blogs - Access to Existing Blog with median\n",
      "Filling 159 missing values in issuetype.name_Dependency upgrade with median\n",
      "Filling 159 missing values in issuetype.name_GitBox Request with median\n",
      "Filling 159 missing values in issuetype.name_Blogs - New Blog User Account Request with median\n",
      "Filling 159 missing values in issuetype.name_Github Integration with median\n",
      "Filling 159 missing values in issuetype.name_New Bugzilla Project with median\n",
      "Filling 159 missing values in issuetype.name_Proposal with median\n",
      "Filling 159 missing values in issuetype.name_Brainstorming with median\n",
      "Filling 159 missing values in issuetype.name_Blog - New Blog Request with median\n",
      "Filling 159 missing values in issuetype.name_SVN->GIT Migration with median\n",
      "Filling 159 missing values in issuetype.name_Suitable Name Search with median\n",
      "Filling 159 missing values in issuetype.name_Request with median\n",
      "Filling 159 missing values in issuetype.name_TCK Challenge with median\n",
      "Filling 159 missing values in priority.name_Missing with median\n",
      "Filling 159 missing values in priority.name_Normal with median\n",
      "Filling 159 missing values in priority.name_P3 with median\n",
      "Filling 159 missing values in priority.name_P0 with median\n",
      "Filling 159 missing values in priority.name_P4 with median\n",
      "Filling 159 missing values in priority.name_Low with median\n",
      "Filling 159 missing values in priority.name_P1 with median\n",
      "Filling 159 missing values in priority.name_Critical with median\n",
      "Filling 159 missing values in priority.name_P2 with median\n",
      "Filling 159 missing values in priority.name_Urgent with median\n",
      "Filling 159 missing values in priority.name_Major with median\n",
      "Filling 159 missing values in priority.name_Trivial with median\n",
      "Filling 159 missing values in priority.name_Blocker with median\n",
      "Filling 159 missing values in priority.name_Minor with median\n",
      "Filling 159 missing values in priority.name_High with median\n",
      "Filling 159 missing values in priority.name_Not a Priority with median\n",
      "Filling 159 missing values in status.name_Accepted with median\n",
      "Filling 159 missing values in status.name_Done with median\n",
      "Filling 159 missing values in status.name_Closed with median\n",
      "Filling 159 missing values in status.name_Open with median\n",
      "Filling 159 missing values in status.name_Patch Available with median\n",
      "Filling 159 missing values in status.name_Waiting for Infra with median\n",
      "Filling 159 missing values in status.name_Resolved with median\n",
      "Filling 159 missing values in status.name_Reopened with median\n",
      "Filling 159 missing values in created_month_1.0 with median\n",
      "Filling 159 missing values in created_month_2.0 with median\n",
      "Filling 159 missing values in created_month_3.0 with median\n",
      "Filling 159 missing values in created_month_4.0 with median\n",
      "Filling 159 missing values in created_month_5.0 with median\n",
      "Filling 159 missing values in created_month_6.0 with median\n",
      "Filling 159 missing values in created_month_7.0 with median\n",
      "Filling 159 missing values in created_month_8.0 with median\n",
      "Filling 159 missing values in created_month_9.0 with median\n",
      "Filling 159 missing values in created_month_10.0 with median\n",
      "Filling 159 missing values in created_month_11.0 with median\n",
      "Filling 190 missing values in issuetype.name_Enhancement with median\n",
      "Filling 190 missing values in issuetype.name_Analysis with median\n",
      "Filling 190 missing values in issuetype.name_Problem with median\n",
      "Filling 190 missing values in issuetype.name_Typo with median\n",
      "Filling 190 missing values in issuetype.name_Problem Ticket with median\n",
      "Filling 190 missing values in issuetype.name_Backport with median\n",
      "Filling 190 missing values in issuetype.name_Technical Requirement with median\n",
      "Filling 190 missing values in issuetype.name_Support Request with median\n",
      "Filling 190 missing values in issuetype.name_Public Security Vulnerability with median\n",
      "Filling 190 missing values in issuetype.name_Third-party issue with median\n",
      "Filling 190 missing values in issuetype.name_Clarification with median\n",
      "Filling 190 missing values in issuetype.name_Plugin Release Request with median\n",
      "Filling 190 missing values in issuetype.name_Dev Task with median\n",
      "Filling 190 missing values in issuetype.name_Spec Change with median\n",
      "Filling 190 missing values in issuetype.name_Feedback with median\n",
      "Filling 190 missing values in issuetype.name_A/B Test with median\n",
      "Filling 190 missing values in issuetype.name_Tracker with median\n",
      "Filling 190 missing values in issuetype.name_Spike with median\n",
      "Filling 190 missing values in issuetype.name_Ask a question with median\n",
      "Filling 190 missing values in issuetype.name_Risk with median\n",
      "Filling 190 missing values in issuetype.name_Support Patch with median\n",
      "Filling 190 missing values in issuetype.name_Market Problem with median\n",
      "Filling 190 missing values in issuetype.name_RFE with median\n",
      "Filling 190 missing values in issuetype.name_Feature Request with median\n",
      "Filling 190 missing values in issuetype.name_Component Upgrade Subtask with median\n",
      "Filling 190 missing values in issuetype.name_Infrastructure with median\n",
      "Filling 190 missing values in issuetype.name_Business Requirement with median\n",
      "Filling 190 missing values in issuetype.name_Feature with median\n",
      "Filling 190 missing values in issuetype.name_Incident with median\n",
      "Filling 190 missing values in issuetype.name_Build Failure with median\n",
      "Filling 190 missing values in issuetype.name_Suggestion with median\n",
      "Filling 190 missing values in issuetype.name_Investigation with median\n",
      "Filling 190 missing values in issuetype.name_Patch with median\n",
      "Filling 190 missing values in issuetype.name_Docs Task with median\n",
      "Filling 190 missing values in issuetype.name_Atlassian Incident with median\n",
      "Filling 190 missing values in issuetype.name_Purchase with median\n",
      "Filling 190 missing values in issuetype.name_Issue with median\n",
      "Filling 190 missing values in issuetype.name_Test Task with median\n",
      "Filling 190 missing values in issuetype.name_Workload with median\n",
      "Filling 190 missing values in issuetype.name_Backport Sub-Task with median\n",
      "Filling 190 missing values in issuetype.name_CTS Challenge with median\n",
      "Filling 190 missing values in issuetype.name_QE Sub-task with median\n",
      "Filling 190 missing values in issuetype.name_Pricing Change with median\n",
      "Filling 190 missing values in issuetype.name_Design Request with median\n",
      "Filling 190 missing values in issuetype.name_Initiative with median\n",
      "Filling 190 missing values in issuetype.name_OKR with median\n",
      "Filling 190 missing values in issuetype.name_Tracking with median\n",
      "Filling 190 missing values in issuetype.name_Component Upgrade with median\n",
      "Filling 190 missing values in issuetype.name_Library Upgrade with median\n",
      "Filling 190 missing values in issuetype.name_Success Metric with median\n",
      "Filling 190 missing values in issuetype.name_Doc API with median\n",
      "Filling 190 missing values in issuetype.name_App Incident with median\n",
      "Filling 190 missing values in issuetype.name_Payment Support with median\n",
      "Filling 190 missing values in issuetype.name_Dev Sub-task with median\n",
      "Filling 190 missing values in issuetype.name_Simple Task with median\n",
      "Filling 190 missing values in issuetype.name_Release with median\n",
      "Filling 190 missing values in issuetype.name_Quality Risk with median\n",
      "Filling 190 missing values in issuetype.name_Doc UI with median\n",
      "Filling 190 missing values in issuetype.name_Docs Sub-task with median\n",
      "Filling 190 missing values in issuetype.name_QE Task with median\n",
      "Filling 190 missing values in status.name_Approved with median\n",
      "Filling 190 missing values in status.name_Verified with median\n",
      "Filling 190 missing values in status.name_Dropped with median\n",
      "Filling 190 missing values in status.name_In Review with median\n",
      "Filling 190 missing values in status.name_Defining with median\n",
      "Replacing 2 negative values in min_resolution_hours with 0\n",
      "Dropped 6 columns with constant values\n",
      "Cleaning complete. Resulting dataset: 349 rows, 159 columns\n",
      "Added team metrics: ['creator_count', 'reporter_count', 'team_size_estimate', 'issues_per_team_member', 'resolution_hours_per_team_member', 'team_role_diversity']\n",
      "\n",
      "Analyzing feature importance for predicting total_resolution_hours...\n",
      "Identified 8 features that account for 90% of importance\n",
      "Top 5 most important features: issuetype.name_Suggestion, issue_count, issuetype.name_Bug, reporter_count, reporter.key\n",
      "Analyzed correlations between 8 key features\n",
      "\n",
      "Using Elbow Method to determine optimal number of clusters...\n",
      "Elbow method suggests optimal number of clusters: 3\n",
      "\n",
      "Using Silhouette Analysis to determine optimal number of clusters...\n",
      "Silhouette analysis suggests optimal number of clusters: 2\n",
      "\n",
      "Using Davies-Bouldin Index to determine optimal number of clusters...\n",
      "Davies-Bouldin index suggests optimal number of clusters: 2\n",
      "\n",
      "Cluster number recommendations:\n",
      "  Elbow Method: k = 3\n",
      "  Silhouette Analysis: k = 2\n",
      "  Davies-Bouldin Index: k = 2\n",
      "\n",
      "Recommended optimal number of clusters: 2\n",
      "\n",
      "Tuning KMeans hyperparameters for k=2...\n",
      "Testing 8 hyperparameter combinations...\n",
      "Best hyperparameters: {'init': 'k-means++', 'n_init': 10, 'max_iter': 300}\n",
      "Silhouette score with best parameters: 0.9482\n",
      "\n",
      "Classifying projects into 2 clusters...\n",
      "\n",
      "Large High-effort Projects (Cluster 0) (2 projects, 0.6%):\n",
      "  Sample projects: Minecraft (Bedrock codebase), Minecraft: Java Edition\n",
      "  Distinguishing characteristics:\n",
      "    - team_size_estimate: 7912.8% higher than average\n",
      "    - reporter_count: 7912.4% higher than average\n",
      "    - reporter.key: 7912.4% higher than average\n",
      "    - issuetype.name_Bug: 5466.2% higher than average\n",
      "    - issue_count: 3282.5% higher than average\n",
      "\n",
      "Medium High-effort Projects (Cluster 1) (347 projects, 99.4%):\n",
      "  Sample projects: Aries, Sentry (Retired), Red Hat 3scale API Management\n",
      "  Distinguishing characteristics:\n",
      "    - team_size_estimate: 45.6% lower than average\n",
      "    - reporter_count: 45.6% lower than average\n",
      "    - reporter.key: 45.6% lower than average\n",
      "    - issuetype.name_Bug: 31.5% lower than average\n",
      "    - issue_count: 18.9% lower than average\n",
      "Project classification complete. Results saved to project_classification_results/results\n",
      "\n",
      "============= PIPELINE COMPLETE =============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class ProjectClassificationSystem:\n",
    "    \"\"\"\n",
    "    A comprehensive system for project classification based on feature importance\n",
    "    and hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"project_analysis\"):\n",
    "        \"\"\"\n",
    "        Initialize the project classification system.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_dir : str\n",
    "            Directory to save outputs and visualizations\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Set up subdirectories\n",
    "        self.feature_dir = os.path.join(output_dir, \"feature_analysis\")\n",
    "        self.cluster_dir = os.path.join(output_dir, \"cluster_analysis\")\n",
    "        self.results_dir = os.path.join(output_dir, \"results\")\n",
    "        \n",
    "        for directory in [self.feature_dir, self.cluster_dir, self.results_dir]:\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "    \n",
    "    def load_data(self, file_paths):\n",
    "        \"\"\"\n",
    "        Load and combine data from CSV files.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_paths : list or str\n",
    "            Path(s) to CSV file(s)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Combined DataFrame\n",
    "        \"\"\"\n",
    "        if isinstance(file_paths, str):\n",
    "            file_paths = [file_paths]\n",
    "        \n",
    "        dfs = []\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Loaded {file_path}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        if len(dfs) == 0:\n",
    "            return None\n",
    "        elif len(dfs) == 1:\n",
    "            return dfs[0]\n",
    "        else:\n",
    "            # Combine datasets (outer join to keep all data)\n",
    "            combined = pd.concat(dfs, ignore_index=True)\n",
    "            print(f\"Combined dataset: {combined.shape[0]} rows, {combined.shape[1]} columns\")\n",
    "            return combined\n",
    "    \n",
    "    def clean_data(self, df):\n",
    "        \"\"\"\n",
    "        Perform basic data cleaning operations.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        print(\"Starting data cleaning...\")\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 1. Clean up column names\n",
    "        # Replace \"fields.\" prefix\n",
    "        renamed_cols = {col: col.replace('fields.', '') for col in df_clean.columns if col.startswith('fields.')}\n",
    "        df_clean = df_clean.rename(columns=renamed_cols)\n",
    "        \n",
    "        # Replace \"<lambda>\" with empty string\n",
    "        renamed_cols = {col: col.replace('_<lambda>', '') for col in df_clean.columns if '_<lambda>' in col}\n",
    "        df_clean = df_clean.rename(columns=renamed_cols)\n",
    "        \n",
    "        # 2. Handle missing values in critical columns\n",
    "        critical_cols = ['project_id', 'project_name', 'issue_count', 'total_resolution_hours']\n",
    "        critical_cols = [col for col in critical_cols if col in df_clean.columns]\n",
    "        \n",
    "        missing_before = df_clean.shape[0]\n",
    "        df_clean = df_clean.dropna(subset=critical_cols)\n",
    "        missing_after = df_clean.shape[0]\n",
    "        \n",
    "        if missing_before > missing_after:\n",
    "            print(f\"Removed {missing_before - missing_after} rows with missing critical values\")\n",
    "        \n",
    "        # 3. Handle missing values in numeric columns\n",
    "        numeric_cols = df_clean.select_dtypes(include=np.number).columns\n",
    "        for col in numeric_cols:\n",
    "            missing_count = df_clean[col].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"Filling {missing_count} missing values in {col} with median\")\n",
    "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "        \n",
    "        # 4. Handle negative values in columns where it doesn't make sense\n",
    "        for col in numeric_cols:\n",
    "            if any(keyword in col.lower() for keyword in ['count', 'hours', 'duration']):\n",
    "                neg_count = (df_clean[col] < 0).sum()\n",
    "                if neg_count > 0:\n",
    "                    print(f\"Replacing {neg_count} negative values in {col} with 0\")\n",
    "                    df_clean.loc[df_clean[col] < 0, col] = 0\n",
    "        \n",
    "        # 5. Drop columns with all zeros or constant values\n",
    "        constant_cols = []\n",
    "        for col in numeric_cols:\n",
    "            if df_clean[col].nunique() <= 1:\n",
    "                constant_cols.append(col)\n",
    "        \n",
    "        if constant_cols:\n",
    "            df_clean = df_clean.drop(columns=constant_cols)\n",
    "            print(f\"Dropped {len(constant_cols)} columns with constant values\")\n",
    "        \n",
    "        print(f\"Cleaning complete. Resulting dataset: {df_clean.shape[0]} rows, {df_clean.shape[1]} columns\")\n",
    "        return df_clean\n",
    "    \n",
    "    def analyze_feature_importance(self, df, target_column='total_resolution_hours', n_estimators=100):\n",
    "        \"\"\"\n",
    "        Analyze feature importance using Random Forest regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input DataFrame\n",
    "        target_column : str\n",
    "            Target variable for prediction\n",
    "        n_estimators : int\n",
    "            Number of trees in Random Forest\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with feature importances\n",
    "        \"\"\"\n",
    "        print(f\"\\nAnalyzing feature importance for predicting {target_column}...\")\n",
    "        \n",
    "        # Ensure target column exists\n",
    "        if target_column not in df.columns:\n",
    "            print(f\"Target column '{target_column}' not found in dataset\")\n",
    "            return None\n",
    "        \n",
    "        # Select numeric columns for feature importance\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Exclude ID and target columns from features\n",
    "        exclude_patterns = ['project_id', 'id']\n",
    "        features = [col for col in numeric_cols if col != target_column and \n",
    "                   not any(pattern in col.lower() for pattern in exclude_patterns)]\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        X = df[features].copy()\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Create and train random forest model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=42,\n",
    "            n_jobs=-1  # Use all available CPU cores\n",
    "        )\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Create a DataFrame for visualization\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Calculate cumulative importance\n",
    "        importance_df['Cumulative_Importance'] = importance_df['Importance'].cumsum()\n",
    "        \n",
    "        # Create a horizontal bar chart for top 15 features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_df.head(15)\n",
    "        \n",
    "        sns.barplot(\n",
    "            x='Importance', \n",
    "            y='Feature', \n",
    "            data=top_features,\n",
    "            palette='viridis'\n",
    "        )\n",
    "        \n",
    "        plt.title(f'Top 15 Most Important Features for Predicting {target_column}', fontsize=16)\n",
    "        plt.xlabel('Importance', fontsize=14)\n",
    "        plt.ylabel('Feature', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(self.feature_dir, 'top_features.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create cumulative importance plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.plot(range(1, len(importance_df) + 1), importance_df['Cumulative_Importance'], 'o-', markersize=8)\n",
    "        plt.axhline(y=0.9, color='r', linestyle='--', label='90% Importance Threshold')\n",
    "        \n",
    "        # Find how many features are needed for 90% importance\n",
    "        features_for_90 = sum(importance_df['Cumulative_Importance'] <= 0.9) + 1\n",
    "        plt.axvline(x=features_for_90, color='g', linestyle='--', \n",
    "                   label=f'{features_for_90} Features = 90% Importance')\n",
    "        \n",
    "        plt.title('Cumulative Feature Importance', fontsize=16)\n",
    "        plt.xlabel('Number of Features', fontsize=14)\n",
    "        plt.ylabel('Cumulative Importance', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(self.feature_dir, 'cumulative_importance.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Identified {features_for_90} features that account for 90% of importance\")\n",
    "        print(f\"Top 5 most important features: {', '.join(importance_df['Feature'].head(5).tolist())}\")\n",
    "        \n",
    "        # Save feature importance to CSV\n",
    "        importance_df.to_csv(os.path.join(self.feature_dir, 'feature_importance.csv'), index=False)\n",
    "        \n",
    "        return importance_df, X, model\n",
    "    \n",
    "    def get_key_features(self, importance_df, importance_threshold=0.9):\n",
    "        \"\"\"\n",
    "        Get key features based on importance threshold.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        importance_df : pandas.DataFrame\n",
    "            DataFrame with feature importances\n",
    "        importance_threshold : float\n",
    "            Threshold for cumulative importance\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of key features\n",
    "        \"\"\"\n",
    "        # Ensure cumulative importance is calculated\n",
    "        if 'Cumulative_Importance' not in importance_df.columns:\n",
    "            importance_df = importance_df.sort_values('Importance', ascending=False).copy()\n",
    "            importance_df['Cumulative_Importance'] = importance_df['Importance'].cumsum()\n",
    "        \n",
    "        # Get features up to threshold\n",
    "        key_features = importance_df[importance_df['Cumulative_Importance'] <= importance_threshold]['Feature'].tolist()\n",
    "        \n",
    "        # Add one more feature to cross the threshold if needed\n",
    "        if len(key_features) < len(importance_df):\n",
    "            next_feature = importance_df.iloc[len(key_features)]['Feature']\n",
    "            key_features.append(next_feature)\n",
    "        \n",
    "        return key_features\n",
    "    \n",
    "    def analyze_feature_correlations(self, df, key_features):\n",
    "        \"\"\"\n",
    "        Analyze correlations between key features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input DataFrame\n",
    "        key_features : list\n",
    "            List of key features to analyze\n",
    "        \"\"\"\n",
    "        # Ensure key features exist in the DataFrame\n",
    "        available_features = [f for f in key_features if f in df.columns]\n",
    "        \n",
    "        if len(available_features) < 2:\n",
    "            print(\"Not enough features for correlation analysis\")\n",
    "            return\n",
    "        \n",
    "        # Limit to top 10 features to avoid overcrowded plot\n",
    "        if len(available_features) > 10:\n",
    "            available_features = available_features[:10]\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = df[available_features].corr()\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        \n",
    "        sns.heatmap(\n",
    "            corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm',\n",
    "            vmin=-1, \n",
    "            vmax=1, \n",
    "            center=0,\n",
    "            fmt='.2f'\n",
    "        )\n",
    "        \n",
    "        plt.title('Correlation Between Key Features', fontsize=16)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(self.feature_dir, 'feature_correlations.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Analyzed correlations between {len(available_features)} key features\")\n",
    "    \n",
    "    def find_optimal_clusters_elbow(self, data, max_k=10):\n",
    "        \"\"\"\n",
    "        Use the elbow method to find the optimal number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            Data for clustering (already scaled)\n",
    "        max_k : int\n",
    "            Maximum number of clusters to try\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (inertias, optimal_k)\n",
    "        \"\"\"\n",
    "        print(\"\\nUsing Elbow Method to determine optimal number of clusters...\")\n",
    "        \n",
    "        inertias = {}\n",
    "        \n",
    "        for k in range(1, max_k + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(data)\n",
    "            inertias[k] = kmeans.inertia_\n",
    "        \n",
    "        # Plot the elbow curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(inertias.keys()), list(inertias.values()), 'o-', markersize=8)\n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=14)\n",
    "        plt.ylabel('Inertia', fontsize=14)\n",
    "        plt.title('Elbow Method for Optimal k', fontsize=16)\n",
    "        plt.xticks(range(1, max_k + 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate the rate of change to find the elbow point\n",
    "        k_values = list(range(1, max_k + 1))\n",
    "        inertia_values = [inertias[k] for k in k_values]\n",
    "        \n",
    "        # Calculate the first derivatives (slopes)\n",
    "        slopes = []\n",
    "        for i in range(1, len(k_values)):\n",
    "            slope = (inertia_values[i] - inertia_values[i-1]) / (k_values[i] - k_values[i-1])\n",
    "            slopes.append(slope)\n",
    "        \n",
    "        # Calculate the second derivatives (changes in slope)\n",
    "        slope_changes = []\n",
    "        for i in range(1, len(slopes)):\n",
    "            change = slopes[i] - slopes[i-1]\n",
    "            slope_changes.append(change)\n",
    "        \n",
    "        # The elbow point is where the second derivative is maximized\n",
    "        # Add 2 because we're looking at the second derivative, which is offset by 2 from the original k values\n",
    "        if len(slope_changes) > 0:\n",
    "            elbow_k = k_values[np.argmax(np.abs(slope_changes)) + 2]\n",
    "        else:\n",
    "            # Fallback if we don't have enough data points\n",
    "            elbow_k = 3\n",
    "        \n",
    "        # Highlight the elbow point\n",
    "        plt.plot(elbow_k, inertias[elbow_k], 'ro', markersize=12)\n",
    "        plt.annotate(f'Elbow Point: k={elbow_k}',\n",
    "                    xy=(elbow_k, inertias[elbow_k]),\n",
    "                    xytext=(elbow_k + 0.5, inertias[elbow_k] * 1.1),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.cluster_dir, 'elbow_method.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Elbow method suggests optimal number of clusters: {elbow_k}\")\n",
    "        \n",
    "        return inertias, elbow_k\n",
    "    \n",
    "    def find_optimal_clusters_silhouette(self, data, max_k=10):\n",
    "        \"\"\"\n",
    "        Use silhouette analysis to find the optimal number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            Data for clustering (already scaled)\n",
    "        max_k : int\n",
    "            Maximum number of clusters to try\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (silhouette_scores, optimal_k)\n",
    "        \"\"\"\n",
    "        print(\"\\nUsing Silhouette Analysis to determine optimal number of clusters...\")\n",
    "        \n",
    "        silhouette_scores = {}\n",
    "        \n",
    "        # Must have at least 2 clusters for silhouette analysis\n",
    "        for k in range(2, max_k + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(data)\n",
    "            \n",
    "            # Calculate silhouette score\n",
    "            silhouette_avg = silhouette_score(data, cluster_labels)\n",
    "            silhouette_scores[k] = silhouette_avg\n",
    "        \n",
    "        # Plot the silhouette scores\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), 'o-', markersize=8)\n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=14)\n",
    "        plt.ylabel('Silhouette Score', fontsize=14)\n",
    "        plt.title('Silhouette Analysis for Optimal k', fontsize=16)\n",
    "        plt.xticks(range(2, max_k + 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Find the k with maximum silhouette score\n",
    "        optimal_k = max(silhouette_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Highlight the optimal point\n",
    "        plt.plot(optimal_k, silhouette_scores[optimal_k], 'ro', markersize=12)\n",
    "        plt.annotate(f'Optimal k={optimal_k}',\n",
    "                    xy=(optimal_k, silhouette_scores[optimal_k]),\n",
    "                    xytext=(optimal_k + 0.5, silhouette_scores[optimal_k] * 0.95),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.cluster_dir, 'silhouette_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Silhouette analysis suggests optimal number of clusters: {optimal_k}\")\n",
    "        \n",
    "        return silhouette_scores, optimal_k\n",
    "    \n",
    "    def find_optimal_clusters_davies_bouldin(self, data, max_k=10):\n",
    "        \"\"\"\n",
    "        Use the Davies-Bouldin index to find the optimal number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            Data for clustering (already scaled)\n",
    "        max_k : int\n",
    "            Maximum number of clusters to try\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (db_scores, optimal_k)\n",
    "        \"\"\"\n",
    "        print(\"\\nUsing Davies-Bouldin Index to determine optimal number of clusters...\")\n",
    "        \n",
    "        db_scores = {}\n",
    "        \n",
    "        # Need at least 2 clusters for Davies-Bouldin index\n",
    "        for k in range(2, max_k + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(data)\n",
    "            \n",
    "            # Calculate Davies-Bouldin index\n",
    "            db_index = davies_bouldin_score(data, labels)\n",
    "            db_scores[k] = db_index\n",
    "        \n",
    "        # Plot the Davies-Bouldin indices\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(list(db_scores.keys()), list(db_scores.values()), 'o-', markersize=8)\n",
    "        plt.xlabel('Number of Clusters (k)', fontsize=14)\n",
    "        plt.ylabel('Davies-Bouldin Index', fontsize=14)\n",
    "        plt.title('Davies-Bouldin Index for Optimal k', fontsize=16)\n",
    "        plt.xticks(range(2, max_k + 1))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Find the k with minimum Davies-Bouldin index\n",
    "        optimal_k = min(db_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Highlight the optimal point\n",
    "        plt.plot(optimal_k, db_scores[optimal_k], 'ro', markersize=12)\n",
    "        plt.annotate(f'Optimal k={optimal_k}',\n",
    "                    xy=(optimal_k, db_scores[optimal_k]),\n",
    "                    xytext=(optimal_k + 0.5, db_scores[optimal_k] * 0.9),\n",
    "                    arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "                    fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.cluster_dir, 'davies_bouldin.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Davies-Bouldin index suggests optimal number of clusters: {optimal_k}\")\n",
    "        \n",
    "        return db_scores, optimal_k\n",
    "    \n",
    "    def determine_optimal_clusters(self, data, max_k=10):\n",
    "        \"\"\"\n",
    "        Determine the optimal number of clusters using multiple methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            Data for clustering (already scaled)\n",
    "        max_k : int\n",
    "            Maximum number of clusters to try\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int\n",
    "            Recommended optimal number of clusters\n",
    "        \"\"\"\n",
    "        # Apply all methods\n",
    "        _, elbow_k = self.find_optimal_clusters_elbow(data, max_k)\n",
    "        _, silhouette_k = self.find_optimal_clusters_silhouette(data, max_k)\n",
    "        _, db_k = self.find_optimal_clusters_davies_bouldin(data, max_k)\n",
    "        \n",
    "        methods = {\n",
    "            'Elbow Method': elbow_k,\n",
    "            'Silhouette Analysis': silhouette_k,\n",
    "            'Davies-Bouldin Index': db_k\n",
    "        }\n",
    "        \n",
    "        # Print the recommendations from each method\n",
    "        print(\"\\nCluster number recommendations:\")\n",
    "        for method, k in methods.items():\n",
    "            print(f\"  {method}: k = {k}\")\n",
    "        \n",
    "        # Make a final recommendation\n",
    "        from collections import Counter\n",
    "        k_values = list(methods.values())\n",
    "        most_common_k = Counter(k_values).most_common(1)[0][0]\n",
    "        \n",
    "        # If there's a tie and silhouette recommends a value, use that\n",
    "        if most_common_k != silhouette_k and k_values.count(most_common_k) == k_values.count(silhouette_k):\n",
    "            final_k = silhouette_k\n",
    "        else:\n",
    "            final_k = most_common_k\n",
    "        \n",
    "        print(f\"\\nRecommended optimal number of clusters: {final_k}\")\n",
    "        \n",
    "        return final_k\n",
    "    \n",
    "    def tune_kmeans_hyperparameters(self, data, k):\n",
    "        \"\"\"\n",
    "        Tune KMeans hyperparameters for a given number of clusters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            Data for clustering (already scaled)\n",
    "        k : int\n",
    "            Number of clusters\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Best hyperparameters\n",
    "        \"\"\"\n",
    "        print(f\"\\nTuning KMeans hyperparameters for k={k}...\")\n",
    "        \n",
    "        # Define hyperparameter grid\n",
    "        param_grid = {\n",
    "            'init': ['k-means++', 'random'],\n",
    "            'n_init': [10, 20],\n",
    "            'max_iter': [300, 500]\n",
    "        }\n",
    "        \n",
    "        # Generate all combinations of parameters\n",
    "        import itertools\n",
    "        all_params = [dict(zip(param_grid.keys(), values)) for values in itertools.product(*param_grid.values())]\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        \n",
    "        print(f\"Testing {len(all_params)} hyperparameter combinations...\")\n",
    "        \n",
    "        # Test each combination\n",
    "        for params in all_params:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, **params)\n",
    "            labels = kmeans.fit_predict(data)\n",
    "            \n",
    "            # Evaluate using silhouette score\n",
    "            score = silhouette_score(data, labels)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "        \n",
    "        print(f\"Best hyperparameters: {best_params}\")\n",
    "        print(f\"Silhouette score with best parameters: {best_score:.4f}\")\n",
    "        \n",
    "        return best_params\n",
    "    \n",
    "    def classify_projects(self, df, key_features, optimal_k, best_params=None):\n",
    "        \"\"\"\n",
    "        Classify projects into clusters using key features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input DataFrame\n",
    "        key_features : list\n",
    "            List of key features to use for clustering\n",
    "        optimal_k : int\n",
    "            Optimal number of clusters\n",
    "        best_params : dict, optional\n",
    "            Best hyperparameters for KMeans\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with cluster assignments\n",
    "        \"\"\"\n",
    "        print(f\"\\nClassifying projects into {optimal_k} clusters...\")\n",
    "        \n",
    "        # Make a copy of the dataframe\n",
    "        df_classified = df.copy()\n",
    "        \n",
    "        # Select only key features that exist in the dataframe\n",
    "        available_key_features = [f for f in key_features if f in df.columns]\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        cluster_data = df[available_key_features].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        cluster_data = cluster_data.fillna(cluster_data.median())\n",
    "        \n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(cluster_data)\n",
    "        \n",
    "        # Apply KMeans with optimal parameters\n",
    "        if best_params is None:\n",
    "            best_params = {'init': 'k-means++', 'n_init': 10, 'max_iter': 300}\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, **best_params)\n",
    "        clusters = kmeans.fit_predict(scaled_data)\n",
    "        \n",
    "        # Add cluster assignments to the DataFrame\n",
    "        df_classified['project_class'] = clusters\n",
    "        \n",
    "        # Apply PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        # Add PCA components for visualization\n",
    "        df_classified['pca_x'] = pca_result[:, 0]\n",
    "        df_classified['pca_y'] = pca_result[:, 1]\n",
    "        \n",
    "        # Visualize the clusters\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Colors for clusters\n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "        \n",
    "        # Plot each cluster\n",
    "        for i in range(optimal_k):\n",
    "            cluster_points = df_classified[df_classified['project_class'] == i]\n",
    "            plt.scatter(\n",
    "                cluster_points['pca_x'], \n",
    "                cluster_points['pca_y'],\n",
    "                s=100, \n",
    "                alpha=0.7,\n",
    "                c=[colors[i]],\n",
    "                label=f'Class {i} ({len(cluster_points)} projects)'\n",
    "            )\n",
    "            \n",
    "            # Add some project names as labels\n",
    "            if len(cluster_points) > 0:\n",
    "                sample_size = min(3, len(cluster_points))\n",
    "                for _, row in cluster_points.sample(sample_size).iterrows():\n",
    "                    plt.annotate(\n",
    "                        row['project_name'],\n",
    "                        (row['pca_x'], row['pca_y']),\n",
    "                        fontsize=9,\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "        \n",
    "        plt.title('Project Classification Using Key Features', fontsize=16)\n",
    "        plt.xlabel('Principal Component 1', fontsize=14)\n",
    "        plt.ylabel('Principal Component 2', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(os.path.join(self.cluster_dir, 'project_classes.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Analyze each cluster\n",
    "        cluster_profiles = self.analyze_clusters(df_classified, available_key_features)\n",
    "        \n",
    "        # Save the classified data\n",
    "        df_classified.to_csv(os.path.join(self.results_dir, 'classified_projects.csv'), index=False)\n",
    "        \n",
    "        # Save a summary of cluster memberships\n",
    "        cluster_counts = df_classified['project_class'].value_counts().sort_index()\n",
    "        cluster_pcts = (cluster_counts / len(df_classified) * 100).round(1)\n",
    "        \n",
    "        summary_df = pd.DataFrame({\n",
    "            'Cluster': cluster_counts.index,\n",
    "            'Projects': cluster_counts.values,\n",
    "            'Percentage': cluster_pcts.values\n",
    "        })\n",
    "        \n",
    "        summary_df.to_csv(os.path.join(self.results_dir, 'cluster_summary.csv'), index=False)\n",
    "        \n",
    "        print(f\"Project classification complete. Results saved to {self.results_dir}\")\n",
    "        \n",
    "        return df_classified, cluster_profiles\n",
    "    def create_team_metrics(self, df):\n",
    "        \"\"\"\n",
    "        Create advanced team-related metrics from basic team data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            DataFrame with added team metrics\n",
    "        \"\"\"\n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        # Find team-related columns\n",
    "        creator_cols = [col for col in df.columns if 'creator' in col.lower()]\n",
    "        reporter_cols = [col for col in df.columns if 'reporter' in col.lower()]\n",
    "        assignee_cols = [col for col in df.columns if 'assignee' in col.lower()]\n",
    "        \n",
    "        # Basic team size metrics\n",
    "        if creator_cols:\n",
    "            df_enhanced['creator_count'] = df[creator_cols].max(axis=1)\n",
    "        \n",
    "        if reporter_cols:\n",
    "            df_enhanced['reporter_count'] = df[reporter_cols].max(axis=1)\n",
    "        \n",
    "        if assignee_cols:\n",
    "            df_enhanced['assignee_count'] = df[assignee_cols].max(axis=1)\n",
    "        \n",
    "        # Combined team size estimate (taking the maximum of available metrics)\n",
    "        team_cols = ['creator_count', 'reporter_count', 'assignee_count']\n",
    "        available_team_cols = [col for col in team_cols if col in df_enhanced.columns]\n",
    "        \n",
    "        if available_team_cols:\n",
    "            df_enhanced['team_size_estimate'] = df_enhanced[available_team_cols].max(axis=1)\n",
    "            \n",
    "            # Team productivity metrics\n",
    "            if 'issue_count' in df.columns:\n",
    "                df_enhanced['issues_per_team_member'] = df['issue_count'] / df_enhanced['team_size_estimate'].replace(0, 1)\n",
    "            \n",
    "            if 'total_resolution_hours' in df.columns:\n",
    "                df_enhanced['resolution_hours_per_team_member'] = df['total_resolution_hours'] / df_enhanced['team_size_estimate'].replace(0, 1)\n",
    "        \n",
    "        # Team diversity approximation\n",
    "        if len(available_team_cols) > 1:\n",
    "            # Calculate the ratio between different types of team members\n",
    "            # This could indicate how evenly distributed the team roles are\n",
    "            df_enhanced['team_role_diversity'] = df_enhanced[available_team_cols].std(axis=1) / df_enhanced[available_team_cols].mean(axis=1).replace(0, 1)\n",
    "        \n",
    "        print(f\"Added team metrics: {[col for col in df_enhanced.columns if col not in df.columns]}\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def analyze_clusters(self, df_classified, key_features):\n",
    "        \"\"\"\n",
    "        Analyze the characteristics of each cluster.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df_classified : pandas.DataFrame\n",
    "            DataFrame with cluster assignments\n",
    "        key_features : list\n",
    "            List of key features used for clustering\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary with cluster profiles\n",
    "        \"\"\"\n",
    "        if 'project_class' not in df_classified.columns:\n",
    "            print(\"No cluster assignments found in DataFrame\")\n",
    "            return {}\n",
    "        \n",
    "        # Create profiles for each cluster\n",
    "        cluster_profiles = {}\n",
    "        \n",
    "        # Get all numeric columns\n",
    "        numeric_cols = df_classified.select_dtypes(include=np.number).columns\n",
    "        \n",
    "        # Remove cluster-related columns\n",
    "        analysis_cols = [col for col in numeric_cols if col not in ['project_class', 'pca_x', 'pca_y']]\n",
    "        \n",
    "        # Calculate overall metrics for comparison\n",
    "        overall_means = df_classified[analysis_cols].mean()\n",
    "        \n",
    "        # Analyze each cluster\n",
    "        for cluster in sorted(df_classified['project_class'].unique()):\n",
    "            cluster_data = df_classified[df_classified['project_class'] == cluster]\n",
    "            \n",
    "            # Basic statistics\n",
    "            profile = {\n",
    "                'size': len(cluster_data),\n",
    "                'percentage': round(len(cluster_data) / len(df_classified) * 100, 1),\n",
    "                'sample_projects': cluster_data['project_name'].sample(min(5, len(cluster_data))).tolist(),\n",
    "                'features': {}\n",
    "            }\n",
    "            \n",
    "            # Calculate statistics for key features\n",
    "            for feature in key_features:\n",
    "                if feature in cluster_data.columns:\n",
    "                    cluster_mean = cluster_data[feature].mean()\n",
    "                    overall_mean = overall_means[feature]\n",
    "                    \n",
    "                    # Calculate the percentage difference from overall mean\n",
    "                    if overall_mean != 0:\n",
    "                        pct_diff = ((cluster_mean - overall_mean) / overall_mean) * 100\n",
    "                    else:\n",
    "                        pct_diff = 0 if cluster_mean == 0 else float('inf')\n",
    "                    \n",
    "                    profile['features'][feature] = {\n",
    "                        'mean': cluster_mean,\n",
    "                        'median': cluster_data[feature].median(),\n",
    "                        'min': cluster_data[feature].min(),\n",
    "                        'max': cluster_data[feature].max(),\n",
    "                        'pct_diff': pct_diff\n",
    "                    }\n",
    "            \n",
    "            # Sort features by absolute percentage difference to identify distinguishing features\n",
    "            sorted_features = sorted(\n",
    "                [(f, profile['features'][f]['pct_diff']) for f in profile['features']],\n",
    "                key=lambda x: abs(x[1]),\n",
    "                reverse=True\n",
    "            )\n",
    "            \n",
    "            # Keep top 5 distinguishing features\n",
    "            top_features = sorted_features[:5]\n",
    "            \n",
    "            # Generate a description based on distinguishing features\n",
    "            description_parts = []\n",
    "            for feature, pct_diff in top_features:\n",
    "                direction = \"higher\" if pct_diff > 0 else \"lower\"\n",
    "                description_parts.append(f\"{feature} is {abs(pct_diff):.1f}% {direction} than average\")\n",
    "            \n",
    "            profile['distinguishing_features'] = top_features\n",
    "            profile['description'] = \"Projects where \" + \", \".join(description_parts)\n",
    "            \n",
    "            # Determine cluster characteristics based on key metrics\n",
    "            if 'issue_count' in cluster_data.columns:\n",
    "                issue_count_mean = cluster_data['issue_count'].mean()\n",
    "                \n",
    "                if issue_count_mean > 10000:\n",
    "                    size_label = \"Large\"\n",
    "                elif issue_count_mean > 1000:\n",
    "                    size_label = \"Medium\"\n",
    "                else:\n",
    "                    size_label = \"Small\"\n",
    "                \n",
    "                profile['size_label'] = size_label\n",
    "            \n",
    "            if 'total_resolution_hours' in cluster_data.columns:\n",
    "                hours_mean = cluster_data['total_resolution_hours'].mean()\n",
    "                \n",
    "                if hours_mean > 1000000:\n",
    "                    effort_label = \"High-effort\"\n",
    "                elif hours_mean > 100000:\n",
    "                    effort_label = \"Medium-effort\"\n",
    "                else:\n",
    "                    effort_label = \"Low-effort\"\n",
    "                \n",
    "                profile['effort_label'] = effort_label\n",
    "            \n",
    "            # Add cluster to profiles\n",
    "            cluster_name = f\"Cluster {cluster}\"\n",
    "            if 'size_label' in profile and 'effort_label' in profile:\n",
    "                cluster_name = f\"{profile['size_label']} {profile['effort_label']} Projects (Cluster {cluster})\"\n",
    "            \n",
    "            cluster_profiles[cluster_name] = profile\n",
    "            \n",
    "            # Print cluster information\n",
    "            print(f\"\\n{cluster_name} ({profile['size']} projects, {profile['percentage']}%):\")\n",
    "            print(f\"  Sample projects: {', '.join(profile['sample_projects'][:3])}\")\n",
    "            print(f\"  Distinguishing characteristics:\")\n",
    "            for feature, pct_diff in top_features:\n",
    "                direction = \"higher\" if pct_diff > 0 else \"lower\"\n",
    "                print(f\"    - {feature}: {abs(pct_diff):.1f}% {direction} than average\")\n",
    "        \n",
    "        # Save cluster profiles to JSON\n",
    "        import json\n",
    "        \n",
    "        # Convert to serializable format\n",
    "        serializable_profiles = {}\n",
    "        for cluster, profile in cluster_profiles.items():\n",
    "            serializable_profiles[cluster] = {\n",
    "                'size': int(profile['size']),\n",
    "                'percentage': float(profile['percentage']),\n",
    "                'sample_projects': profile['sample_projects'],\n",
    "                'description': profile['description']\n",
    "            }\n",
    "            \n",
    "            if 'features' in profile:\n",
    "                serializable_profiles[cluster]['features'] = {}\n",
    "                for feature, stats in profile['features'].items():\n",
    "                    serializable_profiles[cluster]['features'][feature] = {\n",
    "                        stat: float(value) if isinstance(value, (int, float, np.number)) else value\n",
    "                        for stat, value in stats.items()\n",
    "                    }\n",
    "        \n",
    "        with open(os.path.join(self.results_dir, 'cluster_profiles.json'), 'w') as f:\n",
    "            json.dump(serializable_profiles, f, indent=2)\n",
    "        \n",
    "        return cluster_profiles\n",
    "    \n",
    "    def execute_classification_pipeline(self, file_paths, target_column='total_resolution_hours', max_k=10):\n",
    "        \"\"\"\n",
    "        Execute the complete project classification pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_paths : list or str\n",
    "            Path(s) to CSV file(s)\n",
    "        target_column : str\n",
    "            Target variable for feature importance analysis\n",
    "        max_k : int\n",
    "            Maximum number of clusters to consider\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (classified_df, optimal_k, cluster_profiles)\n",
    "        \"\"\"\n",
    "        print(\"\\n========== PROJECT CLASSIFICATION PIPELINE ==========\\n\")\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        df = self.load_data(file_paths)\n",
    "        if df is None:\n",
    "            print(\"Failed to load data. Exiting pipeline.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # Step 2: Clean data\n",
    "        df_clean = self.clean_data(df)\n",
    "\n",
    "        df_clean = self.create_team_metrics(df_clean)\n",
    "\n",
    "        # Step 3: Analyze feature importance\n",
    "        importance_df, X, model = self.analyze_feature_importance(df_clean, target_column)\n",
    "    \n",
    "        \n",
    "        # Step 4: Get key features for classification\n",
    "        key_features = self.get_key_features(importance_df)\n",
    "        \n",
    "        # Step 5: Analyze feature correlations\n",
    "        self.analyze_feature_correlations(df_clean, key_features)\n",
    "        \n",
    "        # Step 6: Prepare data for clustering\n",
    "        # Select only key features that exist in the dataframe\n",
    "        available_key_features = [f for f in key_features if f in df_clean.columns]\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        cluster_data = df_clean[available_key_features].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        cluster_data = cluster_data.fillna(cluster_data.median())\n",
    "        \n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(cluster_data)\n",
    "        \n",
    "        # Step 7: Determine optimal number of clusters\n",
    "        optimal_k = self.determine_optimal_clusters(scaled_data, max_k)\n",
    "        \n",
    "        # Step 8: Tune KMeans hyperparameters\n",
    "        best_params = self.tune_kmeans_hyperparameters(scaled_data, optimal_k)\n",
    "        \n",
    "        # Step 9: Classify projects\n",
    "        df_classified, cluster_profiles = self.classify_projects(df_clean, key_features, optimal_k, best_params)\n",
    "        \n",
    "        print(\"\\n============= PIPELINE COMPLETE =============\\n\")\n",
    "        \n",
    "        return df_classified, optimal_k, cluster_profiles\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = ProjectClassificationSystem(output_dir=\"project_classification_results\")\n",
    "    \n",
    "    df_classified, optimal_k, cluster_profiles = classifier.execute_classification_pipeline(\n",
    "        [\"../DataSets/data_export_1741772203780.csv\", \"../DataSets/data_export_1741699774916.csv\"],\n",
    "        target_column='total_resolution_hours',\n",
    "        max_k=10\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
