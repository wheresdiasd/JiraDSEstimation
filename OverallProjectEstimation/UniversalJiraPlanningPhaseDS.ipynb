{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Hyperledger ...\n",
      "Processing repository: SecondLife ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:396: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:396: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:396: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:396: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n",
      "/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/CleanDSDTale.py:396: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-10-21 14:39:34', '2011-11-08 21:51:34', '2011-05-26 10:27:13',\n",
      " '2011-04-14 09:54:41', '2011-02-10 08:25:57', '2011-11-29 21:56:38',\n",
      " '2011-07-25 14:58:35', '2011-04-25 14:21:44', '2011-07-27 15:52:49',\n",
      " '2012-06-06 18:24:31',\n",
      " ...\n",
      " '2017-02-25 23:03:00', '2018-06-28 17:44:59', '2021-03-15 19:36:15',\n",
      " '2018-02-26 19:59:42', '2020-06-30 13:42:45', '2018-09-16 20:13:08',\n",
      " '2019-07-30 02:59:47', '2019-07-30 16:22:51', '2018-10-05 14:07:20',\n",
      " '2020-08-04 13:38:56']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2010-04-30 12:41:35', '2011-10-29 15:11:12', '2011-05-16 21:13:36',\n",
      " '2010-12-10 22:28:28', '2010-10-21 13:18:54', '2011-03-11 13:05:07',\n",
      " '2011-06-20 22:28:29', '2011-03-06 05:06:03', '2011-07-19 20:00:55',\n",
      " '2011-03-26 13:59:56',\n",
      " ...\n",
      " '2017-01-30 18:14:58', '2018-06-12 00:44:15', '2017-06-14 13:20:57',\n",
      " '2018-02-24 20:29:36', '2019-07-10 18:59:32', '2018-08-15 13:43:35',\n",
      " '2017-08-04 15:14:40', '2019-07-19 06:53:20', '2018-02-10 16:09:54',\n",
      " '2020-07-30 16:58:49']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-10-21 14:39:34', '2011-12-15 22:41:26', '2011-06-24 16:52:05',\n",
      " '2011-05-24 21:59:56', '2011-04-08 17:04:45', '2011-11-29 21:56:38',\n",
      " '2011-07-25 20:58:38', '2013-02-22 17:02:13', '2011-08-05 19:07:42',\n",
      " '2012-06-06 18:24:31',\n",
      " ...\n",
      " '2018-07-20 14:11:24', '2018-06-28 17:44:59', '2021-03-15 19:36:15',\n",
      " '2018-07-20 14:15:51', '2020-06-30 13:44:33', '2018-09-16 20:13:08',\n",
      " '2019-07-30 02:59:47', '2019-08-28 09:15:34', '2018-11-28 00:04:53',\n",
      " '2020-10-22 07:48:40']\n",
      "Length: 267, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2017-06-14 13:20:57', '2016-08-16 20:50:05', '2018-03-23 12:42:44',\n",
      " '2017-07-27 08:24:15', '2010-04-30 12:41:35', '2017-05-26 15:27:23',\n",
      " '2020-04-10 10:29:35', '2011-03-06 05:06:03', '2017-06-20 04:57:29',\n",
      " '2017-05-01 18:54:14', '2016-10-13 12:17:58', '2017-02-26 21:23:16',\n",
      " '2018-03-23 14:15:20', '2017-05-18 15:03:10', '2017-08-01 00:51:46',\n",
      " '2019-01-25 06:57:18', '2019-03-26 13:57:43', '2019-11-05 16:04:22',\n",
      " '2019-04-02 16:13:25', '2017-05-04 03:30:00', '2018-04-25 11:04:53']\n",
      "Length: 21, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2018-08-06 19:39:57', '2021-06-10 15:47:27', '2020-05-17 12:20:52',\n",
      " '2019-07-29 15:25:44', '2012-08-21 13:24:00', '2020-02-17 07:52:39',\n",
      " '2021-02-05 17:23:38', '2011-03-06 05:06:03', '2019-04-10 09:57:06',\n",
      " '2020-09-29 08:31:19', '2020-07-30 16:58:49', '2018-10-17 15:01:17',\n",
      " '2018-08-21 20:29:33', '2017-05-18 15:03:10', '2018-03-10 17:40:02',\n",
      " '2019-03-06 15:24:51', '2019-07-26 12:53:49', '2019-11-08 06:02:31',\n",
      " '2019-04-02 16:13:25', '2018-12-15 19:46:04', '2019-01-11 06:01:47']\n",
      "Length: 21, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… D-Tale session launched successfully.\n",
      "Launching D-Tale session for planning-phase DataFrame...\n",
      "Planning-phase DataFrame columns:\n",
      "['project_id', 'project_name', 'issue_count', 'fields.assignee.key_<lambda>', 'fields.creator.key_<lambda>', 'fields.reporter.key_<lambda>', 'total_links', 'avg_links_per_issue', 'pct_related_issues', 'pct_cloned_issues', 'fields.components_<lambda>', 'total_components', 'avg_components_per_issue', 'total_labels', 'avg_labels_per_issue', 'project_start_date', 'project_latest_date', 'avg_resolution_hours', 'median_resolution_hours', 'min_resolution_hours', 'max_resolution_hours', 'resolution_hours_std', 'total_resolution_hours', 'desc_emb_0_mean', 'desc_emb_1_mean', 'desc_emb_2_mean', 'desc_emb_3_mean', 'desc_emb_4_mean', 'desc_emb_5_mean', 'desc_emb_6_mean', 'desc_emb_7_mean', 'desc_emb_8_mean', 'desc_emb_9_mean', 'desc_emb_10_mean', 'desc_emb_11_mean', 'desc_emb_12_mean', 'desc_emb_13_mean', 'desc_emb_14_mean', 'desc_emb_15_mean', 'desc_emb_16_mean', 'desc_emb_17_mean', 'desc_emb_18_mean', 'desc_emb_19_mean', 'desc_emb_20_mean', 'desc_emb_21_mean', 'desc_emb_22_mean', 'desc_emb_23_mean', 'desc_emb_24_mean', 'desc_emb_25_mean', 'desc_emb_26_mean', 'desc_emb_27_mean', 'desc_emb_28_mean', 'desc_emb_29_mean', 'desc_emb_30_mean', 'desc_emb_31_mean', 'desc_emb_32_mean', 'desc_emb_33_mean', 'desc_emb_34_mean', 'desc_emb_35_mean', 'desc_emb_36_mean', 'desc_emb_37_mean', 'desc_emb_38_mean', 'desc_emb_39_mean', 'desc_emb_40_mean', 'desc_emb_41_mean', 'desc_emb_42_mean', 'desc_emb_43_mean', 'desc_emb_44_mean', 'desc_emb_45_mean', 'desc_emb_46_mean', 'desc_emb_47_mean', 'desc_emb_48_mean', 'desc_emb_49_mean', 'desc_emb_50_mean', 'desc_emb_51_mean', 'desc_emb_52_mean', 'desc_emb_53_mean', 'desc_emb_54_mean', 'desc_emb_55_mean', 'desc_emb_56_mean', 'desc_emb_57_mean', 'desc_emb_58_mean', 'desc_emb_59_mean', 'desc_emb_60_mean', 'desc_emb_61_mean', 'desc_emb_62_mean', 'desc_emb_63_mean', 'desc_emb_64_mean', 'desc_emb_65_mean', 'desc_emb_66_mean', 'desc_emb_67_mean', 'desc_emb_68_mean', 'desc_emb_69_mean', 'desc_emb_70_mean', 'desc_emb_71_mean', 'desc_emb_72_mean', 'desc_emb_73_mean', 'desc_emb_74_mean', 'desc_emb_75_mean', 'desc_emb_76_mean', 'desc_emb_77_mean', 'desc_emb_78_mean', 'desc_emb_79_mean', 'desc_emb_80_mean', 'desc_emb_81_mean', 'desc_emb_82_mean', 'desc_emb_83_mean', 'desc_emb_84_mean', 'desc_emb_85_mean', 'desc_emb_86_mean', 'desc_emb_87_mean', 'desc_emb_88_mean', 'desc_emb_89_mean', 'desc_emb_90_mean', 'desc_emb_91_mean', 'desc_emb_92_mean', 'desc_emb_93_mean', 'desc_emb_94_mean', 'desc_emb_95_mean', 'desc_emb_96_mean', 'desc_emb_97_mean', 'desc_emb_98_mean', 'desc_emb_99_mean', 'desc_emb_100_mean', 'desc_emb_101_mean', 'desc_emb_102_mean', 'desc_emb_103_mean', 'desc_emb_104_mean', 'desc_emb_105_mean', 'desc_emb_106_mean', 'desc_emb_107_mean', 'desc_emb_108_mean', 'desc_emb_109_mean', 'desc_emb_110_mean', 'desc_emb_111_mean', 'desc_emb_112_mean', 'desc_emb_113_mean', 'desc_emb_114_mean', 'desc_emb_115_mean', 'desc_emb_116_mean', 'desc_emb_117_mean', 'desc_emb_118_mean', 'desc_emb_119_mean', 'desc_emb_120_mean', 'desc_emb_121_mean', 'desc_emb_122_mean', 'desc_emb_123_mean', 'desc_emb_124_mean', 'desc_emb_125_mean', 'desc_emb_126_mean', 'desc_emb_127_mean', 'desc_emb_128_mean', 'desc_emb_129_mean', 'desc_emb_130_mean', 'desc_emb_131_mean', 'desc_emb_132_mean', 'desc_emb_133_mean', 'desc_emb_134_mean', 'desc_emb_135_mean', 'desc_emb_136_mean', 'desc_emb_137_mean', 'desc_emb_138_mean', 'desc_emb_139_mean', 'desc_emb_140_mean', 'desc_emb_141_mean', 'desc_emb_142_mean', 'desc_emb_143_mean', 'desc_emb_144_mean', 'desc_emb_145_mean', 'desc_emb_146_mean', 'desc_emb_147_mean', 'desc_emb_148_mean', 'desc_emb_149_mean', 'desc_emb_150_mean', 'desc_emb_151_mean', 'desc_emb_152_mean', 'desc_emb_153_mean', 'desc_emb_154_mean', 'desc_emb_155_mean', 'desc_emb_156_mean', 'desc_emb_157_mean', 'desc_emb_158_mean', 'desc_emb_159_mean', 'desc_emb_160_mean', 'desc_emb_161_mean', 'desc_emb_162_mean', 'desc_emb_163_mean', 'desc_emb_164_mean', 'desc_emb_165_mean', 'desc_emb_166_mean', 'desc_emb_167_mean', 'desc_emb_168_mean', 'desc_emb_169_mean', 'desc_emb_170_mean', 'desc_emb_171_mean', 'desc_emb_172_mean', 'desc_emb_173_mean', 'desc_emb_174_mean', 'desc_emb_175_mean', 'desc_emb_176_mean', 'desc_emb_177_mean', 'desc_emb_178_mean', 'desc_emb_179_mean', 'desc_emb_180_mean', 'desc_emb_181_mean', 'desc_emb_182_mean', 'desc_emb_183_mean', 'desc_emb_184_mean', 'desc_emb_185_mean', 'desc_emb_186_mean', 'desc_emb_187_mean', 'desc_emb_188_mean', 'desc_emb_189_mean', 'desc_emb_190_mean', 'desc_emb_191_mean', 'desc_emb_192_mean', 'desc_emb_193_mean', 'desc_emb_194_mean', 'desc_emb_195_mean', 'desc_emb_196_mean', 'desc_emb_197_mean', 'desc_emb_198_mean', 'desc_emb_199_mean', 'desc_emb_200_mean', 'desc_emb_201_mean', 'desc_emb_202_mean', 'desc_emb_203_mean', 'desc_emb_204_mean', 'desc_emb_205_mean', 'desc_emb_206_mean', 'desc_emb_207_mean', 'desc_emb_208_mean', 'desc_emb_209_mean', 'desc_emb_210_mean', 'desc_emb_211_mean', 'desc_emb_212_mean', 'desc_emb_213_mean', 'desc_emb_214_mean', 'desc_emb_215_mean', 'desc_emb_216_mean', 'desc_emb_217_mean', 'desc_emb_218_mean', 'desc_emb_219_mean', 'desc_emb_220_mean', 'desc_emb_221_mean', 'desc_emb_222_mean', 'desc_emb_223_mean', 'desc_emb_224_mean', 'desc_emb_225_mean', 'desc_emb_226_mean', 'desc_emb_227_mean', 'desc_emb_228_mean', 'desc_emb_229_mean', 'desc_emb_230_mean', 'desc_emb_231_mean', 'desc_emb_232_mean', 'desc_emb_233_mean', 'desc_emb_234_mean', 'desc_emb_235_mean', 'desc_emb_236_mean', 'desc_emb_237_mean', 'desc_emb_238_mean', 'desc_emb_239_mean', 'desc_emb_240_mean', 'desc_emb_241_mean', 'desc_emb_242_mean', 'desc_emb_243_mean', 'desc_emb_244_mean', 'desc_emb_245_mean', 'desc_emb_246_mean', 'desc_emb_247_mean', 'desc_emb_248_mean', 'desc_emb_249_mean', 'desc_emb_250_mean', 'desc_emb_251_mean', 'desc_emb_252_mean', 'desc_emb_253_mean', 'desc_emb_254_mean', 'desc_emb_255_mean', 'desc_emb_256_mean', 'desc_emb_257_mean', 'desc_emb_258_mean', 'desc_emb_259_mean', 'desc_emb_260_mean', 'desc_emb_261_mean', 'desc_emb_262_mean', 'desc_emb_263_mean', 'desc_emb_264_mean', 'desc_emb_265_mean', 'desc_emb_266_mean', 'desc_emb_267_mean', 'desc_emb_268_mean', 'desc_emb_269_mean', 'desc_emb_270_mean', 'desc_emb_271_mean', 'desc_emb_272_mean', 'desc_emb_273_mean', 'desc_emb_274_mean', 'desc_emb_275_mean', 'desc_emb_276_mean', 'desc_emb_277_mean', 'desc_emb_278_mean', 'desc_emb_279_mean', 'desc_emb_280_mean', 'desc_emb_281_mean', 'desc_emb_282_mean', 'desc_emb_283_mean', 'desc_emb_284_mean', 'desc_emb_285_mean', 'desc_emb_286_mean', 'desc_emb_287_mean', 'desc_emb_288_mean', 'desc_emb_289_mean', 'desc_emb_290_mean', 'desc_emb_291_mean', 'desc_emb_292_mean', 'desc_emb_293_mean', 'desc_emb_294_mean', 'desc_emb_295_mean', 'desc_emb_296_mean', 'desc_emb_297_mean', 'desc_emb_298_mean', 'desc_emb_299_mean', 'desc_emb_300_mean', 'desc_emb_301_mean', 'desc_emb_302_mean', 'desc_emb_303_mean', 'desc_emb_304_mean', 'desc_emb_305_mean', 'desc_emb_306_mean', 'desc_emb_307_mean', 'desc_emb_308_mean', 'desc_emb_309_mean', 'desc_emb_310_mean', 'desc_emb_311_mean', 'desc_emb_312_mean', 'desc_emb_313_mean', 'desc_emb_314_mean', 'desc_emb_315_mean', 'desc_emb_316_mean', 'desc_emb_317_mean', 'desc_emb_318_mean', 'desc_emb_319_mean', 'desc_emb_320_mean', 'desc_emb_321_mean', 'desc_emb_322_mean', 'desc_emb_323_mean', 'desc_emb_324_mean', 'desc_emb_325_mean', 'desc_emb_326_mean', 'desc_emb_327_mean', 'desc_emb_328_mean', 'desc_emb_329_mean', 'desc_emb_330_mean', 'desc_emb_331_mean', 'desc_emb_332_mean', 'desc_emb_333_mean', 'desc_emb_334_mean', 'desc_emb_335_mean', 'desc_emb_336_mean', 'desc_emb_337_mean', 'desc_emb_338_mean', 'desc_emb_339_mean', 'desc_emb_340_mean', 'desc_emb_341_mean', 'desc_emb_342_mean', 'desc_emb_343_mean', 'desc_emb_344_mean', 'desc_emb_345_mean', 'desc_emb_346_mean', 'desc_emb_347_mean', 'desc_emb_348_mean', 'desc_emb_349_mean', 'desc_emb_350_mean', 'desc_emb_351_mean', 'desc_emb_352_mean', 'desc_emb_353_mean', 'desc_emb_354_mean', 'desc_emb_355_mean', 'desc_emb_356_mean', 'desc_emb_357_mean', 'desc_emb_358_mean', 'desc_emb_359_mean', 'desc_emb_360_mean', 'desc_emb_361_mean', 'desc_emb_362_mean', 'desc_emb_363_mean', 'desc_emb_364_mean', 'desc_emb_365_mean', 'desc_emb_366_mean', 'desc_emb_367_mean', 'desc_emb_368_mean', 'desc_emb_369_mean', 'desc_emb_370_mean', 'desc_emb_371_mean', 'desc_emb_372_mean', 'desc_emb_373_mean', 'desc_emb_374_mean', 'desc_emb_375_mean', 'desc_emb_376_mean', 'desc_emb_377_mean', 'desc_emb_378_mean', 'desc_emb_379_mean', 'desc_emb_380_mean', 'desc_emb_381_mean', 'desc_emb_382_mean', 'desc_emb_383_mean', 'fields.issuetype.name_<lambda>_Improvement', 'fields.issuetype.name_<lambda>_Epic', 'fields.issuetype.name_<lambda>_Bug', 'fields.issuetype.name_<lambda>_Story', 'fields.issuetype.name_<lambda>_Sub-task', 'fields.issuetype.name_<lambda>_New Feature', 'fields.issuetype.name_<lambda>_Defect', 'fields.issuetype.name_<lambda>_Documentation', 'fields.issuetype.name_<lambda>_Test Task', 'fields.issuetype.name_<lambda>_Task', 'fields.priority.name_<lambda>_Highest', 'fields.priority.name_<lambda>_Unset', 'fields.priority.name_<lambda>_Trivial', 'fields.priority.name_<lambda>_Showstopper', 'fields.priority.name_<lambda>_High', 'fields.priority.name_<lambda>_Low', 'fields.priority.name_<lambda>_Severe', 'fields.priority.name_<lambda>_Minor', 'fields.priority.name_<lambda>_Lowest', 'fields.priority.name_<lambda>_Major', 'fields.priority.name_<lambda>_Medium', 'fields.status.name_<lambda>_Complete', 'fields.status.name_<lambda>_Closed', 'fields.status.name_<lambda>_Done', 'fields.status.name_<lambda>_Needs More Info', 'created_month_<lambda>_1', 'created_month_<lambda>_2', 'created_month_<lambda>_3', 'created_month_<lambda>_4', 'created_month_<lambda>_5', 'created_month_<lambda>_6', 'created_month_<lambda>_7', 'created_month_<lambda>_8', 'created_month_<lambda>_9', 'created_month_<lambda>_10', 'created_month_<lambda>_11', 'created_month_<lambda>_12', 'created_day_of_week_<lambda>_0', 'created_day_of_week_<lambda>_1', 'created_day_of_week_<lambda>_2', 'created_day_of_week_<lambda>_3', 'created_day_of_week_<lambda>_4', 'created_day_of_week_<lambda>_5', 'created_day_of_week_<lambda>_6', 'project_duration_days', 'issues_per_day', 'link_density']\n",
      "Sample rows:\n",
      "   project_id         project_name  issue_count  fields.assignee.key_<lambda>  \\\n",
      "0     10001.0             Sawtooth           13                             8   \n",
      "1     10002.0               Fabric          140                            59   \n",
      "2     10100.0  Blockchain Explorer            9                             5   \n",
      "3     10200.0                Cello           12                             8   \n",
      "4     10244.0            Snowstorm           15                             5   \n",
      "\n",
      "   fields.creator.key_<lambda>  fields.reporter.key_<lambda>  total_links  \\\n",
      "0                            8                             9          2.0   \n",
      "1                           70                            70         84.0   \n",
      "2                            5                             6          2.0   \n",
      "3                            8                             8          5.0   \n",
      "4                           12                            11          5.0   \n",
      "\n",
      "   avg_links_per_issue  pct_related_issues  pct_cloned_issues  ...  \\\n",
      "0             0.153846                 1.0                1.0  ...   \n",
      "1             0.600000                 1.0                1.0  ...   \n",
      "2             0.222222                 1.0                1.0  ...   \n",
      "3             0.416667                 1.0                1.0  ...   \n",
      "4             0.333333                 1.0                1.0  ...   \n",
      "\n",
      "  created_day_of_week_<lambda>_0  created_day_of_week_<lambda>_1  \\\n",
      "0                              3                               3   \n",
      "1                             24                              27   \n",
      "2                              3                               1   \n",
      "3                              3                               2   \n",
      "4                              2                               2   \n",
      "\n",
      "   created_day_of_week_<lambda>_2  created_day_of_week_<lambda>_3  \\\n",
      "0                               1                               5   \n",
      "1                              33                              26   \n",
      "2                               2                               1   \n",
      "3                               2                               1   \n",
      "4                               0                               3   \n",
      "\n",
      "   created_day_of_week_<lambda>_4 created_day_of_week_<lambda>_5  \\\n",
      "0                               1                              0   \n",
      "1                              19                              5   \n",
      "2                               1                              0   \n",
      "3                               2                              0   \n",
      "4                               4                              4   \n",
      "\n",
      "  created_day_of_week_<lambda>_6  project_duration_days  issues_per_day  \\\n",
      "0                              0             418.263194        0.031081   \n",
      "1                              6            1758.789838        0.079600   \n",
      "2                              1             785.984815        0.011451   \n",
      "3                              2             732.292697        0.016387   \n",
      "4                              0             844.029456        0.017772   \n",
      "\n",
      "   link_density  \n",
      "0      0.153846  \n",
      "1      0.600000  \n",
      "2      0.222222  \n",
      "3      0.416667  \n",
      "4      0.333333  \n",
      "\n",
      "[5 rows x 454 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(\"..\")  # adjust based on your directory structure\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from FeatureCleaning.CleanDSDTale import export_clean_df  # Adjust import path as needed\n",
    "import dtale\n",
    "\n",
    "# Add to a cell in your notebook\n",
    "import importlib\n",
    "\n",
    "# Force reload the module\n",
    "if 'FeatureCleaning.CleanDSDTale' in sys.modules:\n",
    "    importlib.reload(sys.modules['FeatureCleaning.CleanDSDTale'])\n",
    "    from FeatureCleaning.CleanDSDTale import export_clean_df  # Re-import after reload\n",
    "\n",
    "def extract_sprint_name(sprint_str):\n",
    "    \"\"\"\n",
    "    Extract the sprint name from a sprint string.\n",
    "    For example, from a value like:\n",
    "      \"com.atlassian.greenhopper.service.sprint.Sprint@16353814[id=5599,...,name=Sprint 9,...]\"\n",
    "    this returns \"Sprint 9\".\n",
    "    \"\"\"\n",
    "    if not sprint_str or not isinstance(sprint_str, str):\n",
    "        return None\n",
    "    match = re.search(r'name=([^,]+)', sprint_str)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_sprint_from_fixversions(fix_versions):\n",
    "    \"\"\"\n",
    "    Given a list of fixVersions dictionaries, return the first version name containing 'Sprint'.\n",
    "    \"\"\"\n",
    "    if isinstance(fix_versions, list):\n",
    "        for version in fix_versions:\n",
    "            name = version.get(\"name\", \"\")\n",
    "            if \"Sprint\" in name:\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "def add_sprint_field(df):\n",
    "    \"\"\"\n",
    "    Add a standardized 'sprint' column to the DataFrame.\n",
    "    First, try using customfield_10557; if not present, fall back to fixVersions.\n",
    "    \"\"\"\n",
    "    if \"fields.customfield_10557\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.customfield_10557\"].apply(extract_sprint_name)\n",
    "    elif \"fields.fixVersions\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.fixVersions\"].apply(extract_sprint_from_fixversions)\n",
    "    else:\n",
    "        df[\"sprint\"] = None\n",
    "    return df\n",
    "\n",
    "def extract_planning_fields(df):\n",
    "    \"\"\"\n",
    "    From the full cleaned DataFrame, extract only the planning-phase fields.\n",
    "    Enhanced to include all critical planning-phase data for task-level and project-level estimation.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = add_sprint_field(df)\n",
    "    \n",
    "    planning_cols = [\n",
    "        # Basic issue identification\n",
    "        \"key\",                    # Issue key\n",
    "        \"fields.summary\",         # Summary text\n",
    "        \n",
    "        # Issue classification\n",
    "        \"fields.issuetype.name\",  # Type\n",
    "        \"fields.status.name\",     # Status\n",
    "        \"fields.priority.name\",   # Priority\n",
    "        \n",
    "        # People involved\n",
    "        \"fields.assignee.key\",    # Assignee ID\n",
    "        \"fields.creator.key\",     # Creator ID\n",
    "        \"fields.reporter.key\",    # Reporter ID\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\",      # Project ID\n",
    "        \"fields.project.name\",    # Project name\n",
    "        \n",
    "        # Time information\n",
    "        \"fields.created\",         # Creation date\n",
    "        \n",
    "        # Relationships\n",
    "        \"sprint\",                 # Extracted sprint info\n",
    "        \"issuelinks_total\",       # Total link count\n",
    "        \"has_issuelinks_relates\", # Has 'relates to' links\n",
    "        \"has_issuelinks_cloners\", # Has 'cloners' links\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\",      # Components\n",
    "        \"fields.labels\",          # Labels\n",
    "        \n",
    "        # For training purposes\n",
    "        \"fields.resolutiondate\"   # Used to calculate resolution time\n",
    "    ]\n",
    "    \n",
    "    # Get all description embedding columns (they start with \"desc_emb_\")\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"desc_emb_\")]\n",
    "    \n",
    "    # Combine all desired columns\n",
    "    all_cols = planning_cols + embedding_cols\n",
    "    \n",
    "    # Filter to only include columns that exist in the DataFrame\n",
    "    available_cols = [col for col in all_cols if col in df.columns]\n",
    "    \n",
    "    # Create the base planning DataFrame\n",
    "    planning_df = df[available_cols].copy()\n",
    "    \n",
    "    # Add derived features\n",
    "    \n",
    "    # Component and label counts\n",
    "    if \"fields.components\" in planning_df.columns:\n",
    "        planning_df[\"component_count\"] = planning_df[\"fields.components\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    if \"fields.labels\" in planning_df.columns:\n",
    "        planning_df[\"label_count\"] = planning_df[\"fields.labels\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Creation date features\n",
    "    if \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"created_date\"] = pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        planning_df[\"created_day_of_week\"] = planning_df[\"created_date\"].dt.dayofweek\n",
    "        planning_df[\"created_month\"] = planning_df[\"created_date\"].dt.month\n",
    "        planning_df[\"created_year\"] = planning_df[\"created_date\"].dt.year\n",
    "    \n",
    "    # Calculate resolution time (if available - for training only)\n",
    "    if \"fields.resolutiondate\" in planning_df.columns and \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"resolution_time\"] = (\n",
    "            pd.to_datetime(planning_df[\"fields.resolutiondate\"]) - \n",
    "            pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        # Drop the raw resolution date as we now have the derived feature\n",
    "        planning_df = planning_df.drop(columns=[\"fields.resolutiondate\"])\n",
    "    \n",
    "    return planning_df\n",
    "\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    Enhanced with project duration metrics.\n",
    "    \"\"\"\n",
    "    # Group by project ID and name\n",
    "    agg_dict = {\n",
    "        # Issue counts\n",
    "        'key': 'count',  # Total number of issues\n",
    "    }\n",
    "    \n",
    "    # Add issue type distribution if available\n",
    "    if 'fields.issuetype.name' in df.columns:\n",
    "        agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add priority distribution if available  \n",
    "    if 'fields.priority.name' in df.columns:\n",
    "        agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add status distribution if available\n",
    "    if 'fields.status.name' in df.columns:\n",
    "        agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Team size metrics if available\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    # Connectivity metrics if available\n",
    "    if 'issuelinks_total' in df.columns:\n",
    "        agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'has_issuelinks_relates' in df.columns:\n",
    "        agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "    \n",
    "    # Component usage if available\n",
    "    if 'fields.components' in df.columns:\n",
    "        agg_dict['fields.components'] = lambda x: set(item for sublist in x if isinstance(sublist, list) for item in sublist)\n",
    "    \n",
    "    # Add component and label counts if available\n",
    "    if 'component_count' in df.columns:\n",
    "        agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'label_count' in df.columns:\n",
    "        agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "    # Add sprint information if available\n",
    "    if 'sprint' in df.columns:\n",
    "        agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add creation time features if available\n",
    "    if 'created_date' in df.columns:\n",
    "        # Min and max dates to calculate project duration\n",
    "        agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "    if 'created_month' in df.columns:\n",
    "        agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    if 'created_day_of_week' in df.columns:\n",
    "        agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add resolution time statistics if available (for model training)\n",
    "    if 'resolution_time' in df.columns:\n",
    "        agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "    # Add description embedding averages if available\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "    for col in embedding_cols:\n",
    "        agg_dict[col] = 'mean'\n",
    "    \n",
    "    # Perform the groupby aggregation\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Reset index to convert groupby result to regular DataFrame\n",
    "    project_features = project_features.reset_index()\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Add derived metrics\n",
    "    \n",
    "    # Compute team-related metrics if available\n",
    "    if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['issue_to_assignee_ratio'] = (\n",
    "            project_features['key_count'] / project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    \n",
    "    # Calculate link density if available\n",
    "    if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "        )\n",
    "    \n",
    "    # Calculate temporal metrics (if date information is available)\n",
    "    if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "        # Project duration in days\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['created_date_max']) - \n",
    "             pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        project_features['issue_creation_rate'] = (\n",
    "            project_features['key_count'] / \n",
    "            project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "        )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "def export_clean_planningphase_df(open_dtale=True):\n",
    "    \"\"\"\n",
    "    Run the full cleaning pipeline to obtain a cleaned task-level DataFrame,\n",
    "    then extract only the planning-phase fields to create a planning-phase dataset.\n",
    "    If open_dtale is True, launch a D-Tale session for interactive exploration.\n",
    "    \n",
    "    Returns:\n",
    "      planning_df (pd.DataFrame): The planning-phase DataFrame with the selected fields.\n",
    "    \"\"\"\n",
    "    full_df = export_clean_df()  # This returns your fully cleaned task-level DataFrame.\n",
    "    planning_df = extract_planning_fields(full_df)\n",
    "    project_features_df = create_project_features(planning_df)\n",
    "\n",
    "    \n",
    "    if open_dtale:\n",
    "        print(\"Launching D-Tale session for planning-phase DataFrame...\")\n",
    "        d_pf = dtale.show(project_features_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "\n",
    "        d_pf.open_browser()\n",
    "        \n",
    "    return project_features_df\n",
    "\n",
    "def expand_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Expand categorical distributions stored as dictionaries.\n",
    "    \"\"\"\n",
    "    # Identify columns that contain dictionaries\n",
    "    dict_cols = [col for col in df.columns \n",
    "                if isinstance(df[col].iloc[0], dict) if len(df) > 0]\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in dict_cols:\n",
    "        # Find all unique categories across all projects\n",
    "        all_categories = set()\n",
    "        for d in df[col]:\n",
    "            if isinstance(d, dict):\n",
    "                all_categories.update(d.keys())\n",
    "        \n",
    "        # Create a column for each category\n",
    "        for category in all_categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            result_df[new_col_name] = df[col].apply(\n",
    "                lambda x: x.get(category, 0) if isinstance(x, dict) else 0\n",
    "            )\n",
    "        \n",
    "        # Drop the original dictionary column\n",
    "        result_df = result_df.drop(columns=[col])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# def create_project_features(df):\n",
    "#     \"\"\"\n",
    "#     Aggregate task-level features to create project-level features.\n",
    "#     Enhanced with project duration metrics and fixed unhashable dict issue.\n",
    "#     \"\"\"\n",
    "#     # Group by project ID and name\n",
    "#     agg_dict = {\n",
    "#         # Issue counts\n",
    "#         'key': 'count',  # Total number of issues\n",
    "#     }\n",
    "    \n",
    "#     # Add issue type distribution if available\n",
    "#     if 'fields.issuetype.name' in df.columns:\n",
    "#         agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add priority distribution if available  \n",
    "#     if 'fields.priority.name' in df.columns:\n",
    "#         agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add status distribution if available\n",
    "#     if 'fields.status.name' in df.columns:\n",
    "#         agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Team size metrics if available\n",
    "#     if 'fields.assignee.key' in df.columns:\n",
    "#         agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "#     if 'fields.creator.key' in df.columns:\n",
    "#         agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "    \n",
    "#     # Connectivity metrics if available\n",
    "#     if 'issuelinks_total' in df.columns:\n",
    "#         agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "#     if 'has_issuelinks_relates' in df.columns:\n",
    "#         agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "    \n",
    "#     # Component usage if available - FIX for unhashable dict error\n",
    "#     if 'fields.components' in df.columns:\n",
    "#         # Extract component names safely, handling different data structures\n",
    "#         agg_dict['fields.components'] = lambda x: set([\n",
    "#             str(comp_name) for row in x \n",
    "#             for comp_name in (row if isinstance(row, list) else []) \n",
    "#         ])\n",
    "    \n",
    "#     # Add component and label counts if available\n",
    "#     if 'component_count' in df.columns:\n",
    "#         agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "#     if 'label_count' in df.columns:\n",
    "#         agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "#     # Add sprint information if available\n",
    "#     if 'sprint' in df.columns:\n",
    "#         agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add creation time features if available\n",
    "#     if 'created_date' in df.columns:\n",
    "#         # Min and max dates to calculate project duration\n",
    "#         agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "#     if 'created_month' in df.columns:\n",
    "#         agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     if 'created_day_of_week' in df.columns:\n",
    "#         agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add resolution time statistics if available (for model training)\n",
    "#     if 'resolution_time' in df.columns:\n",
    "#         agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "#     # Add description embedding averages if available\n",
    "#     embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "#     for col in embedding_cols:\n",
    "#         agg_dict[col] = 'mean'\n",
    "    \n",
    "#     # Perform the groupby aggregation\n",
    "#     project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "#     # Flatten the multi-level columns\n",
    "#     project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "#                                for col in project_features.columns]\n",
    "    \n",
    "#     # Reset index to convert groupby result to regular DataFrame\n",
    "#     project_features = project_features.reset_index()\n",
    "    \n",
    "#     # Expand the categorical distributions\n",
    "#     project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "#     # Add derived metrics\n",
    "    \n",
    "#     # Compute team-related metrics if available\n",
    "#     if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "#         project_features['issue_to_assignee_ratio'] = (\n",
    "#             project_features['key_count'] / project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "#         )\n",
    "    \n",
    "#     # Calculate link density if available\n",
    "#     if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "#         project_features['link_density'] = (\n",
    "#             project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "#         )\n",
    "    \n",
    "#     # Calculate temporal metrics (if date information is available)\n",
    "#     if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "#         # Project duration in days\n",
    "#         project_features['project_duration_days'] = (\n",
    "#             (pd.to_datetime(project_features['created_date_max']) - \n",
    "#              pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "#         )\n",
    "        \n",
    "#         # Issue creation rate (issues per day) - a proxy for velocity\n",
    "#         project_features['issue_creation_rate'] = (\n",
    "#             project_features['key_count'] / \n",
    "#             project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "#         )\n",
    "    \n",
    "#     return project_features\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    Returns a DataFrame with clean, readable column names.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with task-level features\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with project-level features and clean field names\n",
    "    \"\"\"\n",
    "    # Define the aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    agg_dict['key'] = 'count'  # This will create 'key_count' in the result\n",
    "    \n",
    "    # Add issue type distribution if available\n",
    "    if 'fields.issuetype.name' in df.columns:\n",
    "        agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add priority distribution if available  \n",
    "    if 'fields.priority.name' in df.columns:\n",
    "        agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add status distribution if available\n",
    "    if 'fields.status.name' in df.columns:\n",
    "        agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Team size metrics if available\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "        \n",
    "    if 'fields.reporter.key' in df.columns:\n",
    "        agg_dict['fields.reporter.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    # Connectivity metrics if available\n",
    "    if 'issuelinks_total' in df.columns:\n",
    "        agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'has_issuelinks_relates' in df.columns:\n",
    "        agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "        \n",
    "    if 'has_issuelinks_cloners' in df.columns:\n",
    "        agg_dict['has_issuelinks_cloners'] = 'mean'\n",
    "    \n",
    "    # Component usage if available - Fix for unhashable dict error\n",
    "    if 'fields.components' in df.columns:\n",
    "        # Extract component names safely, handling different data structures\n",
    "        agg_dict['fields.components'] = lambda x: set([\n",
    "            str(comp_name) for row in x \n",
    "            for comp_name in (row if isinstance(row, list) else []) \n",
    "        ])\n",
    "    \n",
    "    # Add component and label counts\n",
    "    if 'component_count' in df.columns:\n",
    "        agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'label_count' in df.columns:\n",
    "        agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "    # Add sprint information if available\n",
    "    if 'sprint' in df.columns:\n",
    "        agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add creation time features \n",
    "    if 'created_date' in df.columns:\n",
    "        agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "    if 'created_month' in df.columns:\n",
    "        agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    if 'created_day_of_week' in df.columns:\n",
    "        agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add resolution time statistics\n",
    "    if 'resolution_time' in df.columns:\n",
    "        agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "    # Add description embedding averages\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "    for col in embedding_cols:\n",
    "        agg_dict[col] = 'mean'\n",
    "    \n",
    "    # Perform the groupby aggregation\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Reset index to convert groupby result to regular DataFrame\n",
    "    project_features = project_features.reset_index()\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Create a dictionary of old to new column names\n",
    "    rename_mapping = {\n",
    "        'fields.project.id': 'project_id',\n",
    "        'fields.project.name': 'project_name',\n",
    "        'key_count': 'issue_count',\n",
    "        'fields.assignee.key': 'assignee_count',\n",
    "        'fields.creator.key': 'creator_count',\n",
    "        'fields.reporter.key': 'reporter_count',\n",
    "        'fields.issuetype.name': 'issue_types',\n",
    "        'fields.priority.name': 'priorities',\n",
    "        'fields.status.name': 'statuses',\n",
    "        'issuelinks_total_sum': 'total_links',\n",
    "        'issuelinks_total_mean': 'avg_links_per_issue',\n",
    "        'has_issuelinks_relates_mean': 'pct_related_issues',\n",
    "        'has_issuelinks_cloners_mean': 'pct_cloned_issues',\n",
    "        'fields.components': 'component_names',\n",
    "        'component_count_sum': 'total_components',\n",
    "        'component_count_mean': 'avg_components_per_issue',\n",
    "        'label_count_sum': 'total_labels',\n",
    "        'label_count_mean': 'avg_labels_per_issue',\n",
    "        'sprint': 'sprints',\n",
    "        'created_date_min': 'project_start_date',\n",
    "        'created_date_max': 'project_latest_date',\n",
    "        'created_month': 'creation_months',\n",
    "        'created_day_of_week': 'creation_weekdays',\n",
    "        'resolution_time_mean': 'avg_resolution_hours',\n",
    "        'resolution_time_median': 'median_resolution_hours',\n",
    "        'resolution_time_min': 'min_resolution_hours',\n",
    "        'resolution_time_max': 'max_resolution_hours',\n",
    "        'resolution_time_std': 'resolution_hours_std',\n",
    "        'resolution_time_sum': 'total_resolution_hours'\n",
    "    }\n",
    "    \n",
    "    # Only rename columns that exist in the DataFrame\n",
    "    columns_to_rename = {k: v for k, v in rename_mapping.items() if k in project_features.columns}\n",
    "    project_features = project_features.rename(columns=columns_to_rename)\n",
    "    \n",
    "    # Add derived metrics with clean names\n",
    "    \n",
    "    # Project duration in days - using the original column names first, then renaming\n",
    "    if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['created_date_max']) - \n",
    "             pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        if 'key_count' in project_features.columns:  # Use original column name\n",
    "            project_features['issues_per_day'] = (\n",
    "                project_features['key_count'] / \n",
    "                project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "            )\n",
    "    elif 'project_start_date' in project_features.columns and 'project_latest_date' in project_features.columns:\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['project_latest_date']) - \n",
    "             pd.to_datetime(project_features['project_start_date'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        if 'issue_count' in project_features.columns:  # Use renamed column\n",
    "            project_features['issues_per_day'] = (\n",
    "                project_features['issue_count'] / \n",
    "                project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "            )\n",
    "    \n",
    "    # Compute team-related metrics - check both original and renamed columns\n",
    "    if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['issues_per_assignee'] = (\n",
    "            project_features['key_count'] / \n",
    "            project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    elif 'assignee_count' in project_features.columns and 'issue_count' in project_features.columns:\n",
    "        project_features['issues_per_assignee'] = (\n",
    "            project_features['issue_count'] / \n",
    "            project_features['assignee_count'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    \n",
    "    # Calculate network metrics - check both original and renamed columns\n",
    "    if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "        )\n",
    "    elif 'total_links' in project_features.columns and 'issue_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['total_links'] / project_features['issue_count']\n",
    "        )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df_planning = export_clean_planningphase_df(open_dtale=True)\n",
    "    print(\"Planning-phase DataFrame columns:\")\n",
    "    print(df_planning.columns.tolist())\n",
    "    print(\"Sample rows:\")\n",
    "    print(df_planning.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
