{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: MongoDB ...\n",
      "Found 137172 total issues in 'MongoDB'. Processing in batches of 500...\n",
      "Using fixed maximum of 100 records. Will retrieve 100 issues.\n",
      "Final sample for 'MongoDB': 100 issues (out of 137172 total).\n",
      "  - Processing changelog history batch 1/1\n",
      "Data processed. Launching D-Tale session...\n",
      "âœ… D-Tale session launched successfully.\n",
      "Launching D-Tale session for planning-phase DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2016-05-31 03:37:02', '2015-04-09 01:57:36', '2016-06-02 19:34:23',\n",
      " '2016-08-30 18:49:27', '2021-11-11 18:46:00', '2021-10-14 01:55:17',\n",
      " '2016-08-26 11:59:57', '2021-11-17 18:36:08', '2021-12-09 17:18:04',\n",
      " '2021-06-14 08:18:38', '2021-05-11 17:40:37', '2020-12-10 15:57:59',\n",
      " '2020-04-27 22:15:42', '2020-04-07 18:45:20', '2020-01-15 16:48:20',\n",
      " '2021-12-22 20:20:31', '2020-06-08 18:34:38', '2019-04-30 15:29:29',\n",
      " '2018-12-14 02:39:58', '2018-08-16 15:19:04', '2018-06-11 19:27:35',\n",
      " '2018-04-23 17:53:21', '2018-03-06 23:25:03', '2018-02-05 21:41:41',\n",
      " '2017-04-18 17:41:48', '2017-10-18 16:13:51', '2016-12-05 16:06:58',\n",
      " '2016-10-04 15:48:29', '2016-05-23 17:45:00', '2015-12-23 15:41:37',\n",
      " '2015-09-24 22:02:51', '2015-09-15 18:16:27', '2015-08-14 21:23:05',\n",
      " '2015-08-06 17:29:43', '2015-01-22 17:57:26', '2014-12-12 17:25:31',\n",
      " '2019-06-11 18:56:00', '2014-05-21 17:42:40', '2013-08-19 20:05:48',\n",
      " '2013-08-26 19:44:00', '2012-06-13 14:04:38', '2019-05-16 16:00:22',\n",
      " '2019-07-11 19:11:35', '2018-09-10 18:29:54', '2021-05-25 17:33:17',\n",
      " '2017-08-05 00:14:14', '2017-11-10 20:50:16', '2016-08-01 19:50:04',\n",
      " '2015-04-21 00:07:33', '2021-04-26 13:26:15', '2021-01-11 16:55:02',\n",
      " '2017-07-28 14:31:57', '2015-09-20 13:09:17', '2015-11-03 16:25:03',\n",
      " '2015-05-29 14:11:48', '2015-05-29 14:16:04', '2020-09-21 13:05:26',\n",
      " '2020-04-28 16:41:15', '2017-02-16 23:08:35', '2020-02-03 16:08:39',\n",
      " '2020-05-14 18:40:04', '2019-03-07 20:04:21', '2020-06-01 15:49:01',\n",
      " '2020-04-20 19:25:10', '2020-05-12 16:52:24', '2018-12-04 17:59:22',\n",
      " '2017-09-13 19:27:37', '2015-11-11 19:29:48', '2018-02-06 15:37:33',\n",
      " '2016-07-27 21:47:10', '2016-03-28 23:31:07', '2019-09-27 20:37:33',\n",
      " '2015-06-12 20:45:19', '2015-06-10 01:01:34', '2014-09-18 22:04:10',\n",
      " '2014-10-04 15:39:27', '2016-07-27 19:03:56', '2013-05-10 15:07:53',\n",
      " '2021-08-09 23:40:35', '2020-11-25 15:37:54', '2020-12-03 21:03:43',\n",
      " '2019-03-25 09:16:25', '2021-09-09 21:29:08', '2021-05-04 17:22:48',\n",
      " '2020-02-10 13:42:48', '2018-07-18 16:54:36', '2014-10-30 15:29:06']\n",
      "Length: 87, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2016-03-16 01:19:06', '2015-01-06 07:07:18', '2016-04-27 13:40:31',\n",
      " '2016-01-13 17:02:40', '2021-10-22 18:31:50', '2021-08-09 16:52:52',\n",
      " '2016-08-25 14:06:56', '2021-11-16 01:03:19', '2021-09-27 16:59:47',\n",
      " '2021-04-19 15:24:52', '2021-02-09 16:02:58', '2020-11-01 19:17:19',\n",
      " '2020-04-06 16:08:29', '2020-03-16 13:41:45', '2020-01-09 20:26:27',\n",
      " '2019-09-25 22:32:34', '2019-06-24 18:12:05', '2019-04-24 21:11:25',\n",
      " '2018-12-12 20:05:42', '2018-08-15 17:59:27', '2018-06-09 13:27:51',\n",
      " '2018-03-28 12:02:40', '2018-02-09 00:04:34', '2017-11-15 16:22:51',\n",
      " '2017-04-04 20:57:38', '2017-03-10 18:59:51', '2016-11-30 18:45:00',\n",
      " '2016-09-01 10:08:20', '2016-05-23 17:32:13', '2015-12-18 21:38:13',\n",
      " '2015-09-24 21:47:01', '2015-09-11 13:49:54', '2015-08-14 20:34:00',\n",
      " '2015-07-16 22:54:39', '2014-11-12 04:14:09', '2014-05-27 22:45:58',\n",
      " '2014-04-14 15:01:55', '2013-12-02 21:32:00', '2013-08-16 23:01:56',\n",
      " '2013-07-25 23:21:48', '2011-05-12 16:57:24', '2019-01-10 18:29:10',\n",
      " '2019-06-24 22:02:14', '2018-08-24 20:40:04', '2021-04-29 20:27:47',\n",
      " '2017-08-04 23:56:03', '2017-07-14 23:30:55', '2016-07-29 01:37:35',\n",
      " '2015-04-20 16:56:04', '2020-08-31 20:04:28', '2021-01-11 15:45:34',\n",
      " '2017-07-14 19:27:52', '2015-07-01 06:05:09', '2015-07-22 21:29:16',\n",
      " '2011-09-07 18:43:50', '2010-09-19 21:37:54', '2020-05-06 13:30:06',\n",
      " '2020-04-28 15:41:52', '2017-02-15 20:45:41', '2020-01-27 13:55:30',\n",
      " '2020-04-23 21:40:28', '2018-09-18 20:27:40', '2020-05-31 03:19:13',\n",
      " '2020-04-06 21:30:19', '2020-01-06 22:28:12', '2018-12-04 02:20:48',\n",
      " '2017-09-12 21:36:30', '2014-08-26 16:27:18', '2017-09-07 16:37:21',\n",
      " '2016-04-22 15:48:53', '2015-12-30 09:16:39', '2015-07-23 16:53:36',\n",
      " '2015-06-12 19:39:04', '2015-06-04 08:29:02', '2014-09-18 21:49:13',\n",
      " '2014-09-17 17:27:19', '2014-04-23 17:41:30', '2013-01-21 13:16:26',\n",
      " '2021-08-06 14:03:10', '2020-08-17 14:40:49', '2017-03-17 16:47:33',\n",
      " '2019-03-12 13:06:47', '2021-09-08 15:49:14', '2021-03-30 14:06:05',\n",
      " '2019-11-04 10:52:33', '2018-06-11 02:28:00', '2014-08-26 18:24:21']\n",
      "Length: 87, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2016-07-28 16:09:57', '2015-04-16 21:02:59', '2017-07-19 18:12:32',\n",
      " '2018-05-08 17:50:20', '2021-11-11 18:46:00', '2021-10-14 01:55:17',\n",
      " '2016-09-09 13:38:20', '2021-11-17 18:36:08', '2021-12-09 17:18:04',\n",
      " '2021-10-06 19:28:14', '2021-05-11 17:40:37', '2020-12-11 08:17:24',\n",
      " '2020-09-22 19:58:51', '2020-04-07 18:45:21', '2020-01-15 16:48:26',\n",
      " '2021-12-28 20:13:29', '2020-06-09 04:52:30', '2019-04-30 15:29:29',\n",
      " '2018-12-14 02:39:58', '2018-08-16 19:20:07', '2018-06-11 19:40:06',\n",
      " '2018-04-24 23:54:30', '2018-03-16 21:11:02', '2018-04-12 00:19:59',\n",
      " '2017-12-06 21:15:33', '2017-10-23 19:27:33', '2017-04-05 11:43:15',\n",
      " '2016-10-04 15:48:29', '2016-06-06 21:16:27', '2016-11-21 18:57:50',\n",
      " '2015-10-07 21:35:23', '2015-09-15 18:16:39', '2015-09-29 17:38:32',\n",
      " '2015-09-19 00:20:58', '2015-01-24 17:18:31', '2015-07-16 17:25:35',\n",
      " '2019-06-11 18:56:00', '2014-12-10 23:18:23', '2016-07-11 17:37:21',\n",
      " '2016-07-11 17:40:21', '2012-08-15 14:24:08', '2019-05-16 16:00:22',\n",
      " '2019-07-11 19:11:35', '2018-12-18 20:58:40', '2021-05-25 17:33:55',\n",
      " '2019-11-19 05:03:45', '2017-11-10 20:53:50', '2019-03-19 23:21:00',\n",
      " '2015-05-20 16:33:48', '2021-04-26 13:26:15', '2021-12-16 19:01:43',\n",
      " '2017-07-28 14:31:57', '2015-09-20 13:09:17', '2017-01-12 16:12:21',\n",
      " '2015-05-29 14:11:48', '2017-01-12 16:08:58', '2021-04-01 02:08:07',\n",
      " '2020-04-28 16:41:15', '2020-06-22 16:57:16', '2020-06-22 16:50:01',\n",
      " '2020-09-02 17:19:03', '2020-06-22 16:47:12', '2020-06-12 03:44:23',\n",
      " '2020-06-24 17:12:14', '2020-06-24 17:04:16', '2020-06-24 16:57:46',\n",
      " '2020-06-24 16:48:04', '2019-04-15 17:29:10', '2018-09-11 21:23:30',\n",
      " '2017-11-03 11:12:11', '2017-11-03 11:18:21', '2019-09-27 20:37:33',\n",
      " '2017-11-03 11:16:53', '2017-11-03 11:18:00', '2014-09-18 22:04:10',\n",
      " '2017-01-11 22:39:22', '2017-01-11 22:43:11', '2013-05-10 17:56:45',\n",
      " '2021-08-09 23:40:35', '2021-02-25 12:03:08', '2020-12-03 21:03:43',\n",
      " '2019-04-01 09:04:02', '2021-09-09 21:29:08', '2021-10-01 18:45:05',\n",
      " '2020-07-30 20:01:54', '2018-08-07 19:14:52', '2016-10-19 14:15:00']\n",
      "Length: 87, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2011-05-12 16:57:24', '2015-04-20 16:56:04', '2018-08-24 20:40:04',\n",
      " '2014-08-26 18:24:21', '2017-03-17 16:47:33', '2013-01-21 13:16:26',\n",
      " '2017-07-14 19:27:52', '2014-08-26 16:27:18', '2015-07-01 06:05:09',\n",
      " '2016-01-13 17:02:40', '2020-08-31 20:04:28', '2015-01-06 07:07:18',\n",
      " '2017-02-15 20:45:41', '2010-09-19 21:37:54', '2016-08-25 14:06:56',\n",
      " '2019-03-12 13:06:47', '2020-04-28 15:41:52', '2021-08-09 16:52:52',\n",
      " '2019-01-10 18:29:10', '2020-05-06 13:30:06']\n",
      "Length: 20, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2021-11-16 01:03:19', '2021-04-29 20:27:47', '2019-06-24 22:02:14',\n",
      " '2021-09-08 15:49:14', '2021-08-06 14:03:10', '2017-09-07 16:37:21',\n",
      " '2021-01-11 15:45:34', '2014-08-26 16:27:18', '2015-07-01 06:05:09',\n",
      " '2016-04-27 13:40:31', '2020-08-31 20:04:28', '2016-03-16 01:19:06',\n",
      " '2020-05-31 03:19:13', '2015-07-22 21:29:16', '2016-08-25 14:06:56',\n",
      " '2019-03-12 13:06:47', '2020-04-28 15:41:52', '2021-10-22 18:31:50',\n",
      " '2019-01-10 18:29:10', '2020-05-06 13:30:06']\n",
      "Length: 20, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning-phase DataFrame columns:\n",
      "['project_id', 'project_name', 'issue_count', 'fields.assignee.key_<lambda>', 'fields.creator.key_<lambda>', 'fields.reporter.key_<lambda>', 'total_links', 'avg_links_per_issue', 'pct_cloned_issues', 'fields.components_<lambda>', 'total_components', 'avg_components_per_issue', 'total_labels', 'avg_labels_per_issue', 'project_start_date', 'project_latest_date', 'avg_resolution_hours', 'median_resolution_hours', 'min_resolution_hours', 'max_resolution_hours', 'resolution_hours_std', 'total_resolution_hours', 'fields.issuetype.name_<lambda>_Task', 'fields.issuetype.name_<lambda>_Bug', 'fields.issuetype.name_<lambda>_Problem Ticket', 'fields.issuetype.name_<lambda>_New Feature', 'fields.issuetype.name_<lambda>_Improvement', 'fields.priority.name_<lambda>_Major - P3', 'fields.priority.name_<lambda>_Missing', 'fields.priority.name_<lambda>_Critical - P2', 'fields.priority.name_<lambda>_Unknown', 'fields.priority.name_<lambda>_Trivial - P5', 'fields.priority.name_<lambda>_Minor - P4', 'fields.status.name_<lambda>_Resolved', 'fields.status.name_<lambda>_Closed', 'created_month_<lambda>_1', 'created_month_<lambda>_2', 'created_month_<lambda>_3', 'created_month_<lambda>_4', 'created_month_<lambda>_5', 'created_month_<lambda>_6', 'created_month_<lambda>_7', 'created_month_<lambda>_8', 'created_month_<lambda>_9', 'created_month_<lambda>_10', 'created_month_<lambda>_11', 'created_month_<lambda>_12', 'created_day_of_week_<lambda>_0', 'created_day_of_week_<lambda>_1', 'created_day_of_week_<lambda>_2', 'created_day_of_week_<lambda>_3', 'created_day_of_week_<lambda>_4', 'created_day_of_week_<lambda>_5', 'created_day_of_week_<lambda>_6', 'project_duration_days', 'issues_per_day', 'link_density']\n",
      "Sample rows:\n",
      "   project_id   project_name  issue_count  fields.assignee.key_<lambda>  \\\n",
      "0     10000.0    Core Server           34                            32   \n",
      "1     10004.0  Python Driver            5                             3   \n",
      "2     10005.0    Ruby Driver            2                             1   \n",
      "3     10030.0       C Driver            5                             5   \n",
      "4     10041.0      C# Driver            3                             3   \n",
      "\n",
      "   fields.creator.key_<lambda>  fields.reporter.key_<lambda>  total_links  \\\n",
      "0                           29                            29         23.0   \n",
      "1                            4                             4         10.0   \n",
      "2                            1                             1          2.0   \n",
      "3                            5                             5          8.0   \n",
      "4                            3                             3          3.0   \n",
      "\n",
      "   avg_links_per_issue  pct_cloned_issues  \\\n",
      "0             0.676471                1.0   \n",
      "1             2.000000                1.0   \n",
      "2             1.000000                1.0   \n",
      "3             1.600000                1.0   \n",
      "4             1.000000                1.0   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       fields.components_<lambda>  \\\n",
      "0  {{'self': 'https://jira.mongodb.org/rest/api/2/component/10110', 'id': '10110', 'name': 'Storage'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10030', 'id': '10030', 'name': 'Querying'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10433', 'id': '10433', 'name': 'Security'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10031', 'id': '10031', 'name': 'Build', 'description': 'SCons, etc.  '}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10125', 'id': '10125', 'name': 'Testing Infrastructure'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10840', 'id': '10840', 'name': 'Aggregation Framework'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10025', 'id': '10025', 'name': 'Admin'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10019', 'id': '10019', 'name': 'Performance'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/13940', 'id': '13940', 'name': 'WiredTiger'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10010', 'id': '10010', 'name': 'Replication'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10080', 'id': '10080', 'name': 'Sharding'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10127', 'id': '10127', 'name': 'Networking'}}   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {{'self': 'https://jira.mongodb.org/rest/api/2/component/16834', 'id': '16834', 'name': 'Internal'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/16839', 'id': '16839', 'name': 'Wire Protocol'}}   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 {{'self': 'https://jira.mongodb.org/rest/api/2/component/11749', 'id': '11749', 'name': 'BSON', 'description': 'MongoDB Ruby Driver BSON Gem'}}   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            {{'self': 'https://jira.mongodb.org/rest/api/2/component/14449', 'id': '14449', 'name': 'libbson'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/14448', 'id': '14448', 'name': 'build'}}   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          {{'self': 'https://jira.mongodb.org/rest/api/2/component/13033', 'id': '13033', 'name': 'Connectivity', 'description': 'Related to connections, cluster management, etc...'}, {'self': 'https://jira.mongodb.org/rest/api/2/component/10107', 'id': '10107', 'name': 'Documentation'}}   \n",
      "\n",
      "   ...  created_day_of_week_<lambda>_0  created_day_of_week_<lambda>_1  \\\n",
      "0  ...                               8                               4   \n",
      "1  ...                               1                               0   \n",
      "2  ...                               1                               0   \n",
      "3  ...                               2                               2   \n",
      "4  ...                               1                               0   \n",
      "\n",
      "   created_day_of_week_<lambda>_2  created_day_of_week_<lambda>_3  \\\n",
      "0                               8                               6   \n",
      "1                               0                               1   \n",
      "2                               0                               0   \n",
      "3                               1                               0   \n",
      "4                               0                               0   \n",
      "\n",
      "  created_day_of_week_<lambda>_4 created_day_of_week_<lambda>_5  \\\n",
      "0                              6                              1   \n",
      "1                              3                              0   \n",
      "2                              1                              0   \n",
      "3                              0                              0   \n",
      "4                              2                              0   \n",
      "\n",
      "   created_day_of_week_<lambda>_6  project_duration_days  issues_per_day  \\\n",
      "0                               1            3840.337442        0.008853   \n",
      "1                               0            2201.147025        0.002272   \n",
      "2                               0             304.057060        0.006578   \n",
      "3                               0            2569.892280        0.001946   \n",
      "4                               0            1602.885845        0.001872   \n",
      "\n",
      "   link_density  \n",
      "0      0.676471  \n",
      "1      2.000000  \n",
      "2      1.000000  \n",
      "3      1.600000  \n",
      "4      1.000000  \n",
      "\n",
      "[5 rows x 57 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Address already in use"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Port 40000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n",
      "2025-03-03 09:22:02,433 - ERROR    - 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/serving.py\", line 759, in __init__\n",
      "    self.server_bind()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/http/server.py\", line 137, in server_bind\n",
      "    socketserver.TCPServer.server_bind(self)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/socketserver.py\", line 466, in server_bind\n",
      "    self.socket.bind(self.server_address)\n",
      "OSError: [Errno 48] Address already in use\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/app.py\", line 884, in _start\n",
      "    app.run(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/app.py\", line 225, in run\n",
      "    super(DtaleFlask, self).run(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/app.py\", line 625, in run\n",
      "    run_simple(t.cast(str, host), port, self, **options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/serving.py\", line 1093, in run_simple\n",
      "    srv = make_server(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/serving.py\", line 930, in make_server\n",
      "    return ThreadedWSGIServer(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/serving.py\", line 782, in __init__\n",
      "    sys.exit(1)\n",
      "SystemExit: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(\"..\")  # adjust based on your directory structure\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from FeatureCleaning.CleanDSDTale import export_clean_df  # Adjust import path as needed\n",
    "import dtale\n",
    "\n",
    "# Add to a cell in your notebook\n",
    "import importlib\n",
    "\n",
    "# Force reload the module\n",
    "if 'FeatureCleaning.CleanDSDTale' in sys.modules:\n",
    "    importlib.reload(sys.modules['FeatureCleaning.CleanDSDTale'])\n",
    "    from FeatureCleaning.CleanDSDTale import export_clean_df  # Re-import after reload\n",
    "\n",
    "def extract_sprint_name(sprint_str):\n",
    "    \"\"\"\n",
    "    Extract the sprint name from a sprint string.\n",
    "    For example, from a value like:\n",
    "      \"com.atlassian.greenhopper.service.sprint.Sprint@16353814[id=5599,...,name=Sprint 9,...]\"\n",
    "    this returns \"Sprint 9\".\n",
    "    \"\"\"\n",
    "    if not sprint_str or not isinstance(sprint_str, str):\n",
    "        return None\n",
    "    match = re.search(r'name=([^,]+)', sprint_str)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_sprint_from_fixversions(fix_versions):\n",
    "    \"\"\"\n",
    "    Given a list of fixVersions dictionaries, return the first version name containing 'Sprint'.\n",
    "    \"\"\"\n",
    "    if isinstance(fix_versions, list):\n",
    "        for version in fix_versions:\n",
    "            name = version.get(\"name\", \"\")\n",
    "            if \"Sprint\" in name:\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "def add_sprint_field(df):\n",
    "    \"\"\"\n",
    "    Add a standardized 'sprint' column to the DataFrame.\n",
    "    First, try using customfield_10557; if not present, fall back to fixVersions.\n",
    "    \"\"\"\n",
    "    if \"fields.customfield_10557\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.customfield_10557\"].apply(extract_sprint_name)\n",
    "    elif \"fields.fixVersions\" in df.columns:\n",
    "        df[\"sprint\"] = df[\"fields.fixVersions\"].apply(extract_sprint_from_fixversions)\n",
    "    else:\n",
    "        df[\"sprint\"] = None\n",
    "    return df\n",
    "\n",
    "def extract_planning_fields(df):\n",
    "    \"\"\"\n",
    "    From the full cleaned DataFrame, extract only the planning-phase fields.\n",
    "    Enhanced to include all critical planning-phase data for task-level and project-level estimation.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = add_sprint_field(df)\n",
    "    \n",
    "    planning_cols = [\n",
    "        # Basic issue identification\n",
    "        \"key\",                    # Issue key\n",
    "        \"fields.summary\",         # Summary text\n",
    "        \n",
    "        # Issue classification\n",
    "        \"fields.issuetype.name\",  # Type\n",
    "        \"fields.status.name\",     # Status\n",
    "        \"fields.priority.name\",   # Priority\n",
    "        \n",
    "        # People involved\n",
    "        \"fields.assignee.key\",    # Assignee ID\n",
    "        \"fields.creator.key\",     # Creator ID\n",
    "        \"fields.reporter.key\",    # Reporter ID\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\",      # Project ID\n",
    "        \"fields.project.name\",    # Project name\n",
    "        \n",
    "        # Time information\n",
    "        \"fields.created\",         # Creation date\n",
    "        \n",
    "        # Relationships\n",
    "        \"sprint\",                 # Extracted sprint info\n",
    "        \"issuelinks_total\",       # Total link count\n",
    "        \"has_issuelinks_relates\", # Has 'relates to' links\n",
    "        \"has_issuelinks_cloners\", # Has 'cloners' links\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\",      # Components\n",
    "        \"fields.labels\",          # Labels\n",
    "        \n",
    "        # For training purposes\n",
    "        \"fields.resolutiondate\"   # Used to calculate resolution time\n",
    "    ]\n",
    "    \n",
    "    # Get all description embedding columns (they start with \"desc_emb_\")\n",
    "    embedding_cols = [col for col in df.columns if col.startswith(\"desc_emb_\")]\n",
    "    \n",
    "    # Combine all desired columns\n",
    "    all_cols = planning_cols + embedding_cols\n",
    "    \n",
    "    # Filter to only include columns that exist in the DataFrame\n",
    "    available_cols = [col for col in all_cols if col in df.columns]\n",
    "    \n",
    "    # Create the base planning DataFrame\n",
    "    planning_df = df[available_cols].copy()\n",
    "    \n",
    "    # Add derived features\n",
    "    \n",
    "    # Component and label counts\n",
    "    if \"fields.components\" in planning_df.columns:\n",
    "        planning_df[\"component_count\"] = planning_df[\"fields.components\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    if \"fields.labels\" in planning_df.columns:\n",
    "        planning_df[\"label_count\"] = planning_df[\"fields.labels\"].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "    \n",
    "    # Creation date features\n",
    "    if \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"created_date\"] = pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        planning_df[\"created_day_of_week\"] = planning_df[\"created_date\"].dt.dayofweek\n",
    "        planning_df[\"created_month\"] = planning_df[\"created_date\"].dt.month\n",
    "        planning_df[\"created_year\"] = planning_df[\"created_date\"].dt.year\n",
    "    \n",
    "    # Calculate resolution time (if available - for training only)\n",
    "    if \"fields.resolutiondate\" in planning_df.columns and \"fields.created\" in planning_df.columns:\n",
    "        planning_df[\"resolution_time\"] = (\n",
    "            pd.to_datetime(planning_df[\"fields.resolutiondate\"]) - \n",
    "            pd.to_datetime(planning_df[\"fields.created\"])\n",
    "        ).dt.total_seconds() / 3600  # Convert to hours\n",
    "        \n",
    "        # Drop the raw resolution date as we now have the derived feature\n",
    "        planning_df = planning_df.drop(columns=[\"fields.resolutiondate\"])\n",
    "    \n",
    "    return planning_df\n",
    "\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    Enhanced with project duration metrics.\n",
    "    \"\"\"\n",
    "    # Group by project ID and name\n",
    "    agg_dict = {\n",
    "        # Issue counts\n",
    "        'key': 'count',  # Total number of issues\n",
    "    }\n",
    "    \n",
    "    # Add issue type distribution if available\n",
    "    if 'fields.issuetype.name' in df.columns:\n",
    "        agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add priority distribution if available  \n",
    "    if 'fields.priority.name' in df.columns:\n",
    "        agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add status distribution if available\n",
    "    if 'fields.status.name' in df.columns:\n",
    "        agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Team size metrics if available\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    # Connectivity metrics if available\n",
    "    if 'issuelinks_total' in df.columns:\n",
    "        agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'has_issuelinks_relates' in df.columns:\n",
    "        agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "    \n",
    "    # Component usage if available\n",
    "    if 'fields.components' in df.columns:\n",
    "        agg_dict['fields.components'] = lambda x: set(item for sublist in x if isinstance(sublist, list) for item in sublist)\n",
    "    \n",
    "    # Add component and label counts if available\n",
    "    if 'component_count' in df.columns:\n",
    "        agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'label_count' in df.columns:\n",
    "        agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "    # Add sprint information if available\n",
    "    if 'sprint' in df.columns:\n",
    "        agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add creation time features if available\n",
    "    if 'created_date' in df.columns:\n",
    "        # Min and max dates to calculate project duration\n",
    "        agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "    if 'created_month' in df.columns:\n",
    "        agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    if 'created_day_of_week' in df.columns:\n",
    "        agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add resolution time statistics if available (for model training)\n",
    "    if 'resolution_time' in df.columns:\n",
    "        agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "    # Add description embedding averages if available\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "    for col in embedding_cols:\n",
    "        agg_dict[col] = 'mean'\n",
    "    \n",
    "    # Perform the groupby aggregation\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Reset index to convert groupby result to regular DataFrame\n",
    "    project_features = project_features.reset_index()\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Add derived metrics\n",
    "    \n",
    "    # Compute team-related metrics if available\n",
    "    if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['issue_to_assignee_ratio'] = (\n",
    "            project_features['key_count'] / project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    \n",
    "    # Calculate link density if available\n",
    "    if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "        )\n",
    "    \n",
    "    # Calculate temporal metrics (if date information is available)\n",
    "    if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "        # Project duration in days\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['created_date_max']) - \n",
    "             pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        project_features['issue_creation_rate'] = (\n",
    "            project_features['key_count'] / \n",
    "            project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "        )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "def export_clean_planningphase_df(open_dtale=True):\n",
    "    \"\"\"\n",
    "    Run the full cleaning pipeline to obtain a cleaned task-level DataFrame,\n",
    "    then extract only the planning-phase fields to create a planning-phase dataset.\n",
    "    If open_dtale is True, launch a D-Tale session for interactive exploration.\n",
    "    \n",
    "    Returns:\n",
    "      planning_df (pd.DataFrame): The planning-phase DataFrame with the selected fields.\n",
    "    \"\"\"\n",
    "    full_df = export_clean_df()  # This returns your fully cleaned task-level DataFrame.\n",
    "    planning_df = extract_planning_fields(full_df)\n",
    "    project_features_df = create_project_features(planning_df)\n",
    "\n",
    "    \n",
    "    if open_dtale:\n",
    "        print(\"Launching D-Tale session for planning-phase DataFrame...\")\n",
    "        d_pf = dtale.show(project_features_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "\n",
    "        d_pf.open_browser()\n",
    "        \n",
    "    return project_features_df\n",
    "\n",
    "def expand_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Expand categorical distributions stored as dictionaries.\n",
    "    \"\"\"\n",
    "    # Identify columns that contain dictionaries\n",
    "    dict_cols = [col for col in df.columns \n",
    "                if isinstance(df[col].iloc[0], dict) if len(df) > 0]\n",
    "    \n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for col in dict_cols:\n",
    "        # Find all unique categories across all projects\n",
    "        all_categories = set()\n",
    "        for d in df[col]:\n",
    "            if isinstance(d, dict):\n",
    "                all_categories.update(d.keys())\n",
    "        \n",
    "        # Create a column for each category\n",
    "        for category in all_categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            result_df[new_col_name] = df[col].apply(\n",
    "                lambda x: x.get(category, 0) if isinstance(x, dict) else 0\n",
    "            )\n",
    "        \n",
    "        # Drop the original dictionary column\n",
    "        result_df = result_df.drop(columns=[col])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# def create_project_features(df):\n",
    "#     \"\"\"\n",
    "#     Aggregate task-level features to create project-level features.\n",
    "#     Enhanced with project duration metrics and fixed unhashable dict issue.\n",
    "#     \"\"\"\n",
    "#     # Group by project ID and name\n",
    "#     agg_dict = {\n",
    "#         # Issue counts\n",
    "#         'key': 'count',  # Total number of issues\n",
    "#     }\n",
    "    \n",
    "#     # Add issue type distribution if available\n",
    "#     if 'fields.issuetype.name' in df.columns:\n",
    "#         agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add priority distribution if available  \n",
    "#     if 'fields.priority.name' in df.columns:\n",
    "#         agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add status distribution if available\n",
    "#     if 'fields.status.name' in df.columns:\n",
    "#         agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Team size metrics if available\n",
    "#     if 'fields.assignee.key' in df.columns:\n",
    "#         agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "#     if 'fields.creator.key' in df.columns:\n",
    "#         agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "    \n",
    "#     # Connectivity metrics if available\n",
    "#     if 'issuelinks_total' in df.columns:\n",
    "#         agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "#     if 'has_issuelinks_relates' in df.columns:\n",
    "#         agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "    \n",
    "#     # Component usage if available - FIX for unhashable dict error\n",
    "#     if 'fields.components' in df.columns:\n",
    "#         # Extract component names safely, handling different data structures\n",
    "#         agg_dict['fields.components'] = lambda x: set([\n",
    "#             str(comp_name) for row in x \n",
    "#             for comp_name in (row if isinstance(row, list) else []) \n",
    "#         ])\n",
    "    \n",
    "#     # Add component and label counts if available\n",
    "#     if 'component_count' in df.columns:\n",
    "#         agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "#     if 'label_count' in df.columns:\n",
    "#         agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "#     # Add sprint information if available\n",
    "#     if 'sprint' in df.columns:\n",
    "#         agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add creation time features if available\n",
    "#     if 'created_date' in df.columns:\n",
    "#         # Min and max dates to calculate project duration\n",
    "#         agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "#     if 'created_month' in df.columns:\n",
    "#         agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     if 'created_day_of_week' in df.columns:\n",
    "#         agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "#     # Add resolution time statistics if available (for model training)\n",
    "#     if 'resolution_time' in df.columns:\n",
    "#         agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "#     # Add description embedding averages if available\n",
    "#     embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "#     for col in embedding_cols:\n",
    "#         agg_dict[col] = 'mean'\n",
    "    \n",
    "#     # Perform the groupby aggregation\n",
    "#     project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "#     # Flatten the multi-level columns\n",
    "#     project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "#                                for col in project_features.columns]\n",
    "    \n",
    "#     # Reset index to convert groupby result to regular DataFrame\n",
    "#     project_features = project_features.reset_index()\n",
    "    \n",
    "#     # Expand the categorical distributions\n",
    "#     project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "#     # Add derived metrics\n",
    "    \n",
    "#     # Compute team-related metrics if available\n",
    "#     if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "#         project_features['issue_to_assignee_ratio'] = (\n",
    "#             project_features['key_count'] / project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "#         )\n",
    "    \n",
    "#     # Calculate link density if available\n",
    "#     if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "#         project_features['link_density'] = (\n",
    "#             project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "#         )\n",
    "    \n",
    "#     # Calculate temporal metrics (if date information is available)\n",
    "#     if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "#         # Project duration in days\n",
    "#         project_features['project_duration_days'] = (\n",
    "#             (pd.to_datetime(project_features['created_date_max']) - \n",
    "#              pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "#         )\n",
    "        \n",
    "#         # Issue creation rate (issues per day) - a proxy for velocity\n",
    "#         project_features['issue_creation_rate'] = (\n",
    "#             project_features['key_count'] / \n",
    "#             project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "#         )\n",
    "    \n",
    "#     return project_features\n",
    "def create_project_features(df):\n",
    "    \"\"\"\n",
    "    Aggregate task-level features to create project-level features.\n",
    "    Returns a DataFrame with clean, readable column names.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame with task-level features\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with project-level features and clean field names\n",
    "    \"\"\"\n",
    "    # Define the aggregation dictionary\n",
    "    agg_dict = {}\n",
    "    \n",
    "    # Basic counts\n",
    "    agg_dict['key'] = 'count'  # This will create 'key_count' in the result\n",
    "    \n",
    "    # Add issue type distribution if available\n",
    "    if 'fields.issuetype.name' in df.columns:\n",
    "        agg_dict['fields.issuetype.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add priority distribution if available  \n",
    "    if 'fields.priority.name' in df.columns:\n",
    "        agg_dict['fields.priority.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add status distribution if available\n",
    "    if 'fields.status.name' in df.columns:\n",
    "        agg_dict['fields.status.name'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Team size metrics if available\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        agg_dict['fields.assignee.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        agg_dict['fields.creator.key'] = lambda x: x.nunique()\n",
    "        \n",
    "    if 'fields.reporter.key' in df.columns:\n",
    "        agg_dict['fields.reporter.key'] = lambda x: x.nunique()\n",
    "    \n",
    "    # Connectivity metrics if available\n",
    "    if 'issuelinks_total' in df.columns:\n",
    "        agg_dict['issuelinks_total'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'has_issuelinks_relates' in df.columns:\n",
    "        agg_dict['has_issuelinks_relates'] = 'mean'\n",
    "        \n",
    "    if 'has_issuelinks_cloners' in df.columns:\n",
    "        agg_dict['has_issuelinks_cloners'] = 'mean'\n",
    "    \n",
    "    # Component usage if available - Fix for unhashable dict error\n",
    "    if 'fields.components' in df.columns:\n",
    "        # Extract component names safely, handling different data structures\n",
    "        agg_dict['fields.components'] = lambda x: set([\n",
    "            str(comp_name) for row in x \n",
    "            for comp_name in (row if isinstance(row, list) else []) \n",
    "        ])\n",
    "    \n",
    "    # Add component and label counts\n",
    "    if 'component_count' in df.columns:\n",
    "        agg_dict['component_count'] = ['sum', 'mean']\n",
    "    \n",
    "    if 'label_count' in df.columns:\n",
    "        agg_dict['label_count'] = ['sum', 'mean']\n",
    "    \n",
    "    # Add sprint information if available\n",
    "    if 'sprint' in df.columns:\n",
    "        agg_dict['sprint'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add creation time features \n",
    "    if 'created_date' in df.columns:\n",
    "        agg_dict['created_date'] = ['min', 'max']\n",
    "    \n",
    "    if 'created_month' in df.columns:\n",
    "        agg_dict['created_month'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    if 'created_day_of_week' in df.columns:\n",
    "        agg_dict['created_day_of_week'] = lambda x: x.value_counts().to_dict()\n",
    "    \n",
    "    # Add resolution time statistics\n",
    "    if 'resolution_time' in df.columns:\n",
    "        agg_dict['resolution_time'] = ['mean', 'median', 'min', 'max', 'std', 'sum']\n",
    "    \n",
    "    # Add description embedding averages\n",
    "    embedding_cols = [col for col in df.columns if col.startswith('desc_emb_')]\n",
    "    for col in embedding_cols:\n",
    "        agg_dict[col] = 'mean'\n",
    "    \n",
    "    # Perform the groupby aggregation\n",
    "    project_features = df.groupby(['fields.project.id', 'fields.project.name']).agg(agg_dict)\n",
    "    \n",
    "    # Flatten the multi-level columns\n",
    "    project_features.columns = ['_'.join(col) if isinstance(col, tuple) else col \n",
    "                               for col in project_features.columns]\n",
    "    \n",
    "    # Reset index to convert groupby result to regular DataFrame\n",
    "    project_features = project_features.reset_index()\n",
    "    \n",
    "    # Expand the categorical distributions\n",
    "    project_features = expand_categorical_features(project_features)\n",
    "    \n",
    "    # Create a dictionary of old to new column names\n",
    "    rename_mapping = {\n",
    "        'fields.project.id': 'project_id',\n",
    "        'fields.project.name': 'project_name',\n",
    "        'key_count': 'issue_count',\n",
    "        'fields.assignee.key': 'assignee_count',\n",
    "        'fields.creator.key': 'creator_count',\n",
    "        'fields.reporter.key': 'reporter_count',\n",
    "        'fields.issuetype.name': 'issue_types',\n",
    "        'fields.priority.name': 'priorities',\n",
    "        'fields.status.name': 'statuses',\n",
    "        'issuelinks_total_sum': 'total_links',\n",
    "        'issuelinks_total_mean': 'avg_links_per_issue',\n",
    "        'has_issuelinks_relates_mean': 'pct_related_issues',\n",
    "        'has_issuelinks_cloners_mean': 'pct_cloned_issues',\n",
    "        'fields.components': 'component_names',\n",
    "        'component_count_sum': 'total_components',\n",
    "        'component_count_mean': 'avg_components_per_issue',\n",
    "        'label_count_sum': 'total_labels',\n",
    "        'label_count_mean': 'avg_labels_per_issue',\n",
    "        'sprint': 'sprints',\n",
    "        'created_date_min': 'project_start_date',\n",
    "        'created_date_max': 'project_latest_date',\n",
    "        'created_month': 'creation_months',\n",
    "        'created_day_of_week': 'creation_weekdays',\n",
    "        'resolution_time_mean': 'avg_resolution_hours',\n",
    "        'resolution_time_median': 'median_resolution_hours',\n",
    "        'resolution_time_min': 'min_resolution_hours',\n",
    "        'resolution_time_max': 'max_resolution_hours',\n",
    "        'resolution_time_std': 'resolution_hours_std',\n",
    "        'resolution_time_sum': 'total_resolution_hours'\n",
    "    }\n",
    "    \n",
    "    # Only rename columns that exist in the DataFrame\n",
    "    columns_to_rename = {k: v for k, v in rename_mapping.items() if k in project_features.columns}\n",
    "    project_features = project_features.rename(columns=columns_to_rename)\n",
    "    \n",
    "    # Add derived metrics with clean names\n",
    "    \n",
    "    # Project duration in days - using the original column names first, then renaming\n",
    "    if 'created_date_min' in project_features.columns and 'created_date_max' in project_features.columns:\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['created_date_max']) - \n",
    "             pd.to_datetime(project_features['created_date_min'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        if 'key_count' in project_features.columns:  # Use original column name\n",
    "            project_features['issues_per_day'] = (\n",
    "                project_features['key_count'] / \n",
    "                project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "            )\n",
    "    elif 'project_start_date' in project_features.columns and 'project_latest_date' in project_features.columns:\n",
    "        project_features['project_duration_days'] = (\n",
    "            (pd.to_datetime(project_features['project_latest_date']) - \n",
    "             pd.to_datetime(project_features['project_start_date'])).dt.total_seconds() / (24 * 3600)\n",
    "        )\n",
    "        \n",
    "        # Issue creation rate (issues per day) - a proxy for velocity\n",
    "        if 'issue_count' in project_features.columns:  # Use renamed column\n",
    "            project_features['issues_per_day'] = (\n",
    "                project_features['issue_count'] / \n",
    "                project_features['project_duration_days'].replace(0, 1)  # Avoid division by zero\n",
    "            )\n",
    "    \n",
    "    # Compute team-related metrics - check both original and renamed columns\n",
    "    if 'fields.assignee.key' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['issues_per_assignee'] = (\n",
    "            project_features['key_count'] / \n",
    "            project_features['fields.assignee.key'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    elif 'assignee_count' in project_features.columns and 'issue_count' in project_features.columns:\n",
    "        project_features['issues_per_assignee'] = (\n",
    "            project_features['issue_count'] / \n",
    "            project_features['assignee_count'].apply(lambda x: max(1, x))\n",
    "        )\n",
    "    \n",
    "    # Calculate network metrics - check both original and renamed columns\n",
    "    if 'issuelinks_total_sum' in project_features.columns and 'key_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['issuelinks_total_sum'] / project_features['key_count']\n",
    "        )\n",
    "    elif 'total_links' in project_features.columns and 'issue_count' in project_features.columns:\n",
    "        project_features['link_density'] = (\n",
    "            project_features['total_links'] / project_features['issue_count']\n",
    "        )\n",
    "    \n",
    "    return project_features\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df_planning = export_clean_planningphase_df(open_dtale=True)\n",
    "    print(\"Planning-phase DataFrame columns:\")\n",
    "    print(df_planning.columns.tolist())\n",
    "    print(\"Sample rows:\")\n",
    "    print(df_planning.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
