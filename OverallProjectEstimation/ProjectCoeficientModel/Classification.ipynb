{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== PROJECT CLASSIFICATION PIPELINE ==========\n",
      "\n",
      "Loaded ../DataSets/data_export_1741699774916.csv: 159 rows, 100 columns\n",
      "Starting data cleaning...\n",
      "Replacing 1 negative values in min_resolution_hours with 0\n",
      "Dropped 6 columns with constant values\n",
      "Cleaning complete. Resulting dataset: 159 rows, 94 columns\n",
      "Added team metrics: ['creator_count', 'reporter_count', 'team_size_estimate', 'issues_per_team_member', 'resolution_hours_per_team_member', 'team_role_diversity']\n",
      "\n",
      "Analyzing feature importance for predicting total_resolution_hours...\n",
      "Identified 8 features that account for 90% of importance\n",
      "Top 5 most important features: issue_count, issuetype.name_Suggestion, issuetype.name_Bug, issuetype.name_Support Request, issuetype.name_Public Security Vulnerability\n",
      "Analyzed correlations between 8 key features\n",
      "\n",
      "Using Elbow Method to determine optimal number of clusters...\n",
      "Elbow method suggests optimal number of clusters: 3\n",
      "\n",
      "Using Silhouette Analysis to determine optimal number of clusters...\n",
      "Silhouette analysis suggests optimal number of clusters: 3\n",
      "\n",
      "Using Davies-Bouldin Index to determine optimal number of clusters...\n",
      "Davies-Bouldin index suggests optimal number of clusters: 3\n",
      "\n",
      "Cluster number recommendations:\n",
      "  Elbow Method: k = 3\n",
      "  Silhouette Analysis: k = 3\n",
      "  Davies-Bouldin Index: k = 3\n",
      "\n",
      "Recommended optimal number of clusters: 3\n",
      "\n",
      "Tuning KMeans hyperparameters for k=3...\n",
      "Testing 8 hyperparameter combinations...\n",
      "Best hyperparameters: {'init': 'k-means++', 'n_init': 10, 'max_iter': 300}\n",
      "Silhouette score with best parameters: 0.9048\n",
      "\n",
      "Classifying projects into 3 clusters...\n",
      "\n",
      "Medium High-effort Projects (Cluster 0) (156 projects, 98.1%):\n",
      "  Sample projects: Atlassian Plugins, Subversion JIRA plugin, Fabric CA\n",
      "  Distinguishing characteristics:\n",
      "    - issuetype.name_Public Security Vulnerability: 72.2% lower than average\n",
      "    - creator_count: 65.6% lower than average\n",
      "    - reporter_count: 63.3% lower than average\n",
      "    - issuetype.name_Support Request: 59.2% lower than average\n",
      "    - issuetype.name_Bug: 52.2% lower than average\n",
      "\n",
      "Large High-effort Projects (Cluster 1) (2 projects, 1.3%):\n",
      "  Sample projects: Minecraft: Java Edition, Minecraft (Bedrock codebase)\n",
      "  Distinguishing characteristics:\n",
      "    - creator_count: 4917.0% higher than average\n",
      "    - reporter_count: 4745.5% higher than average\n",
      "    - issuetype.name_Bug: 3911.2% higher than average\n",
      "    - issue_count: 2564.0% higher than average\n",
      "    - issuetype.name_Suggestion: 100.0% lower than average\n",
      "\n",
      "Large High-effort Projects (Cluster 2) (1 projects, 0.6%):\n",
      "  Sample projects: Jira Server and Data Center\n",
      "  Distinguishing characteristics:\n",
      "    - issuetype.name_Public Security Vulnerability: 11463.6% higher than average\n",
      "    - issuetype.name_Support Request: 9432.2% higher than average\n",
      "    - issuetype.name_Suggestion: 3044.5% higher than average\n",
      "    - issue_count: 495.8% higher than average\n",
      "    - creator_count: 396.0% higher than average\n",
      "Project classification complete. Results saved to project_classification_results/results\n",
      "\n",
      "============= PIPELINE COMPLETE =============\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load project data from CSV file.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {len(df)} projects with {len(df.columns)} columns\")\n",
    "    return df\n",
    "\n",
    "def create_project_scale_features(df):\n",
    "    \"\"\"Create features related to project scale.\"\"\"\n",
    "    # Convert date strings to datetime\n",
    "    if 'project_start_date' in df.columns and 'project_latest_date' in df.columns:\n",
    "        df['project_start_date'] = pd.to_datetime(df['project_start_date'])\n",
    "        df['project_latest_date'] = pd.to_datetime(df['project_latest_date'])\n",
    "        \n",
    "        # Project duration\n",
    "        df['project_duration_days'] = (df['project_latest_date'] - df['project_start_date']).dt.days\n",
    "    \n",
    "    # Project size categorization\n",
    "    if 'issue_count' in df.columns:\n",
    "        size_conditions = [\n",
    "            (df['issue_count'] < 500),\n",
    "            (df['issue_count'] >= 500) & (df['issue_count'] < 5000),\n",
    "            (df['issue_count'] >= 5000) & (df['issue_count'] < 20000),\n",
    "            (df['issue_count'] >= 20000)\n",
    "        ]\n",
    "        size_values = ['Small', 'Medium', 'Large', 'Very Large']\n",
    "        df['project_size'] = np.select(size_conditions, size_values)\n",
    "    \n",
    "    # Team size ratio\n",
    "    contributor_cols = ['fields.creator.key_<lambda>', 'fields.reporter.key_<lambda>']\n",
    "    if all(col in df.columns for col in contributor_cols):\n",
    "        df['unique_contributors'] = df[contributor_cols].sum(axis=1)\n",
    "        df['contributor_to_issue_ratio'] = df['unique_contributors'] / df['issue_count']\n",
    "    \n",
    "    # Activity density\n",
    "    if 'project_duration_days' in df.columns and 'issue_count' in df.columns:\n",
    "        df['avg_issues_per_day'] = df['issue_count'] / df['project_duration_days'].clip(lower=1)\n",
    "    \n",
    "    # Project age category\n",
    "    if 'project_duration_days' in df.columns:\n",
    "        age_conditions = [\n",
    "            (df['project_duration_days'] < 365),\n",
    "            (df['project_duration_days'] >= 365) & (df['project_duration_days'] < 365*3),\n",
    "            (df['project_duration_days'] >= 365*3) & (df['project_duration_days'] < 365*5),\n",
    "            (df['project_duration_days'] >= 365*5)\n",
    "        ]\n",
    "        age_values = ['New', 'Established', 'Mature', 'Legacy']\n",
    "        df['project_age_category'] = np.select(age_conditions, age_values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_issue_type_features(df):\n",
    "    \"\"\"Create features related to issue type distribution.\"\"\"\n",
    "    # Get all issue type columns\n",
    "    issue_type_columns = [col for col in df.columns if 'fields.issuetype.name_<lambda>_' in col]\n",
    "    \n",
    "    if not issue_type_columns:\n",
    "        return df\n",
    "    \n",
    "    # Calculate total issues by type for each project\n",
    "    df['total_issues_by_type'] = df[issue_type_columns].sum(axis=1)\n",
    "    \n",
    "    # Calculate ratios for major issue types\n",
    "    for issue_type in ['Bug', 'Task', 'Improvement', 'Enhancement', 'Story', 'Sub-task', 'Feature Request', 'New Feature']:\n",
    "        col_name = f'fields.issuetype.name_<lambda>_{issue_type}'\n",
    "        if col_name in df.columns:\n",
    "            df[f'{issue_type.lower().replace(\" \", \"_\")}_ratio'] = df[col_name] / df['total_issues_by_type']\n",
    "    \n",
    "    # Feature ratio (combine all feature-related types)\n",
    "    feature_cols = [\n",
    "        'fields.issuetype.name_<lambda>_New Feature',\n",
    "        'fields.issuetype.name_<lambda>_Feature',\n",
    "        'fields.issuetype.name_<lambda>_Feature Request'\n",
    "    ]\n",
    "    \n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    if feature_cols:\n",
    "        df['feature_ratio'] = df[feature_cols].sum(axis=1) / df['total_issues_by_type']\n",
    "    \n",
    "    # Create bug to feature ratio (maintenance vs innovation)\n",
    "    if 'bug_ratio' in df.columns and 'feature_ratio' in df.columns:\n",
    "        df['bug_to_feature_ratio'] = df['bug_ratio'] / df['feature_ratio'].replace(0, 0.0001)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_resolution_features(df):\n",
    "    \"\"\"Create features related to issue resolution efficiency.\"\"\"\n",
    "    # Resolution efficiency (inverse of median time)\n",
    "    if 'median_resolution_hours' in df.columns:\n",
    "        df['resolution_efficiency'] = 1 / df['median_resolution_hours'].replace(0, float('inf'))\n",
    "        df['resolution_efficiency'] = df['resolution_efficiency'].replace([float('inf')], 0)\n",
    "    \n",
    "    # Resolution variability (coefficient of variation)\n",
    "    if 'resolution_hours_std' in df.columns and 'avg_resolution_hours' in df.columns:\n",
    "        df['resolution_variability'] = df['resolution_hours_std'] / df['avg_resolution_hours'].replace(0, 1)\n",
    "    \n",
    "    # Bug resolution time (weighted by bug ratio)\n",
    "    if 'bug_ratio' in df.columns and 'avg_resolution_hours' in df.columns:\n",
    "        df['bug_resolution_hours'] = df['avg_resolution_hours'] * df['bug_ratio']\n",
    "    \n",
    "    # Fast vs slow resolution categorization\n",
    "    if 'resolution_efficiency' in df.columns:\n",
    "        resolution_terciles = df['resolution_efficiency'].quantile([0.33, 0.66]).values\n",
    "        \n",
    "        resolution_conditions = [\n",
    "            (df['resolution_efficiency'] <= resolution_terciles[0]),\n",
    "            (df['resolution_efficiency'] > resolution_terciles[0]) & (df['resolution_efficiency'] <= resolution_terciles[1]),\n",
    "            (df['resolution_efficiency'] > resolution_terciles[1])\n",
    "        ]\n",
    "        resolution_categories = ['Slow', 'Medium', 'Fast']\n",
    "        df['resolution_speed_category'] = np.select(resolution_conditions, resolution_categories)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_complexity_features(df):\n",
    "    \"\"\"Create features related to project complexity.\"\"\"\n",
    "    # Use existing complexity metrics from the original dataset\n",
    "    complexity_features = [\n",
    "        'avg_components_per_issue',\n",
    "        'avg_labels_per_issue',\n",
    "        'avg_links_per_issue'\n",
    "    ]\n",
    "    \n",
    "    complexity_features = [f for f in complexity_features if f in df.columns]\n",
    "    \n",
    "    # Calculate complexity score (sum of normalized complexity features)\n",
    "    if complexity_features:\n",
    "        df_temp = df[complexity_features].copy()\n",
    "        \n",
    "        # Standardize each feature\n",
    "        scaler = StandardScaler()\n",
    "        df_temp_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(df_temp),\n",
    "            columns=complexity_features\n",
    "        )\n",
    "        \n",
    "        # Combine into a single score\n",
    "        df['complexity_score'] = df_temp_scaled.mean(axis=1)\n",
    "        \n",
    "        # Create complexity categories\n",
    "        complexity_terciles = df['complexity_score'].quantile([0.33, 0.66]).values\n",
    "        \n",
    "        complexity_conditions = [\n",
    "            (df['complexity_score'] <= complexity_terciles[0]),\n",
    "            (df['complexity_score'] > complexity_terciles[0]) & (df['complexity_score'] <= complexity_terciles[1]),\n",
    "            (df['complexity_score'] > complexity_terciles[1])\n",
    "        ]\n",
    "        complexity_categories = ['Low', 'Medium', 'High']\n",
    "        df['complexity_category'] = np.select(complexity_conditions, complexity_categories)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_team_dynamics_features(df):\n",
    "    \"\"\"Create features related to team dynamics.\"\"\"\n",
    "    # Contributor efficiency\n",
    "    if 'unique_contributors' in df.columns and 'issue_count' in df.columns:\n",
    "        df['contributor_efficiency'] = df['issue_count'] / df['unique_contributors'].replace(0, 1)\n",
    "    \n",
    "    # Contributor density\n",
    "    if 'unique_contributors' in df.columns and 'project_duration_days' in df.columns:\n",
    "        df['contributor_density'] = df['unique_contributors'] / df['project_duration_days'].replace(0, 1)\n",
    "    \n",
    "    # Team size categories\n",
    "    if 'unique_contributors' in df.columns:\n",
    "        team_terciles = df['unique_contributors'].quantile([0.33, 0.66]).values\n",
    "        \n",
    "        team_conditions = [\n",
    "            (df['unique_contributors'] <= team_terciles[0]),\n",
    "            (df['unique_contributors'] > team_terciles[0]) & (df['unique_contributors'] <= team_terciles[1]),\n",
    "            (df['unique_contributors'] > team_terciles[1])\n",
    "        ]\n",
    "        team_categories = ['Small Team', 'Medium Team', 'Large Team']\n",
    "        df['team_size_category'] = np.select(team_conditions, team_categories)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cluster_projects(df, n_clusters=5):\n",
    "    \"\"\"Cluster projects based on numeric features.\"\"\"\n",
    "    # Select numeric columns for clustering\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Exclude ID columns, counts, and target variables\n",
    "    exclude_patterns = ['project_id', 'id', '_count', 'total_']\n",
    "    cluster_features = [col for col in numeric_cols if not any(pattern in col for pattern in exclude_patterns)]\n",
    "    \n",
    "    # Create a copy with only clustering features\n",
    "    cluster_data = df[cluster_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    cluster_data = cluster_data.fillna(cluster_data.median())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Apply KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    df['cluster'] = kmeans.fit_predict(scaled_data)\n",
    "    \n",
    "    # Apply PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Add PCA components to dataframe\n",
    "    df['pca_x'] = pca_result[:, 0]\n",
    "    df['pca_y'] = pca_result[:, 1]\n",
    "    \n",
    "    # Visualize clusters\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        cluster_data = df[df['cluster'] == i]\n",
    "        plt.scatter(\n",
    "            cluster_data['pca_x'], \n",
    "            cluster_data['pca_y'],\n",
    "            s=100, \n",
    "            alpha=0.7,\n",
    "            label=f'Cluster {i} ({len(cluster_data)} projects)'\n",
    "        )\n",
    "    \n",
    "    plt.title('Project Clusters', fontsize=16)\n",
    "    plt.xlabel('Principal Component 1', fontsize=14)\n",
    "    plt.ylabel('Principal Component 2', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig('project_clusters.png', dpi=300)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_clusters(df):\n",
    "    \"\"\"Analyze the characteristics of each cluster.\"\"\"\n",
    "    # Select features for analysis\n",
    "    feature_cols = [\n",
    "        'issue_count', 'project_duration_days', 'unique_contributors',\n",
    "        'avg_issues_per_day', 'contributor_to_issue_ratio', \n",
    "        'bug_ratio', 'task_ratio', 'improvement_ratio', 'feature_ratio',\n",
    "        'resolution_efficiency', 'resolution_variability',\n",
    "        'complexity_score', 'contributor_efficiency'\n",
    "    ]\n",
    "    \n",
    "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    # Calculate overall stats for comparison\n",
    "    overall_stats = df[feature_cols].mean()\n",
    "    \n",
    "    # Analyze each cluster\n",
    "    cluster_profiles = {}\n",
    "    \n",
    "    for cluster in df['cluster'].unique():\n",
    "        cluster_data = df[df['cluster'] == cluster]\n",
    "        \n",
    "        # Basic statistics\n",
    "        profile = {\n",
    "            'size': len(cluster_data),\n",
    "            'percentage': round(len(cluster_data) / len(df) * 100, 1),\n",
    "            'sample_projects': cluster_data['project_name'].sample(min(5, len(cluster_data))).tolist()\n",
    "        }\n",
    "        \n",
    "        # Feature statistics and differences from overall\n",
    "        feature_stats = {}\n",
    "        \n",
    "        for feature in feature_cols:\n",
    "            if feature in cluster_data.columns:\n",
    "                feature_mean = cluster_data[feature].mean()\n",
    "                overall_mean = overall_stats[feature]\n",
    "                \n",
    "                # Calculate percentage difference\n",
    "                if overall_mean != 0:\n",
    "                    pct_diff = ((feature_mean - overall_mean) / overall_mean) * 100\n",
    "                else:\n",
    "                    pct_diff = 0\n",
    "                \n",
    "                feature_stats[feature] = {\n",
    "                    'mean': feature_mean,\n",
    "                    'overall_mean': overall_mean,\n",
    "                    'pct_diff': pct_diff\n",
    "                }\n",
    "        \n",
    "        # Find most distinctive features (largest percentage differences)\n",
    "        distinctive_features = sorted(\n",
    "            [(f, feature_stats[f]['pct_diff']) for f in feature_stats],\n",
    "            key=lambda x: abs(x[1]),\n",
    "            reverse=True\n",
    "        )[:5]  # Top 5 most distinctive\n",
    "        \n",
    "        profile['distinctive_features'] = distinctive_features\n",
    "        profile['feature_stats'] = feature_stats\n",
    "        \n",
    "        # Generate a simple description based on distinctive features\n",
    "        description_parts = []\n",
    "        for feature, pct_diff in distinctive_features[:3]:  # Use top 3 for description\n",
    "            direction = \"higher\" if pct_diff > 0 else \"lower\"\n",
    "            description_parts.append(f\"{feature} is {abs(pct_diff):.1f}% {direction} than average\")\n",
    "        \n",
    "        profile['description'] = \"Projects where \" + \", \".join(description_parts)\n",
    "        \n",
    "        # Add categorical distributions\n",
    "        for cat_col in ['project_size', 'project_age_category', 'resolution_speed_category', \n",
    "                        'complexity_category', 'team_size_category']:\n",
    "            if cat_col in df.columns:\n",
    "                profile[f'{cat_col}_distribution'] = cluster_data[cat_col].value_counts().to_dict()\n",
    "        \n",
    "        # Store the profile\n",
    "        cluster_profiles[f\"Cluster {cluster}\"] = profile\n",
    "        \n",
    "        # Print brief profile\n",
    "        print(f\"\\nCluster {cluster} ({len(cluster_data)} projects, {profile['percentage']}%):\")\n",
    "        print(f\"  Sample projects: {', '.join(profile['sample_projects'][:3])}\")\n",
    "        print(\"  Key characteristics:\")\n",
    "        for feature, pct_diff in distinctive_features[:3]:\n",
    "            direction = \"higher\" if pct_diff > 0 else \"lower\"\n",
    "            print(f\"    - {feature}: {abs(pct_diff):.1f}% {direction} than average\")\n",
    "    \n",
    "    return cluster_profiles\n",
    "\n",
    "def generate_features(input_file):\n",
    "    \"\"\"Generate all features for project classification.\"\"\"\n",
    "    # Load data\n",
    "    df = load_data(input_file)\n",
    "    \n",
    "    # Create features\n",
    "    print(\"Creating project scale features...\")\n",
    "    df = create_project_scale_features(df)\n",
    "    \n",
    "    print(\"Creating issue type features...\")\n",
    "    df = create_issue_type_features(df)\n",
    "    \n",
    "    print(\"Creating resolution features...\")\n",
    "    df = create_resolution_features(df)\n",
    "    \n",
    "    print(\"Creating complexity features...\")\n",
    "    df = create_complexity_features(df)\n",
    "    \n",
    "    print(\"Creating team dynamics features...\")\n",
    "    df = create_team_dynamics_features(df)\n",
    "    \n",
    "    # Cluster projects\n",
    "    print(\"Clustering projects...\")\n",
    "    df = cluster_projects(df)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    print(\"Analyzing clusters...\")\n",
    "    cluster_profiles = analyze_clusters(df)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = input_file.replace('.csv', '_features.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Features saved to {output_file}\")\n",
    "    \n",
    "    return df, cluster_profiles\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the file directly\n",
    "    input_file = \"../DataSets/data_export_1741699774916.csv\"\n",
    "    df, clusters = generate_features(input_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
