{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Individual Task Effort Estimation Model\n\nThis notebook focuses on estimating individual task effort using the cleaned Jira dataset. The goal is to build models that can predict how long it will take to complete a task based on its characteristics."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom scipy.stats import spearmanr\nfrom xgboost import XGBRegressor\nimport pickle\nimport time\nimport os\nimport warnings\nfrom scipy import stats\nwarnings.filterwarnings('ignore')\n\n# Create directory for results\nresults_dir = 'individual_task_effort_results'\nos.makedirs(results_dir, exist_ok=True)\n\n# Set plot style\nplt.style.use('ggplot')\nsns.set(style=\"whitegrid\")\n\n# Define a function to format column groups\ndef format_column_group(col):\n    if col in ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', \n              'is_type_new_feature', 'is_type_epic', 'is_type_sub-task']:\n        return 'Issue Type'\n    elif col in ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', \n               'is_priority_minor', 'is_priority_trivial']:\n        return 'Priority'\n    elif col in ['inward_count', 'outward_count']:\n        return 'Relationships'\n    elif col in ['age_days', 'created_is_weekend', 'created_hour', 'created_month', 'created_year']:\n        return 'Temporal'\n    elif col in ['fields.creator.active', 'fields.issuetype.id', 'is_completed', 'is_resolved']:\n        return 'Status'\n    else:\n        return 'Other'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Loading and Preparation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Load the cleaned Jira dataset\nprint(\"Loading cleaned Jira dataset...\")\ntry:\n    df = pd.read_csv('cleaned_jira_dataset.csv')\n    print(f\"Dataset loaded: {df.shape[0]} tasks, {df.shape[1]} features\")\nexcept Exception as e:\n    print(f\"Error loading dataset: {e}\")\n    raise\n\n# Display the first few rows to understand the data structure\ndf.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Check for missing values\nprint(\"\\nChecking for missing values...\")\nmissing_values = df.isnull().sum()\nmissing_percent = (missing_values / len(df)) * 100\n\n# Display columns with missing values\nmissing_df = pd.DataFrame({\n    'Missing Values': missing_values,\n    'Percentage': missing_percent\n})\n\nmissing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\nprint(missing_df)\n\n# Visualize missing values\nplt.figure(figsize=(12, 6))\nsns.barplot(x=missing_df.index[:15], y=missing_df['Percentage'][:15])\nplt.title('Top 15 Columns with Missing Values')\nplt.xlabel('Columns')\nplt.ylabel('Percentage Missing')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig(f'{results_dir}/missing_values.png')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Get target variable distribution\nplt.figure(figsize=(12, 6))\n\n# Resolution hours distribution (before filtering)\nplt.subplot(1, 2, 1)\nsns.histplot(df['resolution_hours'], bins=50, kde=True)\nplt.title('Resolution Hours Distribution')\nplt.xlabel('Resolution Hours')\nplt.ylabel('Frequency')\n\n# Log-transformed resolution hours\nplt.subplot(1, 2, 2)\ndf['log_resolution_hours'] = np.log1p(df['resolution_hours'])\nsns.histplot(df['log_resolution_hours'], bins=50, kde=True)\nplt.title('Log(Resolution Hours) Distribution')\nplt.xlabel('Log(Resolution Hours)')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/target_distribution_raw.png')\nplt.show()\n\nprint(f\"\\nTarget variable statistics (before filtering):\")\nprint(f\"  Mean: {df['resolution_hours'].mean():.2f} hours\")\nprint(f\"  Median: {df['resolution_hours'].median():.2f} hours\")\nprint(f\"  Min: {df['resolution_hours'].min():.2f} hours\")\nprint(f\"  Max: {df['resolution_hours'].max():.2f} hours\")\nprint(f\"  Standard Deviation: {df['resolution_hours'].std():.2f} hours\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Data cleaning\nprint(\"\\nCleaning the dataset...\")\n\n# 1. Filter out tasks with invalid resolution hours\ndf_filtered = df.copy()\ndf_filtered = df_filtered.dropna(subset=['resolution_hours'])\ndf_filtered = df_filtered[df_filtered['resolution_hours'] >= 0]\n\n# 2. Cap extremely long resolution times (e.g., > 6 months)\nresolution_cap = 6 * 30 * 24  # 6 months in hours\nlong_tasks = df_filtered['resolution_hours'] > resolution_cap\nprint(f\"Capped {long_tasks.sum()} tasks with resolution times > {resolution_cap} hours (6 months)\")\ndf_filtered.loc[long_tasks, 'resolution_hours'] = resolution_cap\ndf_filtered.loc[long_tasks, 'log_resolution_hours'] = np.log1p(resolution_cap)\n\n# 3. Remove columns with high percentage of missing values (e.g., > 50%)\nhigh_missing_cols = missing_df[missing_df['Percentage'] > 50].index.tolist()\ndf_filtered = df_filtered.drop(columns=high_missing_cols, errors='ignore')\nprint(f\"Removed {len(high_missing_cols)} columns with more than 50% missing values\")\n\n# 4. Fill remaining missing values\n# For numeric columns, fill with median\nnumeric_cols = df_filtered.select_dtypes(include=['number']).columns\ndf_filtered[numeric_cols] = df_filtered[numeric_cols].fillna(df_filtered[numeric_cols].median())\n\n# For categorical columns, fill with mode\ncategorical_cols = df_filtered.select_dtypes(include=['object']).columns\nfor col in categorical_cols:\n    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].mode()[0])\n\n# 5. Check for remaining missing values\nremaining_missing = df_filtered.isnull().sum().sum()\nprint(f\"Remaining missing values after cleaning: {remaining_missing}\")\n\n# Display cleaned dataset info\nprint(f\"\\nCleaned dataset shape: {df_filtered.shape[0]} tasks, {df_filtered.shape[1]} features\")\n\n# Visualize cleaned target variable\nplt.figure(figsize=(12, 6))\n\n# Resolution hours distribution (after cleaning)\nplt.subplot(1, 2, 1)\nsns.histplot(df_filtered['resolution_hours'], bins=50, kde=True)\nplt.title('Resolution Hours Distribution (Cleaned)')\nplt.xlabel('Resolution Hours')\nplt.ylabel('Frequency')\n\n# Log-transformed resolution hours\nplt.subplot(1, 2, 2)\nsns.histplot(df_filtered['log_resolution_hours'], bins=50, kde=True)\nplt.title('Log(Resolution Hours) Distribution (Cleaned)')\nplt.xlabel('Log(Resolution Hours)')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/target_distribution_cleaned.png')\nplt.show()\n\nprint(f\"\\nTarget variable statistics (after cleaning):\")\nprint(f\"  Mean: {df_filtered['resolution_hours'].mean():.2f} hours\")\nprint(f\"  Median: {df_filtered['resolution_hours'].median():.2f} hours\")\nprint(f\"  Min: {df_filtered['resolution_hours'].min():.2f} hours\")\nprint(f\"  Max: {df_filtered['resolution_hours'].max():.2f} hours\")\nprint(f\"  Standard Deviation: {df_filtered['resolution_hours'].std():.2f} hours\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Exploratory Data Analysis"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Explore relationships between task characteristics and resolution time\nprint(\"\\nExploring relationships with resolution time...\")\n\n# 1. Issue type impact on resolution time\nplt.figure(figsize=(12, 6))\nfor col in ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', 'is_type_new_feature', 'is_type_epic', 'is_type_sub-task']:\n    if col in df_filtered.columns:\n        plt.subplot(2, 4, ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', \n                         'is_type_new_feature', 'is_type_epic', 'is_type_sub-task'].index(col) % 8 + 1)\n        \n        # Create groups based on the issue type\n        type_true = df_filtered[df_filtered[col] == 1]['resolution_hours']\n        type_false = df_filtered[df_filtered[col] == 0]['resolution_hours']\n        \n        # Boxplot for this issue type\n        plt.boxplot([type_true, type_false], labels=['Yes', 'No'])\n        plt.title(f'{col.replace(\"is_type_\", \"\").replace(\"_\", \" \").title()}')\n        plt.ylabel('Resolution Hours')\n        \n        # Print mean resolution time for this type\n        print(f\"{col.replace('is_type_', '').replace('_', ' ').title()} - \"\n              f\"Mean resolution time: Yes={type_true.mean():.2f}, No={type_false.mean():.2f}\")\n\nplt.tight_layout()\nplt.savefig(f'{results_dir}/issue_type_impact.png')\nplt.show()\n\n# 2. Priority impact\nplt.figure(figsize=(12, 6))\nfor col in ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', 'is_priority_minor', 'is_priority_trivial']:\n    if col in df_filtered.columns:\n        plt.subplot(2, 3, ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', \n                         'is_priority_minor', 'is_priority_trivial'].index(col) % 6 + 1)\n        \n        # Create groups based on the priority\n        priority_true = df_filtered[df_filtered[col] == 1]['resolution_hours']\n        priority_false = df_filtered[df_filtered[col] == 0]['resolution_hours']\n        \n        # Boxplot for this priority\n        plt.boxplot([priority_true, priority_false], labels=['Yes', 'No'])\n        plt.title(f'{col.replace(\"is_priority_\", \"\").replace(\"_\", \" \").title()}')\n        plt.ylabel('Resolution Hours')\n        \n        # Print mean resolution time for this priority\n        print(f\"{col.replace('is_priority_', '').replace('_', ' ').title()} - \"\n              f\"Mean resolution time: Yes={priority_true.mean():.2f}, No={priority_false.mean():.2f}\")\n\nplt.tight_layout()\nplt.savefig(f'{results_dir}/priority_impact.png')\nplt.show()\n\n# 3. Correlation between numeric features and resolution time\nnumeric_features = df_filtered.select_dtypes(include=['number']).columns.tolist()\nnumeric_features.remove('resolution_hours')  # Remove target from features\nif 'log_resolution_hours' in numeric_features:\n    numeric_features.remove('log_resolution_hours')\n\n# Calculate correlations with resolution time\ncorrelations = []\nfor col in numeric_features:\n    corr = df_filtered[col].corr(df_filtered['resolution_hours'])\n    correlations.append((col, corr))\n\n# Sort by absolute correlation value\ncorrelations.sort(key=lambda x: abs(x[1]), reverse=True)\n\n# Display top 15 correlated features\ntop_corr = pd.DataFrame(correlations[:15], columns=['Feature', 'Correlation'])\nprint(\"\\nTop 15 features correlated with resolution time:\")\nprint(top_corr)\n\n# Visualize top 10 correlations\nplt.figure(figsize=(12, 6))\ntop10 = pd.DataFrame(correlations[:10], columns=['Feature', 'Correlation'])\nsns.barplot(x='Correlation', y='Feature', data=top10)\nplt.title('Top 10 Features Correlated with Resolution Time')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/top_correlations.png')\nplt.show()\n\n# 4. Heatmap of correlations between top features\ntop_features = [x[0] for x in correlations[:15]] + ['resolution_hours']\ncorr_matrix = df_filtered[top_features].corr()\n\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('Correlation Matrix of Top Features')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/correlation_heatmap.png')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Analyze feature categories and their correlations with resolution time\nprint(\"\\nAnalyzing feature categories...\")\n\n# Group features by category\ncategory_features = {}\nfor col in numeric_features:\n    category = format_column_group(col)\n    if category not in category_features:\n        category_features[category] = []\n    category_features[category].append(col)\n\n# Calculate average correlation by category\ncategory_correlations = []\nfor category, features in category_features.items():\n    category_corr = np.mean([abs(df_filtered[feat].corr(df_filtered['resolution_hours'])) for feat in features])\n    category_correlations.append((category, category_corr, len(features)))\n\n# Sort by correlation strength\ncategory_correlations.sort(key=lambda x: x[1], reverse=True)\n\n# Display category correlations\ncategory_corr_df = pd.DataFrame(category_correlations, columns=['Category', 'Avg. Abs. Correlation', 'Feature Count'])\nprint(\"Average absolute correlation by feature category:\")\nprint(category_corr_df)\n\n# Visualize category correlations\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Avg. Abs. Correlation', y='Category', data=category_corr_df)\nplt.title('Average Correlation Strength by Feature Category')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/category_correlations.png')\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Feature Selection and Data Preparation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Prepare features and target for modeling\nprint(\"\\nPreparing features for modeling...\")\n\n# 1. Select features for model training\n# Remove ID columns and target-related columns\nid_columns = ['id', 'fields.issuetype.id', 'fields.project.id', 'fields.priority.id']\ntarget_columns = ['resolution_hours', 'log_resolution_hours']\n\n# Columns to drop\ncolumns_to_drop = id_columns + target_columns\n\n# Drop columns that aren't useful for modeling\nfor col in df_filtered.columns:\n    # Drop date columns that have been converted to features\n    if col in ['fields.created', 'fields.updated']:\n        columns_to_drop.append(col)\n        \n    # Drop any string/object columns that haven't been encoded\n    elif df_filtered[col].dtype == 'object' and col not in id_columns:\n        columns_to_drop.append(col)\n\n# Remove columns that don't exist\ncolumns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n\n# Create feature matrix\nX = df_filtered.drop(columns=columns_to_drop)\n\n# Set target variable (log-transformed for better model performance)\ny = df_filtered['log_resolution_hours']\n\n# Display selected features\nprint(f\"Selected {X.shape[1]} features for modeling\")\nprint(\"Feature list:\")\nprint(X.columns.tolist())\n\n# 2. Check for multicollinearity\ncorr_matrix = X.corr()\n\n# Find highly correlated pairs (|r| > 0.8)\nhigh_corr_pairs = []\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i):\n        if abs(corr_matrix.iloc[i, j]) > 0.8:\n            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n\nif high_corr_pairs:\n    print(\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n    for col1, col2, corr in high_corr_pairs:\n        print(f\"{col1} <-> {col2}: {corr:.3f}\")\n\n    # Create a set of features to drop (keep the one with higher correlation to target)\n    to_drop = set()\n    for col1, col2, _ in high_corr_pairs:\n        corr1 = abs(df_filtered[col1].corr(df_filtered['resolution_hours']))\n        corr2 = abs(df_filtered[col2].corr(df_filtered['resolution_hours']))\n        if corr1 >= corr2:\n            to_drop.add(col2)\n        else:\n            to_drop.add(col1)\n    \n    print(f\"\\nDropping {len(to_drop)} features due to multicollinearity:\")\n    print(list(to_drop))\n    X = X.drop(columns=list(to_drop))\n\n# 3. Split data into training, validation, and test sets\nprint(\"\\nSplitting data into train, validation, and test sets...\")\n\n# First split: 80% train+validation, 20% test\nX_train_val, X_test, y_train_val, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Second split: 75% train, 25% validation (60%/20% of original data)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_val, y_train_val, test_size=0.25, random_state=42\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\n\n# Save data splits for reproducibility\nsplits = {\n    'X_train': X_train,\n    'y_train': y_train,\n    'X_val': X_val,\n    'y_val': y_val,\n    'X_test': X_test,\n    'y_test': y_test,\n    'feature_names': X.columns.tolist(),\n    'log_transform': True  # We're using log-transformed target\n}\n\nwith open(f'{results_dir}/data_splits.pkl', 'wb') as f:\n    pickle.dump(splits, f)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model Training and Evaluation (Initial Models)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train and evaluate regression models\nprint(\"\\nTraining initial regression models...\")\n\n# Function to evaluate models and return metrics\ndef evaluate_model(model, X_val, y_val, model_name, is_val=True):\n    # Get predictions\n    y_pred = model.predict(X_val)\n    \n    # Convert back from log space\n    y_val_orig = np.expm1(y_val)\n    y_pred_orig = np.expm1(y_pred)\n    \n    # Calculate metrics\n    mae = mean_absolute_error(y_val_orig, y_pred_orig)\n    rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig))\n    r2 = r2_score(y_val_orig, y_pred_orig)\n    \n    # Calculate Spearman rank correlation (handles non-linear relationships)\n    spearman_corr, _ = spearmanr(y_val_orig, y_pred_orig)\n    \n    # Print results\n    dataset = \"Validation\" if is_val else \"Test\"\n    print(f\"\\n{model_name} ({dataset} Set):\")\n    print(f\"  MAE: {mae:.2f} hours\")\n    print(f\"  RMSE: {rmse:.2f} hours\")\n    print(f\"  R²: {r2:.4f}\")\n    print(f\"  Spearman Correlation: {spearman_corr:.4f}\")\n    \n    # Create actual vs. predicted plot\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_val_orig, y_pred_orig, alpha=0.3)\n    plt.plot([0, y_val_orig.max()], [0, y_val_orig.max()], 'r--')\n    plt.title(f'{model_name}: Actual vs. Predicted')\n    plt.xlabel('Actual Resolution Hours')\n    plt.ylabel('Predicted Resolution Hours')\n    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_predictions.png')\n    plt.close()\n    \n    # Plot residuals\n    residuals = y_val_orig - y_pred_orig\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_pred_orig, residuals, alpha=0.3)\n    plt.axhline(y=0, color='r', linestyle='--')\n    plt.title(f'{model_name}: Residuals')\n    plt.xlabel('Predicted Resolution Hours')\n    plt.ylabel('Residuals (Actual - Predicted)')\n    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_residuals.png')\n    plt.close()\n    \n    return {\n        'model': model,\n        'name': model_name,\n        'mae': mae,\n        'rmse': rmse,\n        'r2': r2,\n        'spearman': spearman_corr,\n        'predictions': y_pred,\n        'original_predictions': y_pred_orig,\n        'original_values': y_val_orig\n    }\n\n# Define base models\nbase_models = {\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n}\n\n# Train and evaluate models\nmodel_results = {}\nfor name, model in base_models.items():\n    print(f\"\\nTraining {name}...\")\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    train_time = time.time() - start_time\n    print(f\"Training completed in {train_time:.2f} seconds\")\n    \n    # Evaluate on validation set\n    results = evaluate_model(model, X_val, y_val, name)\n    results['train_time'] = train_time\n    model_results[name] = results\n    \n    # Save model\n    with open(f'{results_dir}/original_{name.replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(model, f)\n\n# Create ensemble predictions (average of all models)\nprint(\"\\nCreating ensemble prediction...\")\ny_pred_ensemble = np.zeros_like(y_val)\nfor name, results in model_results.items():\n    y_pred_ensemble += results['predictions']\ny_pred_ensemble /= len(model_results)\n\n# Evaluate ensemble\nensemble_results = {\n    'name': 'Ensemble',\n    'predictions': y_pred_ensemble,\n    'original_predictions': np.expm1(y_pred_ensemble),\n    'original_values': np.expm1(y_val)\n}\n\n# Calculate ensemble metrics\nensemble_results['mae'] = mean_absolute_error(\n    ensemble_results['original_values'], \n    ensemble_results['original_predictions']\n)\nensemble_results['rmse'] = np.sqrt(mean_squared_error(\n    ensemble_results['original_values'], \n    ensemble_results['original_predictions']\n))\nensemble_results['r2'] = r2_score(\n    ensemble_results['original_values'], \n    ensemble_results['original_predictions']\n)\nensemble_results['spearman'], _ = spearmanr(\n    ensemble_results['original_values'], \n    ensemble_results['original_predictions']\n)\n\n# Print ensemble results\nprint(f\"\\nEnsemble (Validation Set):\")\nprint(f\"  MAE: {ensemble_results['mae']:.2f} hours\")\nprint(f\"  RMSE: {ensemble_results['rmse']:.2f} hours\")\nprint(f\"  R²: {ensemble_results['r2']:.4f}\")\nprint(f\"  Spearman Correlation: {ensemble_results['spearman']:.4f}\")\n\n# Create ensemble plots\nplt.figure(figsize=(8, 6))\nplt.scatter(\n    ensemble_results['original_values'], \n    ensemble_results['original_predictions'], \n    alpha=0.3\n)\nplt.plot([0, ensemble_results['original_values'].max()], \n         [0, ensemble_results['original_values'].max()], 'r--')\nplt.title('Ensemble: Actual vs. Predicted')\nplt.xlabel('Actual Resolution Hours')\nplt.ylabel('Predicted Resolution Hours')\nplt.savefig(f'{results_dir}/ensemble_predictions.png')\nplt.close()\n\n# Add ensemble to results\nmodel_results['Ensemble'] = ensemble_results\n\n# Save original results\nwith open(f'{results_dir}/original_results.pkl', 'wb') as f:\n    pickle.dump(model_results, f)\n\n# Save original hyperparameters\noriginal_hyperparams = {\n    'Random Forest': {**base_models['Random Forest'].get_params()},\n    'Gradient Boosting': {**base_models['Gradient Boosting'].get_params()},\n    'XGBoost': {**base_models['XGBoost'].get_params()}\n}\n\nwith open(f'{results_dir}/original_hyperparameters.pkl', 'wb') as f:\n    pickle.dump(original_hyperparams, f)\n\n# Compare model performance\nperformance_comparison = pd.DataFrame([\n    (name, results['mae'], results['rmse'], results['r2'], results['spearman'])\n    for name, results in model_results.items()\n], columns=['Model', 'MAE', 'RMSE', 'R²', 'Spearman'])\n\nprint(\"\\nModel Performance Comparison:\")\nprint(performance_comparison.sort_values('Spearman', ascending=False))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Feature importance analysis\nprint(\"\\nAnalyzing feature importance...\")\n\n# Create function to visualize feature importance\ndef plot_feature_importance(model, model_name, top_n=20):\n    if not hasattr(model, 'feature_importances_'):\n        print(f\"{model_name} does not provide feature importances\")\n        return None\n    \n    # Get feature importances\n    importances = model.feature_importances_\n    feature_importance = pd.DataFrame({\n        'Feature': X.columns,\n        'Importance': importances\n    })\n    \n    # Sort by importance\n    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n    \n    # Get top N features\n    top_features = feature_importance.head(top_n)\n    \n    # Plot importance\n    plt.figure(figsize=(10, 8))\n    plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])\n    plt.title(f'{model_name}: Top {top_n} Feature Importance')\n    plt.xlabel('Importance')\n    plt.tight_layout()\n    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_feature_importance.png')\n    plt.close()\n    \n    return feature_importance\n\n# Analyze each model's feature importance\nfeature_importance_results = {}\nfor name, results in model_results.items():\n    if name != 'Ensemble' and hasattr(results['model'], 'feature_importances_'):\n        print(f\"\\nAnalyzing feature importance for {name}...\")\n        importance = plot_feature_importance(results['model'], name)\n        \n        if importance is not None:\n            feature_importance_results[name] = importance\n            \n            # Print top 10 features\n            print(f\"Top 10 features for {name}:\")\n            print(importance.head(10))\n\n# Find most common important features across models\nif feature_importance_results:\n    print(\"\\nIdentifying most common important features across models...\")\n    \n    # Get top 10 features from each model\n    top_features_by_model = {}\n    all_top_features = set()\n    \n    for name, importance in feature_importance_results.items():\n        top10 = importance.head(10)['Feature'].tolist()\n        top_features_by_model[name] = top10\n        all_top_features.update(top10)\n    \n    # Count feature occurrences across models\n    feature_counts = {}\n    for feature in all_top_features:\n        count = sum(1 for model_features in top_features_by_model.values() if feature in model_features)\n        feature_counts[feature] = count\n    \n    # Sort by count\n    sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n    \n    print(\"Features appearing in multiple models' top 10:\")\n    for feature, count in sorted_features:\n        if count > 1:\n            print(f\"  {feature}: {count} models\")\n    \n    # Create consolidated feature importance\n    best_model = performance_comparison.sort_values('R²', ascending=False).iloc[0]['Model']\n    best_importance = feature_importance_results[best_model]\n    \n    # Save best feature importance\n    best_importance.to_csv(f'{results_dir}/best_feature_importance.csv', index=False)\n    \n    # Create a plot of the best model's feature importance\n    plt.figure(figsize=(12, 8))\n    top_n = 20\n    top_features = best_importance.head(top_n)\n    plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])\n    plt.title(f'Top {top_n} Most Important Features for Task Effort Estimation')\n    plt.xlabel('Importance')\n    plt.tight_layout()\n    plt.savefig(f'{results_dir}/best_feature_importance.png')\n    plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Model Hyperparameter Tuning"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Hyperparameter tuning for the models\nprint(\"\\nPerforming hyperparameter tuning...\")\n\n# Define hyperparameter search spaces\nparam_spaces = {\n    'Random Forest': {\n        'n_estimators': [100, 200, 300, 500],\n        'max_depth': [None, 10, 20, 30, 40],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'max_features': ['auto', 'sqrt']\n    },\n    'Gradient Boosting': {\n        'n_estimators': [100, 200, 300, 500],\n        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n        'max_depth': [3, 5, 7, 9],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'subsample': [0.8, 0.9, 1.0]\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200, 300, 500],\n        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n        'max_depth': [3, 5, 7, 9],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'gamma': [0, 0.1, 0.2]\n    }\n}\n\n# Define scoring metric for hyperparameter tuning\ndef spearman_scorer(estimator, X, y):\n    y_pred = estimator.predict(X)\n    # Convert from log space\n    y_orig = np.expm1(y)\n    y_pred_orig = np.expm1(y_pred)\n    corr, _ = spearmanr(y_orig, y_pred_orig)\n    return corr\n\n# Function to tune hyperparameters\ndef tune_model(model, params, model_name, X_train, y_train, X_val, y_val, n_iter=20):\n    print(f\"\\nTuning hyperparameters for {model_name}...\")\n    \n    # Initialize RandomizedSearchCV\n    random_search = RandomizedSearchCV(\n        model, param_distributions=params, n_iter=n_iter,\n        scoring=spearman_scorer, cv=5, random_state=42, n_jobs=-1\n    )\n    \n    # Fit the search\n    start_time = time.time()\n    random_search.fit(X_train, y_train)\n    tuning_time = time.time() - start_time\n    print(f\"Tuning completed in {tuning_time:.2f} seconds\")\n    \n    # Get best model and parameters\n    best_model = random_search.best_estimator_\n    best_params = random_search.best_params_\n    best_score = random_search.best_score_\n    \n    print(f\"Best CV score: {best_score:.4f}\")\n    print(f\"Best parameters: {best_params}\")\n    \n    # Evaluate the tuned model\n    tuned_results = evaluate_model(best_model, X_val, y_val, f\"Tuned {model_name}\")\n    tuned_results['best_params'] = best_params\n    tuned_results['cv_score'] = best_score\n    tuned_results['tuning_time'] = tuning_time\n    \n    # Save tuned model\n    with open(f'{results_dir}/tuned_{model_name.replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    \n    return tuned_results, random_search.cv_results_\n\n# Store tuned results\ntuned_model_results = {}\ncv_results_dict = {}\n\n# Tune each model\nfor name, model in base_models.items():\n    tuned_results, cv_results = tune_model(\n        model, param_spaces[name], name,\n        X_train, y_train, X_val, y_val\n    )\n    tuned_model_results[name] = tuned_results\n    cv_results_dict[name] = cv_results\n\n# Save tuning results\nwith open(f'{results_dir}/tuned_results.pkl', 'wb') as f:\n    pickle.dump(tuned_model_results, f)\n\nwith open(f'{results_dir}/tuned_cv_results.pkl', 'wb') as f:\n    pickle.dump(cv_results_dict, f)\n\n# Create tuned ensemble predictions\nprint(\"\\nCreating tuned ensemble prediction...\")\ny_pred_tuned_ensemble = np.zeros_like(y_val)\nfor name, results in tuned_model_results.items():\n    y_pred_tuned_ensemble += results['predictions']\ny_pred_tuned_ensemble /= len(tuned_model_results)\n\n# Evaluate tuned ensemble\ntuned_ensemble_results = {\n    'name': 'Tuned Ensemble',\n    'predictions': y_pred_tuned_ensemble,\n    'original_predictions': np.expm1(y_pred_tuned_ensemble),\n    'original_values': np.expm1(y_val)\n}\n\n# Calculate tuned ensemble metrics\ntuned_ensemble_results['mae'] = mean_absolute_error(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\ntuned_ensemble_results['rmse'] = np.sqrt(mean_squared_error(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n))\ntuned_ensemble_results['r2'] = r2_score(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\ntuned_ensemble_results['spearman'], _ = spearmanr(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\n\n# Print tuned ensemble results\nprint(f\"\\nTuned Ensemble (Validation Set):\")\nprint(f\"  MAE: {tuned_ensemble_results['mae']:.2f} hours\")\nprint(f\"  RMSE: {tuned_ensemble_results['rmse']:.2f} hours\")\nprint(f\"  R²: {tuned_ensemble_results['r2']:.4f}\")\nprint(f\"  Spearman Correlation: {tuned_ensemble_results['spearman']:.4f}\")\n\n# Create tuned ensemble plots\nplt.figure(figsize=(8, 6))\nplt.scatter(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions'], \n    alpha=0.3\n)\nplt.plot([0, tuned_ensemble_results['original_values'].max()], \n         [0, tuned_ensemble_results['original_values'].max()], 'r--')\nplt.title('Tuned Ensemble: Actual vs. Predicted')\nplt.xlabel('Actual Resolution Hours')\nplt.ylabel('Predicted Resolution Hours')\nplt.savefig(f'{results_dir}/tuned_ensemble_predictions.png')\nplt.close()\n\n# Add tuned ensemble to results\ntuned_model_results['Ensemble'] = tuned_ensemble_results\n\n# Compare original vs. tuned model performance\nmodel_comparison = []\nfor name in base_models.keys():\n    original_results = model_results[name]\n    tuned_results = tuned_model_results[name]\n    \n    model_comparison.append({\n        'Model': name,\n        'Original MAE': original_results['mae'],\n        'Tuned MAE': tuned_results['mae'],\n        'MAE Improvement': original_results['mae'] - tuned_results['mae'],\n        'MAE Improvement %': (original_results['mae'] - tuned_results['mae']) / original_results['mae'] * 100,\n        'Original RMSE': original_results['rmse'],\n        'Tuned RMSE': tuned_results['rmse'],\n        'RMSE Improvement': original_results['rmse'] - tuned_results['rmse'],\n        'RMSE Improvement %': (original_results['rmse'] - tuned_results['rmse']) / original_results['rmse'] * 100,\n        'Original R²': original_results['r2'],\n        'Tuned R²': tuned_results['r2'],\n        'R² Improvement': tuned_results['r2'] - original_results['r2'],\n        'Original Spearman': original_results['spearman'],\n        'Tuned Spearman': tuned_results['spearman'],\n        'Spearman Improvement': tuned_results['spearman'] - original_results['spearman']\n    })\n\n# Add ensemble comparison\nmodel_comparison.append({\n    'Model': 'Ensemble',\n    'Original MAE': model_results['Ensemble']['mae'],\n    'Tuned MAE': tuned_ensemble_results['mae'],\n    'MAE Improvement': model_results['Ensemble']['mae'] - tuned_ensemble_results['mae'],\n    'MAE Improvement %': (model_results['Ensemble']['mae'] - tuned_ensemble_results['mae']) / model_results['Ensemble']['mae'] * 100,\n    'Original RMSE': model_results['Ensemble']['rmse'],\n    'Tuned RMSE': tuned_ensemble_results['rmse'],\n    'RMSE Improvement': model_results['Ensemble']['rmse'] - tuned_ensemble_results['rmse'],\n    'RMSE Improvement %': (model_results['Ensemble']['rmse'] - tuned_ensemble_results['rmse']) / model_results['Ensemble']['rmse'] * 100,\n    'Original R²': model_results['Ensemble']['r2'],\n    'Tuned R²': tuned_ensemble_results['r2'],\n    'R² Improvement': tuned_ensemble_results['r2'] - model_results['Ensemble']['r2'],\n    'Original Spearman': model_results['Ensemble']['spearman'],\n    'Tuned Spearman': tuned_ensemble_results['spearman'],\n    'Spearman Improvement': tuned_ensemble_results['spearman'] - model_results['Ensemble']['spearman']\n})\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(model_comparison)\nprint(\"\\nComparison of Original vs. Tuned Models:\")\nprint(comparison_df[['Model', 'Original MAE', 'Tuned MAE', 'MAE Improvement %', \n                     'Original R²', 'Tuned R²', 'R² Improvement',\n                     'Original Spearman', 'Tuned Spearman', 'Spearman Improvement']])\n\n# Save comparison\ncomparison_df.to_csv(f'{results_dir}/model_comparison.csv', index=False)\n\n# Create comparison directory\ncomparison_dir = f'{results_dir}/comparison'\nos.makedirs(comparison_dir, exist_ok=True)\n\n# Visualize improvements\n# MAE comparison\nplt.figure(figsize=(10, 6))\nbarWidth = 0.35\nr1 = np.arange(len(comparison_df))\nr2 = [x + barWidth for x in r1]\n\nplt.bar(r1, comparison_df['Original MAE'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned MAE'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('MAE Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Mean Absolute Error')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/MAE_comparison.png')\nplt.close()\n\n# RMSE comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original RMSE'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned RMSE'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('RMSE Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Root Mean Squared Error')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/RMSE_comparison.png')\nplt.close()\n\n# R² comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original R²'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned R²'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('R² Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('R²')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/R2_comparison.png')\nplt.close()\n\n# Spearman comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original Spearman'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned Spearman'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('Spearman Correlation Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Spearman Correlation')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/Spearman_comparison.png')\nplt.close()\n\n# Improvement heatmap\nimprovement_data = comparison_df[['Model', 'MAE Improvement %', 'RMSE Improvement %', 'R² Improvement', 'Spearman Improvement']]\nimprovement_data = improvement_data.set_index('Model')\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(improvement_data, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Model Improvement Heatmap')\nplt.tight_layout()\nplt.savefig(f'{comparison_dir}/improvement_heatmap.png')\nplt.close()\n\n# Best model comparison\nplt.figure(figsize=(12, 6))\n\n# Find best model based on Spearman correlation\nbest_model_idx = comparison_df['Tuned Spearman'].idxmax()\nbest_model = comparison_df.iloc[best_model_idx]['Model']\n\nmetrics = ['MAE', 'RMSE', 'R²', 'Spearman']\noriginal_values = [comparison_df.iloc[best_model_idx]['Original MAE'],\n                  comparison_df.iloc[best_model_idx]['Original RMSE'],\n                  comparison_df.iloc[best_model_idx]['Original R²'],\n                  comparison_df.iloc[best_model_idx]['Original Spearman']]\ntuned_values = [comparison_df.iloc[best_model_idx]['Tuned MAE'],\n                comparison_df.iloc[best_model_idx]['Tuned RMSE'],\n                comparison_df.iloc[best_model_idx]['Tuned R²'],\n                comparison_df.iloc[best_model_idx]['Tuned Spearman']]\n\n# Create barplot\nplt.subplot(1, 2, 1)\nbarWidth = 0.35\nr1 = np.arange(len(metrics))\nr2 = [x + barWidth for x in r1]\n\nplt.bar(r1, original_values[:2], width=barWidth, label='Original', color='skyblue')  # MAE and RMSE\nplt.bar(r2, tuned_values[:2], width=barWidth, label='Tuned', color='lightgreen')  # MAE and RMSE\nplt.title(f'Best Model: {best_model} (Error Metrics)')\nplt.ylabel('Error Value (lower is better)')\nplt.xticks([r + barWidth/2 for r in range(2)], metrics[:2])\nplt.legend()\n\n# Create barplot for R² and Spearman\nplt.subplot(1, 2, 2)\nplt.bar(r1[2:], original_values[2:], width=barWidth, label='Original', color='skyblue')  # R² and Spearman\nplt.bar(r2[2:], tuned_values[2:], width=barWidth, label='Tuned', color='lightgreen')  # R² and Spearman\nplt.title(f'Best Model: {best_model} (Correlation Metrics)')\nplt.ylabel('Correlation Value (higher is better)')\nplt.xticks([r + barWidth/2 for r in range(2)], metrics[2:])\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(f'{comparison_dir}/best_models_comparison.png')\nplt.close()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Final Model Evaluation and Selection"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Final model evaluation on the test set\nprint(\"\\nEvaluating best model on the test set...\")\n\n# Find the best model based on validation results\nmodel_names = list(tuned_model_results.keys())\nmodel_names.remove('Ensemble')  # Remove ensemble from consideration\n\nbest_model_name = model_names[0]\nbest_score = tuned_model_results[best_model_name]['spearman']\n\nfor name in model_names[1:]:\n    if tuned_model_results[name]['spearman'] > best_score:\n        best_score = tuned_model_results[name]['spearman']\n        best_model_name = name\n\nprint(f\"Best model is {best_model_name} with validation Spearman correlation of {best_score:.4f}\")\n\n# Load the best model\nwith open(f'{results_dir}/tuned_{best_model_name.replace(\" \", \"_\")}_model.pkl', 'rb') as f:\n    best_model = pickle.load(f)\n\n# Evaluate on test set\ntest_results = evaluate_model(best_model, X_test, y_test, f\"Best Model ({best_model_name})\", is_val=False)\n\n# Save as the best tuned model\nwith open(f'{results_dir}/best_tuned_model.pkl', 'wb') as f:\n    pickle.dump(best_model, f)\n\n# Generate feature importance for the best model\nif hasattr(best_model, 'feature_importances_'):\n    best_importance = plot_feature_importance(best_model, f\"Best Model ({best_model_name})\")\n    best_importance.to_csv(f'{results_dir}/best_tuned_model_feature_importance.csv', index=False)\n    \n    # Plot feature importance by category\n    if best_importance is not None:\n        # Add category to importance DataFrame\n        best_importance['Category'] = best_importance['Feature'].apply(format_column_group)\n        \n        # Aggregate by category\n        category_importance = best_importance.groupby('Category')['Importance'].sum().reset_index()\n        category_importance = category_importance.sort_values('Importance', ascending=False)\n        \n        # Plot category importance\n        plt.figure(figsize=(10, 6))\n        plt.pie(category_importance['Importance'], labels=category_importance['Category'], \n                autopct='%1.1f%%', startangle=90)\n        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n        plt.title(f'Feature Importance by Category')\n        plt.tight_layout()\n        plt.savefig(f'{results_dir}/best_tuned_model_feature_importance_by_category.png')\n        plt.close()\n        \n        # Also create a bar chart of category importance\n        plt.figure(figsize=(10, 6))\n        sns.barplot(x='Importance', y='Category', data=category_importance)\n        plt.title('Feature Category Importance')\n        plt.tight_layout()\n        plt.savefig(f'{results_dir}/category_importance_bar.png')\n        plt.close()\n        \n        # Save category importance\n        category_importance.to_csv(f'{results_dir}/category_importance.csv', index=False)\n\n# Create text summary of the best model\nwith open(f'{results_dir}/model_summary.txt', 'w') as f:\n    f.write(f\"# Task Effort Estimation Model Summary\\n\\n\")\n    f.write(f\"## Best Model: {best_model_name}\\n\\n\")\n    \n    f.write(f\"### Test Set Performance\\n\")\n    f.write(f\"- MAE: {test_results['mae']:.2f} hours\\n\")\n    f.write(f\"- RMSE: {test_results['rmse']:.2f} hours\\n\")\n    f.write(f\"- R²: {test_results['r2']:.4f}\\n\")\n    f.write(f\"- Spearman Correlation: {test_results['spearman']:.4f}\\n\\n\")\n    \n    if hasattr(best_model, 'feature_importances_'):\n        f.write(f\"### Top 10 Most Important Features\\n\")\n        for idx, row in best_importance.head(10).iterrows():\n            f.write(f\"- {row['Feature']}: {row['Importance']:.4f}\\n\")\n        \n        f.write(f\"\\n### Feature Importance by Category\\n\")\n        for idx, row in category_importance.iterrows():\n            f.write(f\"- {row['Category']}: {row['Importance']:.4f} ({row['Importance']*100:.1f}%)\\n\")\n    \n    f.write(f\"\\n### Model Hyperparameters\\n\")\n    for param, value in best_model.get_params().items():\n        f.write(f\"- {param}: {value}\\n\")\n\nprint(\"\\nFinal model evaluation complete. Results saved to the 'model_summary.txt' file.\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Task Effort Predictor Function"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create a prediction function for new tasks\ndef predict_task_effort(task_features, model_path=f'{results_dir}/best_tuned_model.pkl'):\n    \"\"\"\n    Predict resolution hours for a new task.\n    \n    Parameters:\n    -----------\n    task_features : dict\n        Dictionary of task features\n    model_path : str\n        Path to the trained model pickle file\n    \n    Returns:\n    --------\n    dict\n        Dictionary containing the prediction and additional information\n    \"\"\"\n    # Load the trained model\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n    \n    # Load the feature names used during training\n    with open(f'{results_dir}/data_splits.pkl', 'rb') as f:\n        splits = pickle.load(f)\n        feature_names = splits['feature_names']\n    \n    # Convert task features to DataFrame\n    task_df = pd.DataFrame([task_features])\n    \n    # Ensure all required features are present\n    for feat in feature_names:\n        if feat not in task_df.columns:\n            task_df[feat] = 0  # Default value for missing features\n    \n    # Keep only the features used in training\n    task_df = task_df[feature_names]\n    \n    # Make prediction\n    pred_log = model.predict(task_df)[0]\n    \n    # Convert from log space\n    pred_hours = np.expm1(pred_log)\n    \n    # Calculate prediction intervals (approximate)\n    lower_bound = pred_hours * 0.7  # 30% lower than prediction\n    upper_bound = pred_hours * 1.3  # 30% higher than prediction\n    \n    return {\n        'prediction_hours': pred_hours,\n        'prediction_days': pred_hours / 24,\n        'prediction_work_days': pred_hours / 8,  # Assuming 8-hour workdays\n        'lower_bound': lower_bound,\n        'upper_bound': upper_bound\n    }\n\n# Example usage\nprint(\"\\nExample Task Effort Prediction:\")\n\n# Create example task features\nexample_task = {\n    'is_type_bug': 1,\n    'is_priority_major': 1,\n    'inward_count': 2,\n    'outward_count': 1,\n    'age_days': 5,\n    'created_is_weekend': 0,\n    'created_hour': 14,\n    'is_completed': 0\n}\n\n# Make prediction\nprediction = predict_task_effort(example_task)\nprint(f\"Predicted resolution time: {prediction['prediction_hours']:.2f} hours\")\nprint(f\"Equivalent to: {prediction['prediction_days']:.2f} days or {prediction['prediction_work_days']:.2f} work days\")\nprint(f\"Prediction interval: {prediction['lower_bound']:.2f} to {prediction['upper_bound']:.2f} hours\")\n\n# Save the predictor function as a separate Python module\nwith open(f'{results_dir}/task_effort_predictor.py', 'w') as f:\n    f.write(\"\"\"\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n\ndef predict_task_effort(task_features, model_path='best_tuned_model.pkl'):\n    \"\"\"\n    Predict resolution hours for a new task.\n    \n    Parameters:\n    -----------\n    task_features : dict\n        Dictionary of task features\n    model_path : str\n        Path to the trained model pickle file\n    \n    Returns:\n    --------\n    dict\n        Dictionary containing the prediction and additional information\n    \"\"\"\n    # Load the trained model\n    with open(model_path, 'rb') as f:\n        model = pickle.load(f)\n    \n    # Load the feature names used during training\n    with open('data_splits.pkl', 'rb') as f:\n        splits = pickle.load(f)\n        feature_names = splits['feature_names']\n    \n    # Convert task features to DataFrame\n    task_df = pd.DataFrame([task_features])\n    \n    # Ensure all required features are present\n    for feat in feature_names:\n        if feat not in task_df.columns:\n            task_df[feat] = 0  # Default value for missing features\n    \n    # Keep only the features used in training\n    task_df = task_df[feature_names]\n    \n    # Make prediction\n    pred_log = model.predict(task_df)[0]\n    \n    # Convert from log space\n    pred_hours = np.expm1(pred_log)\n    \n    # Calculate prediction intervals (approximate)\n    lower_bound = pred_hours * 0.7  # 30% lower than prediction\n    upper_bound = pred_hours * 1.3  # 30% higher than prediction\n    \n    return {\n        'prediction_hours': pred_hours,\n        'prediction_days': pred_hours / 24,\n        'prediction_work_days': pred_hours / 8,  # Assuming 8-hour workdays\n        'lower_bound': lower_bound,\n        'upper_bound': upper_bound\n    }\n\ndef get_task_effort_range(task_type, priority, complexity):\n    \"\"\"\n    Get predefined effort ranges based on task type, priority, and complexity.\n    This is a simplified example that could be expanded with more detailed logic.\n    \n    Parameters:\n    -----------\n    task_type : str\n        Type of task (e.g. 'bug', 'feature', 'improvement')\n    priority : str\n        Priority level (e.g. 'low', 'medium', 'high', 'critical')\n    complexity : str\n        Complexity level (e.g. 'low', 'medium', 'high')\n    \n    Returns:\n    --------\n    dict\n        Dictionary with min, max, and typical effort in hours\n    \"\"\"\n    # Define base hours by task type\n    if task_type.lower() == 'bug':\n        base_hours = 8  # 1 day\n    elif task_type.lower() == 'feature':\n        base_hours = 24  # 3 days\n    elif task_type.lower() == 'improvement':\n        base_hours = 16  # 2 days\n    else:\n        base_hours = 12  # 1.5 days (default)\n    \n    # Apply priority multiplier\n    if priority.lower() == 'critical':\n        priority_mult = 0.8  # Critical items might be fixed faster\n    elif priority.lower() == 'high':\n        priority_mult = 0.9\n    elif priority.lower() == 'low':\n        priority_mult = 1.2\n    else:  # medium\n        priority_mult = 1.0\n    \n    # Apply complexity multiplier\n    if complexity.lower() == 'high':\n        complexity_mult = 2.0\n    elif complexity.lower() == 'low':\n        complexity_mult = 0.5\n    else:  # medium\n        complexity_mult = 1.0\n    \n    # Calculate typical hours\n    typical_hours = base_hours * priority_mult * complexity_mult\n    \n    # Define range\n    return {\n        'min_hours': typical_hours * 0.7,\n        'typical_hours': typical_hours,\n        'max_hours': typical_hours * 1.5,\n        'work_days': typical_hours / 8\n    }\n\"\"\")\n\nprint(\"\\nTask effort predictor module saved to 'task_effort_predictor.py'\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusion and Insights"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Summarize key findings and insights\nprint(\"\\nKey Findings and Insights:\")\n\n# Load the best model metrics\nwith open(f'{results_dir}/model_summary.txt', 'r') as f:\n    model_summary = f.read()\n\n# Extract metrics from summary\nimport re\nmae_match = re.search(r'MAE: ([0-9.]+)', model_summary)\nrmse_match = re.search(r'RMSE: ([0-9.]+)', model_summary)\nr2_match = re.search(r'R²: ([0-9.]+)', model_summary)\nspearman_match = re.search(r'Spearman Correlation: ([0-9.]+)', model_summary)\n\nmae = float(mae_match.group(1)) if mae_match else 0\nrmse = float(rmse_match.group(1)) if rmse_match else 0\nr2 = float(r2_match.group(1)) if r2_match else 0\nspearman = float(spearman_match.group(1)) if spearman_match else 0\n\n# Print model metrics\nprint(f\"1. Model Performance:\")\nprint(f\"   - Our best model achieves a Mean Absolute Error of {mae:.2f} hours\")\nprint(f\"   - The Spearman correlation of {spearman:.4f} indicates a moderate to strong ability to rank tasks by effort\")\nprint(f\"   - The model explains approximately {r2*100:.1f}% of the variance in task resolution time\")\n\n# Load category importance\ntry:\n    category_importance = pd.read_csv(f'{results_dir}/category_importance.csv')\n    print(f\"\\n2. Most Important Feature Categories:\")\n    for idx, row in category_importance.head(3).iterrows():\n        print(f\"   - {row['Category']}: {row['Importance']*100:.1f}% importance\")\nexcept:\n    pass\n\n# Load feature importance\ntry:\n    feature_importance = pd.read_csv(f'{results_dir}/best_tuned_model_feature_importance.csv')\n    print(f\"\\n3. Top 5 Individual Features:\")\n    for idx, row in feature_importance.head(5).iterrows():\n        print(f\"   - {row['Feature']}: {row['Importance']*100:.1f}% importance\")\nexcept:\n    pass\n\n# Create final visualization summarizing actual vs. predicted\nprint(\"\\nGenerating final visualization of model performance...\")\n\n# Get predictions from best model on test set\ny_pred = best_model.predict(X_test)\ny_test_orig = np.expm1(y_test)\ny_pred_orig = np.expm1(y_pred)\n\n# Create scatter plot with hexbin for density\nplt.figure(figsize=(10, 8))\nplt.hexbin(y_test_orig, y_pred_orig, gridsize=50, cmap='viridis', bins='log')\nplt.plot([0, y_test_orig.max()], [0, y_test_orig.max()], 'r--', linewidth=2)\nplt.xlabel('Actual Resolution Hours', fontsize=14)\nplt.ylabel('Predicted Resolution Hours', fontsize=14)\nplt.title('Task Effort Estimation: Actual vs. Predicted', fontsize=16)\n\n# Add annotation with model performance\nplt.annotate(\n    f\"MAE: {mae:.1f} hours\\nRMSE: {rmse:.1f} hours\\nR²: {r2:.3f}\\nSpearman: {spearman:.3f}\", \n    xy=(0.05, 0.95), xycoords='axes fraction',\n    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n    fontsize=12, verticalalignment='top'\n)\n\ncb = plt.colorbar()\ncb.set_label('Log Count (Density)', fontsize=12)\nplt.tight_layout()\nplt.savefig(f'{results_dir}/actual_vs_predicted.png', dpi=300)\nplt.savefig('task_effort_prediction.png', dpi=300)  # Save a copy in the main directory\nplt.close()\n\nprint(\"\\nSummary visualization saved as 'task_effort_prediction.png'\")\nprint(\"\\nTask effort estimation model complete!\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}