{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset and perform initial preprocessing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['fields.resolutiondate', 'fields.created', 'fields.updated']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Create basic derived features\n",
    "    if 'fields.created' in df.columns and 'fields.resolutiondate' in df.columns:\n",
    "        # Only calculate resolution time for resolved issues\n",
    "        resolved_mask = ~df['fields.resolutiondate'].isna()\n",
    "        df.loc[resolved_mask, 'resolution_time_hours'] = (\n",
    "            df.loc[resolved_mask, 'fields.resolutiondate'] - \n",
    "            df.loc[resolved_mask, 'fields.created']\n",
    "        ).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Extract day of week and hour of day for temporal analysis\n",
    "    if 'fields.created' in df.columns:\n",
    "        df['created_day_of_week'] = df['fields.created'].dt.dayofweek\n",
    "        df['created_hour'] = df['fields.created'].dt.hour\n",
    "        df['created_month'] = df['fields.created'].dt.month\n",
    "        df['created_year'] = df['fields.created'].dt.year\n",
    "    \n",
    "    if 'fields.resolutiondate' in df.columns:\n",
    "        df['resolved_day_of_week'] = df['fields.resolutiondate'].dt.dayofweek\n",
    "        df['resolved_hour'] = df['fields.resolutiondate'].dt.hour\n",
    "    \n",
    "    return df\n",
    "\n",
    "def aggregate_project_features(df, project_id_col='fields.project.id'):\n",
    "    \"\"\"\n",
    "    Aggregate features at the project level to create a single row per project\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing issue-level data\n",
    "    project_id_col : str\n",
    "        The column name that contains project IDs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with one row per project and aggregated features\n",
    "    \"\"\"\n",
    "    project_features = {}\n",
    "    \n",
    "    # Group data by project\n",
    "    project_groups = df.groupby(project_id_col)\n",
    "    \n",
    "    for project_id, project_df in project_groups:\n",
    "        features = {}\n",
    "        \n",
    "        # Add project identifier\n",
    "        features['project_id'] = project_id\n",
    "        features['project_key'] = project_df['fields.project.key'].iloc[0] if 'fields.project.key' in project_df.columns else None\n",
    "        features['project_name'] = project_df['fields.project.name'].iloc[0] if 'fields.project.name' in project_df.columns else None\n",
    "        \n",
    "        # Basic issue counts\n",
    "        features['total_issues'] = len(project_df)\n",
    "        \n",
    "        # 1. Temporal Features\n",
    "        # Overall project timespan\n",
    "        if 'fields.created' in project_df.columns and 'fields.resolutiondate' in project_df.columns:\n",
    "            features['project_start_date'] = project_df['fields.created'].min()\n",
    "            \n",
    "            # For end date, use the latest resolved issue or latest update if no resolutions\n",
    "            resolved_issues = project_df[~project_df['fields.resolutiondate'].isna()]\n",
    "            if len(resolved_issues) > 0:\n",
    "                features['project_latest_resolved_date'] = resolved_issues['fields.resolutiondate'].max()\n",
    "            else:\n",
    "                features['project_latest_resolved_date'] = None\n",
    "                \n",
    "            if 'fields.updated' in project_df.columns:\n",
    "                features['project_latest_update_date'] = project_df['fields.updated'].max()\n",
    "            \n",
    "            # Calculate project duration in days (using latest of either resolution or update)\n",
    "            if features['project_latest_resolved_date'] is not None:\n",
    "                latest_date = max(\n",
    "                    features['project_latest_resolved_date'],\n",
    "                    features.get('project_latest_update_date', features['project_latest_resolved_date'])\n",
    "                )\n",
    "                features['project_duration_days'] = (latest_date - features['project_start_date']).days\n",
    "            elif 'project_latest_update_date' in features:\n",
    "                features['project_duration_days'] = (features['project_latest_update_date'] - features['project_start_date']).days\n",
    "            else:\n",
    "                features['project_duration_days'] = None\n",
    "        \n",
    "        # Resolution time statistics for resolved issues\n",
    "        resolved_issues = project_df[~project_df['fields.resolutiondate'].isna()]\n",
    "        if 'resolution_time_hours' in project_df.columns and len(resolved_issues) > 0:\n",
    "            resolution_times = resolved_issues['resolution_time_hours']\n",
    "            \n",
    "            features['avg_resolution_hours'] = resolution_times.mean()\n",
    "            features['median_resolution_hours'] = resolution_times.median()\n",
    "            features['min_resolution_hours'] = resolution_times.min()\n",
    "            features['max_resolution_hours'] = resolution_times.max()\n",
    "            features['resolution_hours_std'] = resolution_times.std()\n",
    "            features['total_resolution_hours'] = resolution_times.sum()\n",
    "            \n",
    "            # Distribution metrics for resolution times\n",
    "            if len(resolution_times) > 2:  # Need at least 3 points for skewness/kurtosis\n",
    "                features['resolution_time_skewness'] = stats.skew(resolution_times)\n",
    "                features['resolution_time_kurtosis'] = stats.kurtosis(resolution_times)\n",
    "            \n",
    "            # Resolution time percentiles\n",
    "            features['resolution_time_p25'] = resolution_times.quantile(0.25)\n",
    "            features['resolution_time_p75'] = resolution_times.quantile(0.75)\n",
    "            features['resolution_time_p90'] = resolution_times.quantile(0.90)\n",
    "            features['resolution_time_iqr'] = features['resolution_time_p75'] - features['resolution_time_p25']\n",
    "            \n",
    "            # Proportion of issues resolved in different time frames\n",
    "            features['pct_resolved_within_24h'] = (resolution_times <= 24).mean() * 100\n",
    "            features['pct_resolved_within_week'] = (resolution_times <= 168).mean() * 100  # 168 hours = 1 week\n",
    "            features['pct_resolved_within_month'] = (resolution_times <= 720).mean() * 100  # 720 hours â‰ˆ 30 days\n",
    "        \n",
    "        # Temporal patterns\n",
    "        if 'fields.created' in project_df.columns:\n",
    "            # Weekend vs. weekday metrics\n",
    "            if 'created_day_of_week' in project_df.columns:\n",
    "                weekend_created = project_df['created_day_of_week'].isin([5, 6])  # 5=Saturday, 6=Sunday\n",
    "                features['pct_issues_created_on_weekend'] = weekend_created.mean() * 100\n",
    "            \n",
    "            if 'resolved_day_of_week' in project_df.columns:\n",
    "                weekend_resolved = project_df['resolved_day_of_week'].isin([5, 6])\n",
    "                resolved_issues_count = (~project_df['fields.resolutiondate'].isna()).sum()\n",
    "                if resolved_issues_count > 0:\n",
    "                    features['pct_issues_resolved_on_weekend'] = (\n",
    "                        weekend_resolved & ~project_df['fields.resolutiondate'].isna()\n",
    "                    ).sum() / resolved_issues_count * 100\n",
    "            \n",
    "            # Creation patterns by month\n",
    "            if 'created_month' in project_df.columns and 'created_year' in project_df.columns:\n",
    "                # Group issues by year-month and count\n",
    "                monthly_counts = project_df.groupby(['created_year', 'created_month']).size()\n",
    "                if len(monthly_counts) > 0:\n",
    "                    features['max_issues_per_month'] = monthly_counts.max()\n",
    "                    features['avg_issues_per_month'] = monthly_counts.mean()\n",
    "                    features['months_with_activity'] = len(monthly_counts)\n",
    "                    if features['months_with_activity'] > 1:\n",
    "                        features['issue_creation_volatility'] = monthly_counts.std() / monthly_counts.mean()\n",
    "        \n",
    "        # 2. Priority and Issue Type Features\n",
    "        # Count issues by priority\n",
    "        if 'priority_name' in project_df.columns:\n",
    "            priority_counts = project_df['priority_name'].value_counts()\n",
    "            total_with_priority = priority_counts.sum()\n",
    "            \n",
    "            for priority in priority_counts.index:\n",
    "                col_name = f'priority_{priority.lower().replace(\" \", \"_\")}_count'\n",
    "                features[col_name] = priority_counts[priority]\n",
    "                \n",
    "                # Also add as percentage\n",
    "                col_pct_name = f'priority_{priority.lower().replace(\" \", \"_\")}_pct'\n",
    "                features[col_pct_name] = (priority_counts[priority] / total_with_priority * 100) if total_with_priority > 0 else 0\n",
    "        \n",
    "        # Alternative approach for priority using binary columns if they exist\n",
    "        priority_cols = [col for col in project_df.columns if col.startswith('priority_') and col != 'priority_name' and col != 'priority_id']\n",
    "        if priority_cols:\n",
    "            for col in priority_cols:\n",
    "                features[f'{col}_count'] = project_df[col].sum()\n",
    "                features[f'{col}_pct'] = (project_df[col].sum() / len(project_df) * 100)\n",
    "        \n",
    "        # Count issues by type\n",
    "        if 'issue_type' in project_df.columns:\n",
    "            type_counts = project_df['issue_type'].value_counts()\n",
    "            \n",
    "            for issue_type in type_counts.index:\n",
    "                col_name = f'type_{issue_type.lower().replace(\" \", \"_\")}_count'\n",
    "                features[col_name] = type_counts[issue_type]\n",
    "                \n",
    "                # Also add as percentage\n",
    "                col_pct_name = f'type_{issue_type.lower().replace(\" \", \"_\")}_pct'\n",
    "                features[col_pct_name] = (type_counts[issue_type] / len(project_df) * 100)\n",
    "        \n",
    "        # Alternative approach for issue types using binary columns if they exist\n",
    "        type_cols = [col for col in project_df.columns if col.startswith('type_')]\n",
    "        if type_cols:\n",
    "            for col in type_cols:\n",
    "                features[f'{col}_count'] = project_df[col].sum()\n",
    "                features[f'{col}_pct'] = (project_df[col].sum() / len(project_df) * 100)\n",
    "        \n",
    "        # Priority and issue type combinations\n",
    "        # For each priority, calculate resolution metrics by issue type\n",
    "        if 'priority_name' in project_df.columns and 'issue_type' in project_df.columns and 'resolution_time_hours' in project_df.columns:\n",
    "            for priority in project_df['priority_name'].unique():\n",
    "                for issue_type in project_df['issue_type'].unique():\n",
    "                    # Filter issues with this priority and type that have been resolved\n",
    "                    filtered = project_df[\n",
    "                        (project_df['priority_name'] == priority) & \n",
    "                        (project_df['issue_type'] == issue_type) &\n",
    "                        ~project_df['fields.resolutiondate'].isna()\n",
    "                    ]\n",
    "                    \n",
    "                    if len(filtered) > 0:\n",
    "                        prefix = f'priority_{priority.lower().replace(\" \", \"_\")}_type_{issue_type.lower().replace(\" \", \"_\")}'\n",
    "                        features[f'{prefix}_count'] = len(filtered)\n",
    "                        features[f'{prefix}_avg_resolution_hours'] = filtered['resolution_time_hours'].mean()\n",
    "        \n",
    "        # 3. Issue Dependencies and Complexity\n",
    "        if 'inward_count' in project_df.columns and 'outward_count' in project_df.columns:\n",
    "            # Average link counts\n",
    "            features['avg_inward_links'] = project_df['inward_count'].mean()\n",
    "            features['avg_outward_links'] = project_df['outward_count'].mean()\n",
    "            features['avg_total_links'] = project_df['inward_count'].add(project_df['outward_count']).mean()\n",
    "            \n",
    "            # Total link counts for the project\n",
    "            features['total_inward_links'] = project_df['inward_count'].sum()\n",
    "            features['total_outward_links'] = project_df['outward_count'].sum()\n",
    "            features['total_links'] = features['total_inward_links'] + features['total_outward_links']\n",
    "            \n",
    "            # Issues with many dependencies\n",
    "            high_dependency_threshold = project_df['inward_count'].quantile(0.75)\n",
    "            features['pct_issues_with_high_dependencies'] = (\n",
    "                (project_df['inward_count'] > high_dependency_threshold).mean() * 100\n",
    "            )\n",
    "            \n",
    "            # Link density (average links per issue)\n",
    "            features['link_density'] = features['total_links'] / features['total_issues'] if features['total_issues'] > 0 else 0\n",
    "        \n",
    "        # 4. Resolution Efficiency\n",
    "        if 'is_resolved' in project_df.columns:\n",
    "            features['num_resolved_issues'] = project_df['is_resolved'].sum()\n",
    "            features['pct_resolved_issues'] = features['num_resolved_issues'] / features['total_issues'] * 100 if features['total_issues'] > 0 else 0\n",
    "        \n",
    "        # Resolution rate over time\n",
    "        if 'fields.created' in project_df.columns and 'fields.resolutiondate' in project_df.columns and features.get('project_duration_days', 0) > 0:\n",
    "            features['resolution_rate_per_day'] = features.get('num_resolved_issues', 0) / features['project_duration_days']\n",
    "        \n",
    "        # Resolution efficiency by issue type\n",
    "        if 'issue_type' in project_df.columns and 'is_resolved' in project_df.columns:\n",
    "            for issue_type in project_df['issue_type'].unique():\n",
    "                filtered = project_df[project_df['issue_type'] == issue_type]\n",
    "                if len(filtered) > 0:\n",
    "                    type_key = issue_type.lower().replace(\" \", \"_\")\n",
    "                    features[f'type_{type_key}_resolution_rate'] = filtered['is_resolved'].mean() * 100\n",
    "        \n",
    "        # Add features to the project_features dictionary\n",
    "        project_features[project_id] = features\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame.from_dict(project_features, orient='index')\n",
    "    \n",
    "    # Fill NaN values with appropriate defaults or remove them\n",
    "    result_df = result_df.fillna({\n",
    "        'pct_resolved_issues': 0,\n",
    "        'resolution_rate_per_day': 0,\n",
    "        # Add other fields as needed\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def feature_engineering(project_df):\n",
    "    \"\"\"\n",
    "    Perform additional feature engineering on the aggregated project data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project_df : pandas.DataFrame\n",
    "        DataFrame with one row per project\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df = project_df.copy()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    \n",
    "    # 1. Efficiency ratio = completed issues / total issues / project duration in weeks\n",
    "    if 'num_resolved_issues' in df.columns and 'project_duration_days' in df.columns and 'total_issues' in df.columns:\n",
    "        # Avoid division by zero\n",
    "        denominator = df['project_duration_days'] * df['total_issues']\n",
    "        df['weekly_efficiency_ratio'] = np.where(\n",
    "            denominator > 0,\n",
    "            df['num_resolved_issues'] * 7 / denominator,\n",
    "            0  # Default value when denominator is 0\n",
    "        )\n",
    "    \n",
    "    # 2. Complexity-weighted resolution time\n",
    "    if 'avg_resolution_hours' in df.columns and 'avg_total_links' in df.columns:\n",
    "        # Add a small constant to ensure we don't multiply by zero\n",
    "        df['complexity_weighted_resolution_time'] = df['avg_resolution_hours'] * (df['avg_total_links'] + 1)\n",
    "    \n",
    "    # 3. Priority balance - ratio of high priority to low priority issues\n",
    "    high_priority_cols = [col for col in df.columns if ('priority_critical' in col or 'priority_blocker' in col or 'priority_major' in col) and '_count' in col]\n",
    "    low_priority_cols = [col for col in df.columns if ('priority_minor' in col or 'priority_trivial' in col) and '_count' in col]\n",
    "    \n",
    "    if high_priority_cols and low_priority_cols:\n",
    "        high_priority_sum = df[high_priority_cols].sum(axis=1)\n",
    "        low_priority_sum = df[low_priority_cols].sum(axis=1)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        df['high_to_low_priority_ratio'] = np.where(\n",
    "            low_priority_sum > 0,\n",
    "            high_priority_sum / low_priority_sum,\n",
    "            high_priority_sum  # If no low priority issues, just use the high priority count\n",
    "        )\n",
    "    \n",
    "    # 4. Bug ratio - proportion of bugs to total issues\n",
    "    bug_cols = [col for col in df.columns if 'type_bug' in col and '_count' in col]\n",
    "    if bug_cols and 'total_issues' in df.columns:\n",
    "        df['bug_ratio'] = df[bug_cols].sum(axis=1) / df['total_issues']\n",
    "    \n",
    "    # 5. Creation-resolution balance - how evenly distributed is the workload\n",
    "    if 'avg_issues_per_month' in df.columns and 'resolution_rate_per_day' in df.columns:\n",
    "        monthly_creation_rate = df['avg_issues_per_month'] / 30  # Convert to daily rate\n",
    "        # Balance = 1 means perfect balance, < 1 means resolution is slower than creation\n",
    "        df['creation_resolution_balance'] = np.where(\n",
    "            monthly_creation_rate > 0,\n",
    "            df['resolution_rate_per_day'] / monthly_creation_rate,\n",
    "            0  # Default value when monthly_creation_rate is 0\n",
    "        )\n",
    "    \n",
    "    # 6. Weighted priority score\n",
    "    priority_weight_cols = {\n",
    "        'priority_blocker': 5, \n",
    "        'priority_critical': 4, \n",
    "        'priority_major': 3, \n",
    "        'priority_minor': 2, \n",
    "        'priority_trivial': 1\n",
    "    }\n",
    "    \n",
    "    priority_count_cols = [col for col in df.columns if any(p in col for p in priority_weight_cols.keys()) and '_count' in col]\n",
    "    \n",
    "    if priority_count_cols and 'total_issues' in df.columns:\n",
    "        weighted_sum = 0\n",
    "        for col in priority_count_cols:\n",
    "            # Extract the priority name from the column name\n",
    "            for priority_name, weight in priority_weight_cols.items():\n",
    "                if priority_name in col:\n",
    "                    weighted_sum += df[col] * weight\n",
    "                    break\n",
    "        \n",
    "        df['weighted_priority_score'] = weighted_sum / df['total_issues']\n",
    "    \n",
    "    # 7. Issue diversity - entropy of issue type distribution\n",
    "    type_pct_cols = [col for col in df.columns if col.startswith('type_') and '_pct' in col]\n",
    "    if type_pct_cols:\n",
    "        # Convert percentages to proportions\n",
    "        proportions = df[type_pct_cols].div(100)\n",
    "        \n",
    "        # Calculate entropy for each row (project)\n",
    "        def entropy(row):\n",
    "            # Filter out zero values to avoid log(0)\n",
    "            props = row[row > 0]\n",
    "            if len(props) == 0:\n",
    "                return 0\n",
    "            return -sum(props * np.log2(props))\n",
    "        \n",
    "        df['issue_type_entropy'] = proportions.apply(entropy, axis=1)\n",
    "    \n",
    "    # 8. Project velocity over time (if there's enough temporal data)\n",
    "    if 'num_resolved_issues' in df.columns and 'months_with_activity' in df.columns and df['months_with_activity'].max() > 1:\n",
    "        df['monthly_velocity'] = df['num_resolved_issues'] / df['months_with_activity']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_repository_folders(input_base_dir, output_base_dir):\n",
    "    \"\"\"\n",
    "    Process all repositories in the input base directory and save the results to the output base directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_base_dir : str\n",
    "        Base directory containing repository folders with issue data\n",
    "    output_base_dir : str\n",
    "        Base directory to save processed project-level data\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Get a list of all repository folders\n",
    "    repo_folders = [f for f in os.listdir(input_base_dir) if os.path.isdir(os.path.join(input_base_dir, f))]\n",
    "    \n",
    "    print(f\"Found {len(repo_folders)} repository folders in {input_base_dir}\")\n",
    "    \n",
    "    # Process each repository folder\n",
    "    for repo_folder in repo_folders:\n",
    "        repo_path = os.path.join(input_base_dir, repo_folder)\n",
    "        repo_output_dir = os.path.join(output_base_dir, repo_folder)\n",
    "        \n",
    "        # Create output repository folder if it doesn't exist\n",
    "        os.makedirs(repo_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all CSV files in the repository folder\n",
    "        csv_files = glob.glob(os.path.join(repo_path, \"*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {repo_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing repository: {repo_folder} - Found {len(csv_files)} CSV files\")\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            print(f\"  Processing file: {file_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Load and preprocess the data\n",
    "                df = load_and_preprocess_data(csv_file)\n",
    "                \n",
    "                # Aggregate features by project\n",
    "                project_df = aggregate_project_features(df)\n",
    "                \n",
    "                # Perform feature engineering\n",
    "                final_df = feature_engineering(project_df)\n",
    "                \n",
    "                # Save to output file\n",
    "                output_file = os.path.join(repo_output_dir, f\"project_level_{file_name}\")\n",
    "                final_df.to_csv(output_file, index=False)\n",
    "                \n",
    "                print(f\"  Saved project-level features to {output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "# Paths specific to the notebook environment\n",
    "INPUT_BASE_DIR = \"/Users/diegodias/Documents/Projects/JiraDataset/FeatureCleaning/jira_extracted_data\"\n",
    "OUTPUT_BASE_DIR = \"/Users/diegodias/Documents/Projects/JiraDataset/OverallProjectEstimation/project_level_data\"\n",
    "\n",
    "# If this script is run directly, execute the pipeline with the specified input and output paths\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting pipeline to process repository folders...\")\n",
    "    process_repository_folders(INPUT_BASE_DIR, OUTPUT_BASE_DIR)\n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./project_level_data/MongoDB/project_level_10000_Core_Server.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
