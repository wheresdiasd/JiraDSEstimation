# Resolution Hours Ensemble Model with Hyperparameter Tuning

## Performance Comparison

| Model | Metric | Original | Tuned | Improvement (%) |
|-------|--------|----------|-------|----------------|
| Random_Forest | MAE | 665788.6828 | 297.7082 | 99.96 |
| Random_Forest | RMSE | 974012.6795 | 6654.1795 | 99.32 |
| Random_Forest | R2 | 0.9580 | 1.0000 | 4.39 |
| Random_Forest | Spearman | 0.9307 | 0.9424 | 1.25 |
| Gradient_Boosting | MAE | 480758.5376 | 52279.3943 | 89.13 |
| Gradient_Boosting | RMSE | 774046.5627 | 100337.0628 | 87.04 |
| Gradient_Boosting | R2 | 0.9735 | 0.9996 | 2.68 |
| Gradient_Boosting | Spearman | 0.9329 | 0.9419 | 0.97 |
| XGBoost | MAE | 548091.4057 | 120530.7695 | 78.01 |
| XGBoost | RMSE | 864029.5784 | 226056.6121 | 73.84 |
| XGBoost | R2 | 0.9669 | 0.9977 | 3.19 |
| XGBoost | Spearman | 0.9304 | 0.9407 | 1.10 |
| Ensemble | MAE | 548743.0430 | 54669.8686 | 90.04 |
| Ensemble | RMSE | 838064.3333 | 103503.0087 | 87.65 |
| Ensemble | R2 | 0.9689 | 0.9995 | 3.16 |
| Ensemble | Spearman | 0.9326 | 0.9419 | 0.99 |

## Hyperparameter Changes

### Random_Forest

| Parameter | Original | Tuned |
|-----------|----------|-------|
| n_estimators | 100 | 50 |
| min_samples_leaf | 5 | 1 |
| max_features | sqrt | sqrt |
| max_depth | 10 | None |

### Gradient_Boosting

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 1.0 | 1.0 |
| n_estimators | 100 | 200 |
| min_samples_leaf | 5 | 3 |
| max_depth | 5 | 7 |
| learning_rate | 0.05 | 0.05 |

### XGBoost

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 0.8 | 1.0 |
| n_estimators | 100 | 200 |
| min_child_weight | 5 | 3 |
| max_depth | 5 | 5 |
| learning_rate | 0.05 | 0.1 |
| colsample_bytree | 0.8 | 0.8 |


## Conclusion

Hyperparameter tuning resulted in an average MAE improvement of 89.28% and an average RÂ² improvement of 3.35%.

Before tuning, Gradient_Boosting was the best performing model. After tuning, Random_Forest became the best performing model.
