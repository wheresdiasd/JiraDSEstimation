{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset and perform initial preprocessing\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    date_columns = ['fields.resolutiondate', 'fields.created', 'fields.updated']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    # Create basic derived features\n",
    "    if 'fields.created' in df.columns and 'fields.resolutiondate' in df.columns:\n",
    "        # Only calculate resolution time for resolved issues\n",
    "        resolved_mask = ~df['fields.resolutiondate'].isna()\n",
    "        df.loc[resolved_mask, 'resolution_time_hours'] = (\n",
    "            df.loc[resolved_mask, 'fields.resolutiondate'] - \n",
    "            df.loc[resolved_mask, 'fields.created']\n",
    "        ).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Extract day of week and hour of day for temporal analysis\n",
    "    if 'fields.created' in df.columns:\n",
    "        df['created_day_of_week'] = df['fields.created'].dt.dayofweek\n",
    "        df['created_hour'] = df['fields.created'].dt.hour\n",
    "        df['created_month'] = df['fields.created'].dt.month\n",
    "        df['created_year'] = df['fields.created'].dt.year\n",
    "    \n",
    "    if 'fields.resolutiondate' in df.columns:\n",
    "        df['resolved_day_of_week'] = df['fields.resolutiondate'].dt.dayofweek\n",
    "        df['resolved_hour'] = df['fields.resolutiondate'].dt.hour\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_creator_experience(issue_df):\n",
    "    \"\"\"\n",
    "    Calculate experience metrics for each creator based on historical data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    issue_df : pandas.DataFrame\n",
    "        DataFrame with issue-level data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with creator experience metrics\n",
    "    \"\"\"\n",
    "    if 'fields.creator.key' not in issue_df.columns and 'fields.creator.name' not in issue_df.columns:\n",
    "        # Try to find any creator identifier column\n",
    "        creator_cols = [col for col in issue_df.columns if 'creator' in col.lower()]\n",
    "        if not creator_cols:\n",
    "            print(\"No creator information found in dataset\")\n",
    "            return pd.DataFrame()\n",
    "        creator_col = creator_cols[0]\n",
    "    else:\n",
    "        creator_col = 'fields.creator.key' if 'fields.creator.key' in issue_df.columns else 'fields.creator.name'\n",
    "    \n",
    "    if 'fields.created' not in issue_df.columns:\n",
    "        print(\"No creation date information found in dataset\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Sort issues by creation date\n",
    "    sorted_issues = issue_df.sort_values('fields.created').copy()\n",
    "    \n",
    "    # Calculate experience for each creator at each issue creation point\n",
    "    creators = sorted_issues[creator_col].unique()\n",
    "    creator_exp = {creator: [] for creator in creators}\n",
    "    creator_first_issue = {creator: None for creator in creators}\n",
    "    creator_issue_count = {creator: 0 for creator in creators}\n",
    "    \n",
    "    # Track issue types per creator to calculate specialization\n",
    "    creator_issue_types = {creator: {} for creator in creators}\n",
    "    \n",
    "    for _, issue in sorted_issues.iterrows():\n",
    "        creator = issue[creator_col]\n",
    "        created_date = issue['fields.created']\n",
    "        issue_key = issue.get('key', str(issue.name))\n",
    "        issue_type = issue.get('issue_type', issue.get('fields.issuetype.name', 'unknown'))\n",
    "        \n",
    "        # Record first issue date\n",
    "        if creator_first_issue[creator] is None:\n",
    "            creator_first_issue[creator] = created_date\n",
    "        \n",
    "        # Track issue types for this creator\n",
    "        if issue_type in creator_issue_types[creator]:\n",
    "            creator_issue_types[creator][issue_type] += 1\n",
    "        else:\n",
    "            creator_issue_types[creator][issue_type] = 1\n",
    "        \n",
    "        # Calculate experience in days\n",
    "        creator_issue_count[creator] += 1\n",
    "        days_experience = (created_date - creator_first_issue[creator]).days\n",
    "        \n",
    "        # Calculate specialization (% of most common issue type)\n",
    "        type_counts = creator_issue_types[creator]\n",
    "        most_common_type_count = max(type_counts.values()) if type_counts else 0\n",
    "        specialization = most_common_type_count / creator_issue_count[creator] if creator_issue_count[creator] > 0 else 0\n",
    "        \n",
    "        # Store experience data\n",
    "        creator_exp[creator].append({\n",
    "            'issue_key': issue_key,\n",
    "            'created_date': created_date,\n",
    "            'days_experience': max(0, days_experience),\n",
    "            'issue_count': creator_issue_count[creator],\n",
    "            'specialization': specialization,\n",
    "            'issue_type': issue_type\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame with experience data\n",
    "    exp_data = []\n",
    "    for creator, issues in creator_exp.items():\n",
    "        for issue in issues:\n",
    "            exp_data.append({\n",
    "                'creator_key': creator,\n",
    "                'issue_key': issue['issue_key'],\n",
    "                'created_date': issue['created_date'],\n",
    "                'days_experience': issue['days_experience'],\n",
    "                'issue_count': issue['issue_count'],\n",
    "                'specialization': issue['specialization'],\n",
    "                'issue_type': issue['issue_type']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(exp_data)\n",
    "\n",
    "def entropy_calc(props):\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy for a series of proportions\n",
    "    \"\"\"\n",
    "    # Filter out zero values to avoid log(0)\n",
    "    if isinstance(props, pd.Series):\n",
    "        props = props[props > 0]\n",
    "    else:\n",
    "        props = [p for p in props if p > 0]\n",
    "    \n",
    "    if len(props) == 0:\n",
    "        return 0\n",
    "    \n",
    "    if isinstance(props, pd.Series):\n",
    "        return -sum(props * np.log2(props))\n",
    "    else:\n",
    "        return -sum(p * np.log2(p) for p in props)\n",
    "\n",
    "def gini_coefficient(values):\n",
    "    \"\"\"\n",
    "    Calculate Gini coefficient as a measure of inequality\n",
    "    \n",
    "    A value of 0 expresses perfect equality (everyone has the same)\n",
    "    A value of 1 expresses maximal inequality (one person has everything)\n",
    "    \"\"\"\n",
    "    if len(values) < 2 or sum(values) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Sort values\n",
    "    sorted_values = np.sort(values)\n",
    "    n = len(values)\n",
    "    cumsum = np.cumsum(sorted_values)\n",
    "    \n",
    "    # Calculate Gini coefficient\n",
    "    return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n if cumsum[-1] > 0 else 0\n",
    "\n",
    "def extract_team_features(df, project_id_col='fields.project.id'):\n",
    "    \"\"\"\n",
    "    Extract team-based features from issue-level data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing issue-level data\n",
    "    project_id_col : str\n",
    "        The column name that contains project IDs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with one row per project and team-based features\n",
    "    \"\"\"\n",
    "    # Identify creator column\n",
    "    if 'fields.creator.key' in df.columns:\n",
    "        creator_col = 'fields.creator.key'\n",
    "    elif 'fields.creator.name' in df.columns:\n",
    "        creator_col = 'fields.creator.name'\n",
    "    else:\n",
    "        # Try to find any creator identifier column\n",
    "        creator_cols = [col for col in df.columns if 'creator' in col.lower()]\n",
    "        if not creator_cols:\n",
    "            print(\"No creator information found in dataset\")\n",
    "            return pd.DataFrame()\n",
    "        creator_col = creator_cols[0]\n",
    "    \n",
    "    # Identify assignee column\n",
    "    if 'fields.assignee.key' in df.columns:\n",
    "        assignee_col = 'fields.assignee.key'\n",
    "    elif 'fields.assignee.name' in df.columns:\n",
    "        assignee_col = 'fields.assignee.name'\n",
    "    else:\n",
    "        # Try to find any assignee identifier column\n",
    "        assignee_cols = [col for col in df.columns if 'assignee' in col.lower()]\n",
    "        assignee_col = assignee_cols[0] if assignee_cols else None\n",
    "    \n",
    "    # Group data by project\n",
    "    project_groups = df.groupby(project_id_col)\n",
    "    team_features = {}\n",
    "    \n",
    "    for project_id, project_df in project_groups:\n",
    "        features = {}\n",
    "        \n",
    "        # Add project identifier\n",
    "        features['project_id'] = project_id\n",
    "        \n",
    "        # 1. Team Composition Metrics\n",
    "        # Team size (unique creators and assignees)\n",
    "        creators = project_df[creator_col].unique()\n",
    "        features['team_size_creators'] = len(creators)\n",
    "        \n",
    "        if assignee_col and assignee_col in project_df.columns:\n",
    "            # Remove NaN values\n",
    "            valid_assignees = project_df[project_df[assignee_col].notna()]\n",
    "            assignees = valid_assignees[assignee_col].unique() if len(valid_assignees) > 0 else []\n",
    "            features['team_size_assignees'] = len(assignees)\n",
    "            \n",
    "            # Combined team size (unique individuals who have either created or been assigned an issue)\n",
    "            combined_team = set(creators) | set(assignees)\n",
    "            features['team_size_combined'] = len(combined_team)\n",
    "        else:\n",
    "            features['team_size_assignees'] = 0\n",
    "            features['team_size_combined'] = features['team_size_creators']\n",
    "        \n",
    "        # Core team ratio (what % of issues are created by the top 20% of creators)\n",
    "        creator_counts = project_df[creator_col].value_counts()\n",
    "        if len(creator_counts) > 0:\n",
    "            top_creator_count = max(1, int(len(creator_counts) * 0.2))  # at least 1\n",
    "            top_creators = creator_counts.nlargest(top_creator_count)\n",
    "            features['core_team_ratio'] = top_creators.sum() / len(project_df) if len(project_df) > 0 else 0\n",
    "            \n",
    "            # Creator workload distribution - Gini coefficient\n",
    "            features['creator_workload_gini'] = gini_coefficient(creator_counts.values)\n",
    "            \n",
    "            # Creator diversity (entropy)\n",
    "            creator_props = creator_counts / creator_counts.sum()\n",
    "            features['creator_diversity'] = entropy_calc(creator_props)\n",
    "        \n",
    "        # 2. Developer Activity Metrics\n",
    "        # Average issues per creator\n",
    "        features['avg_issues_per_creator'] = len(project_df) / features['team_size_creators'] if features['team_size_creators'] > 0 else 0\n",
    "        \n",
    "        # Top creator contribution\n",
    "        features['top_creator_contribution'] = creator_counts.max() / len(project_df) if len(project_df) > 0 and len(creator_counts) > 0 else 0\n",
    "        \n",
    "        # Creator activity variance\n",
    "        if len(creator_counts) > 1:\n",
    "            features['creator_activity_variance'] = creator_counts.var()\n",
    "            features['creator_activity_std'] = creator_counts.std()\n",
    "        \n",
    "        # 3. Team Focus by Issue Type\n",
    "        if 'issue_type' in project_df.columns or 'fields.issuetype.name' in project_df.columns:\n",
    "            issue_type_col = 'issue_type' if 'issue_type' in project_df.columns else 'fields.issuetype.name'\n",
    "            \n",
    "            # Calculate type specialization index\n",
    "            if len(project_df[issue_type_col].unique()) > 1 and len(creators) > 0:\n",
    "                try:\n",
    "                    creator_type_matrix = pd.crosstab(\n",
    "                        project_df[creator_col], \n",
    "                        project_df[issue_type_col],\n",
    "                        normalize='index'\n",
    "                    )\n",
    "                    \n",
    "                    # Average entropy of issue types per creator\n",
    "                    type_entropies = creator_type_matrix.apply(entropy_calc, axis=1)\n",
    "                    max_entropy = np.log2(creator_type_matrix.shape[1]) if creator_type_matrix.shape[1] > 0 else 1\n",
    "                    \n",
    "                    features['team_type_specialization_index'] = 1 - (type_entropies.mean() / max_entropy) if max_entropy > 0 else 0\n",
    "                except:\n",
    "                    features['team_type_specialization_index'] = 0\n",
    "            \n",
    "            # Bug ratio\n",
    "            bug_issues = project_df[project_df[issue_type_col].str.contains('Bug', case=False, na=False)]\n",
    "            features['bug_creation_ratio'] = len(bug_issues) / len(project_df) if len(project_df) > 0 else 0\n",
    "            \n",
    "            # Feature request ratio\n",
    "            feature_issues = project_df[project_df[issue_type_col].str.contains('Feature|Enhancement|Improvement', case=False, na=False)]\n",
    "            features['feature_request_ratio'] = len(feature_issues) / len(project_df) if len(project_df) > 0 else 0\n",
    "            \n",
    "            # Bug creator concentration\n",
    "            if len(bug_issues) > 0:\n",
    "                bug_creators = bug_issues[creator_col].value_counts()\n",
    "                features['bug_creator_concentration'] = gini_coefficient(bug_creators.values)\n",
    "                \n",
    "                # Bug/Developer Ratio\n",
    "                features['bug_developer_ratio'] = len(bug_issues) / features['team_size_creators'] if features['team_size_creators'] > 0 else 0\n",
    "            \n",
    "            # Feature/Developer Ratio\n",
    "            if len(feature_issues) > 0:\n",
    "                features['feature_developer_ratio'] = len(feature_issues) / features['team_size_creators'] if features['team_size_creators'] > 0 else 0\n",
    "        \n",
    "        # 4. Weekend Activity Patterns\n",
    "        if 'created_day_of_week' in project_df.columns:\n",
    "            weekend_issues = project_df[project_df['created_day_of_week'].isin([5, 6])]  # 5=Saturday, 6=Sunday\n",
    "            weekday_issues = project_df[~project_df['created_day_of_week'].isin([5, 6])]\n",
    "            \n",
    "            if len(weekday_issues) > 0:\n",
    "                # Weekend activity ratio (normalized by the fact weekends are 2/7 of days)\n",
    "                features['weekend_activity_ratio'] = (\n",
    "                    len(weekend_issues) / (len(weekday_issues) * 2/5)\n",
    "                ) if len(weekday_issues) > 0 else 0\n",
    "        \n",
    "        # 5. Team Temporal Patterns\n",
    "        if 'fields.created' in project_df.columns:\n",
    "            # Group by month and year\n",
    "            project_df['created_yearmonth'] = project_df['fields.created'].dt.to_period('M')\n",
    "            monthly_counts = project_df.groupby('created_yearmonth').size()\n",
    "            \n",
    "            if len(monthly_counts) > 1:\n",
    "                # Creation rate stability\n",
    "                features['creation_rate_stability'] = 1 - (monthly_counts.std() / monthly_counts.mean()) if monthly_counts.mean() > 0 else 0\n",
    "                \n",
    "                # Calculate creator onboarding rate (new creators per month)\n",
    "                first_creation_by_creator = project_df.groupby(creator_col)['fields.created'].min()\n",
    "                first_creation_by_creator = pd.DataFrame({creator_col: first_creation_by_creator.index, 'first_creation': first_creation_by_creator.values})\n",
    "                first_creation_by_creator['yearmonth'] = first_creation_by_creator['first_creation'].dt.to_period('M')\n",
    "                new_creators_per_month = first_creation_by_creator.groupby('yearmonth').size()\n",
    "                \n",
    "                features['avg_new_creators_per_month'] = new_creators_per_month.mean()\n",
    "                features['creator_onboarding_volatility'] = new_creators_per_month.std() / new_creators_per_month.mean() if new_creators_per_month.mean() > 0 else 0\n",
    "        \n",
    "        # 6. Team Issue Complexity Handling\n",
    "        if 'inward_count' in project_df.columns and 'outward_count' in project_df.columns:\n",
    "            # Link density per creator\n",
    "            creator_links = project_df.groupby(creator_col)['inward_count'].sum() + project_df.groupby(creator_col)['outward_count'].sum()\n",
    "            creator_issues = project_df[creator_col].value_counts()\n",
    "            \n",
    "            # Links per issue per creator\n",
    "            creator_link_density = creator_links / creator_issues\n",
    "            features['creator_link_density_mean'] = creator_link_density.mean()\n",
    "            features['creator_link_density_std'] = creator_link_density.std()\n",
    "            \n",
    "            # Complex issue distribution\n",
    "            link_threshold = project_df['inward_count'].quantile(0.75)\n",
    "            complex_issues = project_df[project_df['inward_count'] > link_threshold]\n",
    "            \n",
    "            if len(complex_issues) > 0:\n",
    "                complex_by_creator = complex_issues[creator_col].value_counts()\n",
    "                features['complex_issue_distribution'] = gini_coefficient(complex_by_creator.values)\n",
    "                \n",
    "                # Team complexity handling capacity\n",
    "                features['team_complexity_capacity'] = features['team_size_creators'] / complex_issues['inward_count'].mean() if complex_issues['inward_count'].mean() > 0 else 0\n",
    "        \n",
    "        # 7. Creation Time Patterns\n",
    "        if 'created_hour' in project_df.columns:\n",
    "            # Work hour ratio (9am-5pm considered standard)\n",
    "            work_hours = project_df[(project_df['created_hour'] >= 9) & (project_df['created_hour'] < 17)]\n",
    "            features['work_hour_creation_ratio'] = len(work_hours) / len(project_df) if len(project_df) > 0 else 0\n",
    "            \n",
    "            # Hour of day entropy (how distributed are creation times)\n",
    "            hour_counts = project_df['created_hour'].value_counts(normalize=True)\n",
    "            features['creation_hour_entropy'] = entropy_calc(hour_counts)\n",
    "        \n",
    "        # 8. Resolution Timing Features\n",
    "        if 'fields.created' in project_df.columns and 'fields.resolutiondate' in project_df.columns:\n",
    "            resolved_issues = project_df[~project_df['fields.resolutiondate'].isna()]\n",
    "            \n",
    "            if len(resolved_issues) > 0:\n",
    "                # Resolution time by creator\n",
    "                if 'resolution_time_hours' in resolved_issues.columns:\n",
    "                    creator_res_times = resolved_issues.groupby(creator_col)['resolution_time_hours'].mean()\n",
    "                    features['creator_resolution_time_variability'] = creator_res_times.std() / creator_res_times.mean() if creator_res_times.mean() > 0 else 0\n",
    "                    \n",
    "                    # Resolution time predictability\n",
    "                    features['team_resolution_predictability'] = 1 - (resolved_issues['resolution_time_hours'].std() / resolved_issues['resolution_time_hours'].mean() if resolved_issues['resolution_time_hours'].mean() > 0 else 0)\n",
    "        \n",
    "        # Add features to the team_features dictionary\n",
    "        team_features[project_id] = features\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame.from_dict(team_features, orient='index')\n",
    "    \n",
    "    # Fill NaN values with appropriate defaults\n",
    "    result_df = result_df.fillna(0)  # Replace all NaN with 0 for simplicity\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def aggregate_project_features(df, project_id_col='fields.project.id'):\n",
    "    \"\"\"\n",
    "    Aggregate features at the project level to create a single row per project\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing issue-level data\n",
    "    project_id_col : str\n",
    "        The column name that contains project IDs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with one row per project and aggregated features\n",
    "    \"\"\"\n",
    "    project_features = {}\n",
    "    \n",
    "    # Group data by project\n",
    "    project_groups = df.groupby(project_id_col)\n",
    "    \n",
    "    for project_id, project_df in project_groups:\n",
    "        features = {}\n",
    "        \n",
    "        # Add project identifier\n",
    "        features['project_id'] = project_id\n",
    "        features['project_key'] = project_df['fields.project.key'].iloc[0] if 'fields.project.key' in project_df.columns else None\n",
    "        features['project_name'] = project_df['fields.project.name'].iloc[0] if 'fields.project.name' in project_df.columns else None\n",
    "        \n",
    "        # Basic issue counts\n",
    "        features['total_issues'] = len(project_df)\n",
    "        \n",
    "        # 1. Temporal Features\n",
    "        # Overall project timespan\n",
    "        if 'fields.created' in project_df.columns and 'fields.resolutiondate' in project_df.columns:\n",
    "            features['project_start_date'] = project_df['fields.created'].min()\n",
    "            \n",
    "            # For end date, use the latest resolved issue or latest update if no resolutions\n",
    "            resolved_issues = project_df[~project_df['fields.resolutiondate'].isna()]\n",
    "            if len(resolved_issues) > 0:\n",
    "                features['project_latest_resolved_date'] = resolved_issues['fields.resolutiondate'].max()\n",
    "            else:\n",
    "                features['project_latest_resolved_date'] = None\n",
    "                \n",
    "            if 'fields.updated' in project_df.columns:\n",
    "                features['project_latest_update_date'] = project_df['fields.updated'].max()\n",
    "            \n",
    "            # Calculate project duration in days (using latest of either resolution or update)\n",
    "            if features['project_latest_resolved_date'] is not None:\n",
    "                latest_date = max(\n",
    "                    features['project_latest_resolved_date'],\n",
    "                    features.get('project_latest_update_date', features['project_latest_resolved_date'])\n",
    "                )\n",
    "                features['project_duration_days'] = (latest_date - features['project_start_date']).days\n",
    "            elif 'project_latest_update_date' in features:\n",
    "                features['project_duration_days'] = (features['project_latest_update_date'] - features['project_start_date']).days\n",
    "            else:\n",
    "                features['project_duration_days'] = None\n",
    "        \n",
    "        # Resolution time statistics for resolved issues\n",
    "        resolved_issues = project_df[~project_df['fields.resolutiondate'].isna()]\n",
    "        if 'resolution_time_hours' in project_df.columns and len(resolved_issues) > 0:\n",
    "            resolution_times = resolved_issues['resolution_time_hours']\n",
    "            \n",
    "            features['avg_resolution_hours'] = resolution_times.mean()\n",
    "            features['median_resolution_hours'] = resolution_times.median()\n",
    "            features['min_resolution_hours'] = resolution_times.min()\n",
    "            features['max_resolution_hours'] = resolution_times.max()\n",
    "            features['resolution_hours_std'] = resolution_times.std()\n",
    "            features['total_resolution_hours'] = resolution_times.sum()\n",
    "            \n",
    "            # Distribution metrics for resolution times\n",
    "            if len(resolution_times) > 2:  # Need at least 3 points for skewness/kurtosis\n",
    "                features['resolution_time_skewness'] = stats.skew(resolution_times)\n",
    "                features['resolution_time_kurtosis'] = stats.kurtosis(resolution_times)\n",
    "            \n",
    "            # Resolution time percentiles\n",
    "            features['resolution_time_p25'] = resolution_times.quantile(0.25)\n",
    "            features['resolution_time_p75'] = resolution_times.quantile(0.75)\n",
    "            features['resolution_time_p90'] = resolution_times.quantile(0.90)\n",
    "            features['resolution_time_iqr'] = features['resolution_time_p75'] - features['resolution_time_p25']\n",
    "            \n",
    "            # Proportion of issues resolved in different time frames\n",
    "            features['pct_resolved_within_24h'] = (resolution_times <= 24).mean() * 100\n",
    "            features['pct_resolved_within_week'] = (resolution_times <= 168).mean() * 100  # 168 hours = 1 week\n",
    "            features['pct_resolved_within_month'] = (resolution_times <= 720).mean() * 100  # 720 hours ≈ 30 days\n",
    "        \n",
    "        # Temporal patterns\n",
    "        if 'fields.created' in project_df.columns:\n",
    "            # Weekend vs. weekday metrics\n",
    "            if 'created_day_of_week' in project_df.columns:\n",
    "                weekend_created = project_df['created_day_of_week'].isin([5, 6])  # 5=Saturday, 6=Sunday\n",
    "                features['pct_issues_created_on_weekend'] = weekend_created.mean() * 100\n",
    "            \n",
    "            if 'resolved_day_of_week' in project_df.columns:\n",
    "                weekend_resolved = project_df['resolved_day_of_week'].isin([5, 6])\n",
    "                resolved_issues_count = (~project_df['fields.resolutiondate'].isna()).sum()\n",
    "                if resolved_issues_count > 0:\n",
    "                    features['pct_issues_resolved_on_weekend'] = (\n",
    "                        weekend_resolved & ~project_df['fields.resolutiondate'].isna()\n",
    "                    ).sum() / resolved_issues_count * 100\n",
    "            \n",
    "            # Creation patterns by month\n",
    "            if 'created_month' in project_df.columns and 'created_year' in project_df.columns:\n",
    "                # Group issues by year-month and count\n",
    "                monthly_counts = project_df.groupby(['created_year', 'created_month']).size()\n",
    "                if len(monthly_counts) > 0:\n",
    "                    features['max_issues_per_month'] = monthly_counts.max()\n",
    "                    features['avg_issues_per_month'] = monthly_counts.mean()\n",
    "                    features['months_with_activity'] = len(monthly_counts)\n",
    "                    if features['months_with_activity'] > 1:\n",
    "                        features['issue_creation_volatility'] = monthly_counts.std() / monthly_counts.mean()\n",
    "        \n",
    "        # 2. Priority and Issue Type Features\n",
    "        # Count issues by priority\n",
    "        if 'priority_name' in project_df.columns:\n",
    "            priority_counts = project_df['priority_name'].value_counts()\n",
    "            total_with_priority = priority_counts.sum()\n",
    "            \n",
    "            for priority in priority_counts.index:\n",
    "                col_name = f'priority_{priority.lower().replace(\" \", \"_\")}_count'\n",
    "                features[col_name] = priority_counts[priority]\n",
    "                \n",
    "                # Also add as percentage\n",
    "                col_pct_name = f'priority_{priority.lower().replace(\" \", \"_\")}_pct'\n",
    "                features[col_pct_name] = (priority_counts[priority] / total_with_priority * 100) if total_with_priority > 0 else 0\n",
    "        \n",
    "        # Alternative approach for priority using binary columns if they exist\n",
    "        priority_cols = [col for col in project_df.columns if col.startswith('priority_') and col != 'priority_name' and col != 'priority_id']\n",
    "        if priority_cols:\n",
    "            for col in priority_cols:\n",
    "                features[f'{col}_count'] = project_df[col].sum()\n",
    "                features[f'{col}_pct'] = (project_df[col].sum() / len(project_df) * 100)\n",
    "        \n",
    "        # Count issues by type\n",
    "        if 'issue_type' in project_df.columns:\n",
    "            type_counts = project_df['issue_type'].value_counts()\n",
    "            \n",
    "            for issue_type in type_counts.index:\n",
    "                col_name = f'type_{issue_type.lower().replace(\" \", \"_\")}_count'\n",
    "                features[col_name] = type_counts[issue_type]\n",
    "                \n",
    "                # Also add as percentage\n",
    "                col_pct_name = f'type_{issue_type.lower().replace(\" \", \"_\")}_pct'\n",
    "                features[col_pct_name] = (type_counts[issue_type] / len(project_df) * 100)\n",
    "        \n",
    "        # Alternative approach for issue types using binary columns if they exist\n",
    "        type_cols = [col for col in project_df.columns if col.startswith('type_')]\n",
    "        if type_cols:\n",
    "            for col in type_cols:\n",
    "                features[f'{col}_count'] = project_df[col].sum()\n",
    "                features[f'{col}_pct'] = (project_df[col].sum() / len(project_df) * 100)\n",
    "        \n",
    "        # Priority and issue type combinations\n",
    "        # For each priority, calculate resolution metrics by issue type\n",
    "        if 'priority_name' in project_df.columns and 'issue_type' in project_df.columns and 'resolution_time_hours' in project_df.columns:\n",
    "            for priority in project_df['priority_name'].unique():\n",
    "                for issue_type in project_df['issue_type'].unique():\n",
    "                    # Filter issues with this priority and type that have been resolved\n",
    "                    filtered = project_df[\n",
    "                        (project_df['priority_name'] == priority) & \n",
    "                        (project_df['issue_type'] == issue_type) &\n",
    "                        ~project_df['fields.resolutiondate'].isna()\n",
    "                    ]\n",
    "                    \n",
    "                    if len(filtered) > 0:\n",
    "                        prefix = f'priority_{priority.lower().replace(\" \", \"_\")}_type_{issue_type.lower().replace(\" \", \"_\")}'\n",
    "                        features[f'{prefix}_count'] = len(filtered)\n",
    "                        features[f'{prefix}_avg_resolution_hours'] = filtered['resolution_time_hours'].mean()\n",
    "        \n",
    "        # 3. Issue Dependencies and Complexity\n",
    "        if 'inward_count' in project_df.columns and 'outward_count' in project_df.columns:\n",
    "            # Average link counts\n",
    "            features['avg_inward_links'] = project_df['inward_count'].mean()\n",
    "            features['avg_outward_links'] = project_df['outward_count'].mean()\n",
    "            features['avg_total_links'] = project_df['inward_count'].add(project_df['outward_count']).mean()\n",
    "            \n",
    "            # Total link counts for the project\n",
    "            features['total_inward_links'] = project_df['inward_count'].sum()\n",
    "            features['total_outward_links'] = project_df['outward_count'].sum()\n",
    "            features['total_links'] = features['total_inward_links'] + features['total_outward_links']\n",
    "            \n",
    "            # Issues with many dependencies\n",
    "            high_dependency_threshold = project_df['inward_count'].quantile(0.75)\n",
    "            features['pct_issues_with_high_dependencies'] = (\n",
    "                (project_df['inward_count'] > high_dependency_threshold).mean() * 100\n",
    "            )\n",
    "            \n",
    "            # Link density (average links per issue)\n",
    "            features['link_density'] = features['total_links'] / features['total_issues'] if features['total_issues'] > 0 else 0\n",
    "        \n",
    "        # 4. Resolution Efficiency\n",
    "        if 'is_resolved' in project_df.columns:\n",
    "            features['num_resolved_issues'] = project_df['is_resolved'].sum()\n",
    "            features['pct_resolved_issues'] = features['num_resolved_issues'] / features['total_issues'] * 100 if features['total_issues'] > 0 else 0\n",
    "        \n",
    "        # Resolution rate over time\n",
    "        if 'fields.created' in project_df.columns and 'fields.resolutiondate' in project_df.columns and features.get('project_duration_days', 0) > 0:\n",
    "            features['resolution_rate_per_day'] = features.get('num_resolved_issues', 0) / features['project_duration_days']\n",
    "        \n",
    "        # Resolution efficiency by issue type\n",
    "        if 'issue_type' in project_df.columns and 'is_resolved' in project_df.columns:\n",
    "            for issue_type in project_df['issue_type'].unique():\n",
    "                filtered = project_df[project_df['issue_type'] == issue_type]\n",
    "                if len(filtered) > 0:\n",
    "                    type_key = issue_type.lower().replace(\" \", \"_\")\n",
    "                    features[f'type_{type_key}_resolution_rate'] = filtered['is_resolved'].mean() * 100\n",
    "        \n",
    "        # Add features to the project_features dictionary\n",
    "        project_features[project_id] = features\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame.from_dict(project_features, orient='index')\n",
    "    \n",
    "    # Fill NaN values with appropriate defaults or remove them\n",
    "    result_df = result_df.fillna({\n",
    "        'pct_resolved_issues': 0,\n",
    "        'resolution_rate_per_day': 0,\n",
    "        # Add other fields as needed\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def feature_engineering(project_df):\n",
    "    \"\"\"\n",
    "    Perform additional feature engineering on the aggregated project data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project_df : pandas.DataFrame\n",
    "        DataFrame with one row per project\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df = project_df.copy()\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    \n",
    "    # 1. Efficiency ratio = completed issues / total issues / project duration in weeks\n",
    "    if 'num_resolved_issues' in df.columns and 'project_duration_days' in df.columns and 'total_issues' in df.columns:\n",
    "        # Avoid division by zero\n",
    "        denominator = df['project_duration_days'] * df['total_issues']\n",
    "        df['weekly_efficiency_ratio'] = np.where(\n",
    "            denominator > 0,\n",
    "            df['num_resolved_issues'] * 7 / denominator,\n",
    "            0  # Default value when denominator is 0\n",
    "        )\n",
    "    \n",
    "    # 2. Complexity-weighted resolution time\n",
    "    if 'avg_resolution_hours' in df.columns and 'avg_total_links' in df.columns:\n",
    "        # Add a small constant to ensure we don't multiply by zero\n",
    "        df['complexity_weighted_resolution_time'] = df['avg_resolution_hours'] * (df['avg_total_links'] + 1)\n",
    "    \n",
    "    # 3. Priority balance - ratio of high priority to low priority issues\n",
    "    high_priority_cols = [col for col in df.columns if ('priority_critical' in col or 'priority_blocker' in col or 'priority_major' in col) and '_count' in col]\n",
    "    low_priority_cols = [col for col in df.columns if ('priority_minor' in col or 'priority_trivial' in col) and '_count' in col]\n",
    "    \n",
    "    if high_priority_cols and low_priority_cols:\n",
    "        high_priority_sum = df[high_priority_cols].sum(axis=1)\n",
    "        low_priority_sum = df[low_priority_cols].sum(axis=1)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        df['high_to_low_priority_ratio'] = np.where(\n",
    "            low_priority_sum > 0,\n",
    "            high_priority_sum / low_priority_sum,\n",
    "            high_priority_sum  # If no low priority issues, just use the high priority count\n",
    "        )\n",
    "    \n",
    "    # 4. Bug ratio - proportion of bugs to total issues\n",
    "    bug_cols = [col for col in df.columns if 'type_bug' in col and '_count' in col]\n",
    "    if bug_cols and 'total_issues' in df.columns:\n",
    "        df['bug_ratio'] = df[bug_cols].sum(axis=1) / df['total_issues']\n",
    "    \n",
    "    # 5. Creation-resolution balance - how evenly distributed is the workload\n",
    "    if 'avg_issues_per_month' in df.columns and 'resolution_rate_per_day' in df.columns:\n",
    "        monthly_creation_rate = df['avg_issues_per_month'] / 30  # Convert to daily rate\n",
    "        # Balance = 1 means perfect balance, < 1 means resolution is slower than creation\n",
    "        df['creation_resolution_balance'] = np.where(\n",
    "            monthly_creation_rate > 0,\n",
    "            df['resolution_rate_per_day'] / monthly_creation_rate,\n",
    "            0  # Default value when monthly_creation_rate is 0\n",
    "        )\n",
    "    \n",
    "    # 6. Weighted priority score\n",
    "    priority_weight_cols = {\n",
    "        'priority_blocker': 5, \n",
    "        'priority_critical': 4, \n",
    "        'priority_major': 3, \n",
    "        'priority_minor': 2, \n",
    "        'priority_trivial': 1\n",
    "    }\n",
    "    \n",
    "    priority_count_cols = [col for col in df.columns if any(p in col for p in priority_weight_cols.keys()) and '_count' in col]\n",
    "    \n",
    "    if priority_count_cols and 'total_issues' in df.columns:\n",
    "        weighted_sum = 0\n",
    "        for col in priority_count_cols:\n",
    "            # Extract the priority name from the column name\n",
    "            for priority_name, weight in priority_weight_cols.items():\n",
    "                if priority_name in col:\n",
    "                    weighted_sum += df[col] * weight\n",
    "                    break\n",
    "        \n",
    "        df['weighted_priority_score'] = weighted_sum / df['total_issues']\n",
    "    \n",
    "    # 7. Issue diversity - entropy of issue type distribution\n",
    "    type_pct_cols = [col for col in df.columns if col.startswith('type_') and '_pct' in col]\n",
    "    if type_pct_cols:\n",
    "        # Convert percentages to proportions\n",
    "        proportions = df[type_pct_cols].div(100)\n",
    "        \n",
    "        # Calculate entropy for each row (project)\n",
    "        def entropy(row):\n",
    "            # Filter out zero values to avoid log(0)\n",
    "            props = row[row > 0]\n",
    "            if len(props) == 0:\n",
    "                return 0\n",
    "            return -sum(props * np.log2(props))\n",
    "        \n",
    "        df['issue_type_entropy'] = proportions.apply(entropy, axis=1)\n",
    "    \n",
    "    # 8. Project velocity over time (if there's enough temporal data)\n",
    "    if 'num_resolved_issues' in df.columns and 'months_with_activity' in df.columns and df['months_with_activity'].max() > 1:\n",
    "        df['monthly_velocity'] = df['num_resolved_issues'] / df['months_with_activity']\n",
    "    \n",
    "    # 9. Team efficiency factors\n",
    "    if 'team_size_creators' in df.columns:\n",
    "        # Team size to issue ratio\n",
    "        if 'total_issues' in df.columns:\n",
    "            df['team_size_to_issue_ratio'] = df['team_size_creators'] / df['total_issues']\n",
    "        \n",
    "        # Team experience utilization\n",
    "        if 'creator_workload_gini' in df.columns:\n",
    "            df['team_experience_utilization'] = 1 - df['creator_workload_gini']\n",
    "        \n",
    "        # Team complexity handling efficiency\n",
    "        if 'team_complexity_capacity' in df.columns and 'link_density' in df.columns:\n",
    "            df['team_complexity_efficiency'] = df['team_complexity_capacity'] / (df['link_density'] + 1)\n",
    "    \n",
    "    # 10. Team specialization effectiveness\n",
    "    if 'team_type_specialization_index' in df.columns and 'issue_type_entropy' in df.columns:\n",
    "        # How well the team specialization matches the diversity of issue types\n",
    "        df['team_specialization_effectiveness'] = 1 - abs(df['team_type_specialization_index'] - df['issue_type_entropy'])\n",
    "    \n",
    "    # 11. Team time management \n",
    "    if 'weekend_activity_ratio' in df.columns:\n",
    "        # Lower ratio means better work-life balance\n",
    "        df['team_work_balance_index'] = 1 / (1 + df['weekend_activity_ratio'])\n",
    "    \n",
    "    # 12. Team growth sustainability\n",
    "    if 'avg_new_creators_per_month' in df.columns and 'team_size_creators' in df.columns and 'months_with_activity' in df.columns:\n",
    "        # Measure of how sustainable the team growth is\n",
    "        df['team_growth_sustainability'] = np.where(\n",
    "            df['months_with_activity'] > 0,\n",
    "            df['avg_new_creators_per_month'] / (df['team_size_creators'] / df['months_with_activity']),\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combine_with_team_features(project_df, issue_df, project_id_col='fields.project.id'):\n",
    "    \"\"\"\n",
    "    Extract team features and combine with project features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    project_df : pandas.DataFrame\n",
    "        DataFrame with project-level features\n",
    "    issue_df : pandas.DataFrame\n",
    "        DataFrame with issue-level data\n",
    "    project_id_col : str\n",
    "        The column name that contains project IDs\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with combined project and team features\n",
    "    \"\"\"\n",
    "    # Extract team features\n",
    "    team_df = extract_team_features(issue_df, project_id_col)\n",
    "    \n",
    "    # Calculate experience metrics\n",
    "    exp_df = calculate_creator_experience(issue_df)\n",
    "    \n",
    "    # If experience data is available, aggregate it to project level\n",
    "    if not exp_df.empty:\n",
    "        # Group by project and calculate average experience metrics\n",
    "        project_exp = {}\n",
    "        \n",
    "        # Group issues by project\n",
    "        issues_by_project = issue_df.groupby(project_id_col)\n",
    "        \n",
    "        for project_id, project_issues in issues_by_project:\n",
    "            # Find experience data for this project's issues\n",
    "            project_keys = project_issues.get('key', project_issues.index).tolist()\n",
    "            project_exp_data = exp_df[exp_df['issue_key'].isin(project_keys)]\n",
    "            \n",
    "            if not project_exp_data.empty:\n",
    "                project_exp[project_id] = {\n",
    "                    'project_id': project_id,\n",
    "                    'avg_creator_experience_days': project_exp_data['days_experience'].mean(),\n",
    "                    'avg_creator_issue_count': project_exp_data['issue_count'].mean(),\n",
    "                    'avg_creator_specialization': project_exp_data['specialization'].mean()\n",
    "                }\n",
    "        \n",
    "        # Convert to DataFrame and merge with team features\n",
    "        if project_exp:\n",
    "            exp_project_df = pd.DataFrame.from_dict(project_exp, orient='index')\n",
    "            team_df = team_df.merge(exp_project_df, on='project_id', how='left')\n",
    "    \n",
    "    # Combine with project features\n",
    "    if 'project_id' in project_df.columns:\n",
    "        combined_df = project_df.merge(team_df, on='project_id', how='left')\n",
    "    else:\n",
    "        # If project_id is index in project_df\n",
    "        if isinstance(project_df.index, pd.Index) and project_df.index.name != 'project_id':\n",
    "            project_df = project_df.reset_index().rename(columns={'index': 'project_id'})\n",
    "        \n",
    "        combined_df = project_df.merge(team_df, on='project_id', how='left')\n",
    "    \n",
    "    # Fill NaN values for team features\n",
    "    team_cols = team_df.columns.difference(['project_id'])\n",
    "    combined_df[team_cols] = combined_df[team_cols].fillna(0)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def process_repository_folders(input_base_dir, output_base_dir):\n",
    "    \"\"\"\n",
    "    Process all repositories in the input base directory and save the results to the output base directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_base_dir : str\n",
    "        Base directory containing repository folders with issue data\n",
    "    output_base_dir : str\n",
    "        Base directory to save processed project-level data\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Get a list of all repository folders\n",
    "    repo_folders = [f for f in os.listdir(input_base_dir) if os.path.isdir(os.path.join(input_base_dir, f))]\n",
    "    \n",
    "    print(f\"Found {len(repo_folders)} repository folders in {input_base_dir}\")\n",
    "    \n",
    "    # Process each repository folder\n",
    "    for repo_folder in repo_folders:\n",
    "        repo_path = os.path.join(input_base_dir, repo_folder)\n",
    "        repo_output_dir = os.path.join(output_base_dir, repo_folder)\n",
    "        \n",
    "        # Create output repository folder if it doesn't exist\n",
    "        os.makedirs(repo_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Find all CSV files in the repository folder\n",
    "        csv_files = glob.glob(os.path.join(repo_path, \"*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No CSV files found in {repo_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing repository: {repo_folder} - Found {len(csv_files)} CSV files\")\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            print(f\"  Processing file: {file_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Load and preprocess the data\n",
    "                issue_df = load_and_preprocess_data(csv_file)\n",
    "                \n",
    "                # Save creator experience data\n",
    "                exp_df = calculate_creator_experience(issue_df)\n",
    "                if not exp_df.empty:\n",
    "                    exp_output_file = os.path.join(repo_output_dir, f\"creator_experience_{file_name}\")\n",
    "                    exp_df.to_csv(exp_output_file, index=False)\n",
    "                    print(f\"  Saved creator experience data to {exp_output_file}\")\n",
    "                \n",
    "                # Aggregate features by project\n",
    "                project_df = aggregate_project_features(issue_df)\n",
    "                \n",
    "                # Perform feature engineering\n",
    "                project_df = feature_engineering(project_df)\n",
    "                \n",
    "                # Combine with team features\n",
    "                final_df = combine_with_team_features(project_df, issue_df)\n",
    "                \n",
    "                # Save to output file\n",
    "                output_file = os.path.join(repo_output_dir, f\"project_level_{file_name}\")\n",
    "                final_df.to_csv(output_file, index=False)\n",
    "                \n",
    "                print(f\"  Saved project-level features with team metrics to {output_file}\")\n",
    "                \n",
    "                # Also save processed issue-level data for later use in task-level estimation\n",
    "                issue_output_file = os.path.join(repo_output_dir, f\"processed_{file_name}\")\n",
    "                issue_df.to_csv(issue_output_file, index=False)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {file_name}: {str(e)}\")\n",
    "\n",
    "# Paths specific to the environment\n",
    "INPUT_BASE_DIR = \"../FeatureCleaning/jira_extracted_data\"\n",
    "OUTPUT_BASE_DIR = \"./project_level_data\"\n",
    "\n",
    "# If this script is run directly, execute the pipeline with the specified input and output paths\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting pipeline to process repository folders with enhanced team metrics...\")\n",
    "    process_repository_folders(INPUT_BASE_DIR, OUTPUT_BASE_DIR)\n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./project_level_data/combined/combined_projects_raw.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
