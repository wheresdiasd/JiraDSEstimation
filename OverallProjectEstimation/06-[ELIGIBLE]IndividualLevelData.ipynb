{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"../FeatureCleaning/jira_extracted_data/Hyperledger/10001_Sawtooth.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "base_folder = \"../FeatureCleaning/jira_extracted_data\"  # Change this to your base folder containing all repositories\n",
    "output_file = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "\n",
    "# Collect all task data into a single dataframe\n",
    "all_tasks = []\n",
    "\n",
    "# Traverse through all repository folders\n",
    "for repo_folder in glob.glob(os.path.join(base_folder, \"*\")):\n",
    "    if os.path.isdir(repo_folder):\n",
    "        repo_name = os.path.basename(repo_folder)\n",
    "        \n",
    "        # Find all CSV files in this repository\n",
    "        csv_files = glob.glob(os.path.join(repo_folder, \"*.csv\"))\n",
    "        \n",
    "        print(f\"Processing repository: {repo_name} ({len(csv_files)} files)\")\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            print(f\"  Reading file: {file_name}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Add repository and file information\n",
    "                df['repository'] = repo_name\n",
    "                df['source_file'] = file_name\n",
    "                \n",
    "                # Append to our collection\n",
    "                all_tasks.append(df)\n",
    "                print(f\"    Added {len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {file_name}: {e}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_tasks:\n",
    "    combined_df = pd.concat(all_tasks, ignore_index=True)\n",
    "    \n",
    "    # Save the consolidated file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSuccessfully created consolidated task data file:\")\n",
    "    print(f\"- File: {output_file}\")\n",
    "    print(f\"- Total rows: {len(combined_df)}\")\n",
    "    print(f\"- Columns: {', '.join(combined_df.columns)}\")\n",
    "else:\n",
    "    print(\"No data found to consolidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_5584/3784868714.py:8: DtypeWarning:\n",
      "\n",
      "Columns (10,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "csv_file_path = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "output_dir = \"./TaskLevel/analysis_output\"\n",
    "missing_threshold = 35  # Threshold for dropping columns (%)\n",
    "chunk_size = 50000  # Process in chunks to manage memory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def analyze_full_dataset(filepath, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Analyze the entire dataset in chunks to find missing values and data types\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        chunk_size: Size of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with missing value analysis and data type information\n",
    "    \"\"\"\n",
    "    print(f\"Starting analysis of {filepath}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    # First pass: get column names and count total rows\n",
    "    print(\"First pass: counting rows and getting column names...\")\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Get column names from the first row\n",
    "    columns = pd.read_csv(filepath, nrows=0).columns.tolist()\n",
    "    \n",
    "    # Count rows without loading entire file\n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Total columns: {len(columns)}\")\n",
    "    \n",
    "    # Initialize counters for missing values and data type analysis\n",
    "    missing_counts = {col: 0 for col in columns}\n",
    "    \n",
    "    # Data type detection structures\n",
    "    non_null_sample_values = defaultdict(list)\n",
    "    unique_values = defaultdict(set)\n",
    "    numeric_cols = set()\n",
    "    likely_date_cols = set()\n",
    "    likely_boolean_cols = set()\n",
    "    \n",
    "    # Second pass: analyze missing values and collect sample data for type detection\n",
    "    print(\"Second pass: analyzing missing values and data types...\")\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "        chunk_count += 1\n",
    "        \n",
    "        # Update missing value counts\n",
    "        for col in columns:\n",
    "            if col in chunk.columns:\n",
    "                missing_counts[col] += chunk[col].isna().sum()\n",
    "        \n",
    "        # Collect data for type detection (from first few chunks only)\n",
    "        if chunk_count <= 5:  # Limit sample collection to first 5 chunks\n",
    "            for col in columns:\n",
    "                if col in chunk.columns:\n",
    "                    # Skip columns with all missing values in this chunk\n",
    "                    if chunk[col].isna().all():\n",
    "                        continue\n",
    "                    \n",
    "                    # Get non-null values for sampling\n",
    "                    non_null_vals = chunk[col].dropna()\n",
    "                    if len(non_null_vals) > 0:\n",
    "                        # Add a small sample to our collection\n",
    "                        sample = non_null_vals.sample(min(20, len(non_null_vals)))\n",
    "                        non_null_sample_values[col].extend(sample.tolist())\n",
    "                        \n",
    "                        # Track unique values (up to a limit)\n",
    "                        if len(unique_values[col]) < 1000:  # Limit unique value tracking\n",
    "                            unique_values[col].update(sample.tolist())\n",
    "                        \n",
    "                        # Check if column is numeric\n",
    "                        if pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "                            numeric_cols.add(col)\n",
    "                        \n",
    "                        # Check if column might be a date\n",
    "                        if col not in likely_date_cols and pd.api.types.is_object_dtype(chunk[col]):\n",
    "                            date_samples = pd.to_datetime(sample, errors='coerce')\n",
    "                            if date_samples.notna().sum() / len(date_samples) > 0.9:\n",
    "                                likely_date_cols.add(col)\n",
    "                        \n",
    "                        # Check if column might be boolean\n",
    "                        if col not in likely_boolean_cols:\n",
    "                            bool_values = [True, False, 0, 1, \"0\", \"1\", \"true\", \"false\", \"True\", \"False\"]\n",
    "                            if all(str(val).lower() in [str(bv).lower() for bv in bool_values] for val in sample):\n",
    "                                likely_boolean_cols.add(col)\n",
    "        \n",
    "        # Report progress\n",
    "        if chunk_count % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Processed {chunk_count} chunks ({chunk_count * chunk_size} rows) in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Calculate missing percentages\n",
    "    missing_percent = {col: (count / total_rows) * 100 for col, count in missing_counts.items()}\n",
    "    \n",
    "    # Determine data types\n",
    "    data_types = {}\n",
    "    for col in columns:\n",
    "        if col in likely_date_cols:\n",
    "            data_types[col] = 'datetime'\n",
    "        elif col in likely_boolean_cols:\n",
    "            data_types[col] = 'boolean'\n",
    "        elif col in numeric_cols:\n",
    "            data_types[col] = 'numeric'\n",
    "        elif col in unique_values and len(unique_values[col]) < 20:\n",
    "            data_types[col] = 'categorical'\n",
    "        elif col in unique_values and len(unique_values[col]) < 100:\n",
    "            data_types[col] = 'categorical_high_cardinality'\n",
    "        else:\n",
    "            data_types[col] = 'text'\n",
    "    \n",
    "    # Create DataFrame with missing value statistics\n",
    "    missing_df = pd.DataFrame({\n",
    "        'column': list(missing_counts.keys()),\n",
    "        'missing_count': list(missing_counts.values()),\n",
    "        'missing_percent': [missing_percent[col] for col in missing_counts.keys()],\n",
    "        'data_type': [data_types.get(col, 'unknown') for col in missing_counts.keys()]\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_df = missing_df.sort_values('missing_percent', ascending=False)\n",
    "    \n",
    "    # Add action based on threshold\n",
    "    missing_df['action'] = np.where(\n",
    "        missing_df['missing_percent'] > missing_threshold, \n",
    "        'drop', \n",
    "        'impute'\n",
    "    )\n",
    "    \n",
    "    # Add sample values\n",
    "    missing_df['sample_values'] = missing_df['column'].apply(\n",
    "        lambda col: str(non_null_sample_values.get(col, [])[:3])\n",
    "    )\n",
    "    \n",
    "    # Add unique value counts\n",
    "    missing_df['unique_count'] = missing_df['column'].apply(\n",
    "        lambda col: len(unique_values.get(col, set()))\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Analysis completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    return missing_df, total_rows\n",
    "\n",
    "def determine_imputation_strategies(missing_df):\n",
    "    \"\"\"\n",
    "    Determine appropriate imputation strategies based on data types and missing percentages\n",
    "    \n",
    "    Args:\n",
    "        missing_df: DataFrame with missing value analysis\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with recommended imputation strategies\n",
    "    \"\"\"\n",
    "    # Add imputation strategy column\n",
    "    missing_df['imputation_strategy'] = 'none'\n",
    "    \n",
    "    # Strategies based on data type\n",
    "    for idx, row in missing_df.iterrows():\n",
    "        col = row['column']\n",
    "        dtype = row['data_type']\n",
    "        missing_pct = row['missing_percent']\n",
    "        \n",
    "        if row['action'] == 'drop':\n",
    "            missing_df.loc[idx, 'imputation_strategy'] = 'drop_column'\n",
    "            continue\n",
    "        \n",
    "        # Low missing values (<5%)\n",
    "        if missing_pct < 5:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median'\n",
    "            elif dtype in ['categorical', 'categorical_high_cardinality', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "                \n",
    "        # Medium missing values (5-15%)\n",
    "        elif missing_pct < 15:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median_by_group'\n",
    "            elif dtype in ['categorical', 'categorical_high_cardinality', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode_by_group'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "                \n",
    "        # High missing values (15-35%)\n",
    "        else:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median_by_group'\n",
    "            elif dtype in ['categorical', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'new_category'\n",
    "            elif dtype == 'categorical_high_cardinality':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode_by_group'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "def generate_imputation_code(missing_df):\n",
    "    \"\"\"\n",
    "    Generate Python code for implementing the recommended imputation strategies\n",
    "    \n",
    "    Args:\n",
    "        missing_df: DataFrame with missing value analysis and strategies\n",
    "        \n",
    "    Returns:\n",
    "        String containing Python code for imputation\n",
    "    \"\"\"\n",
    "    # Group columns by imputation strategy\n",
    "    strategy_groups = {}\n",
    "    for _, row in missing_df.iterrows():\n",
    "        strategy = row['imputation_strategy']\n",
    "        if strategy not in strategy_groups:\n",
    "            strategy_groups[strategy] = []\n",
    "        \n",
    "        strategy_groups[strategy].append((row['column'], row['data_type']))\n",
    "    \n",
    "    # Generate code\n",
    "    code_lines = [\n",
    "        \"import pandas as pd\",\n",
    "        \"import numpy as np\",\n",
    "        \"from sklearn.impute import SimpleImputer\",\n",
    "        \"\",\n",
    "        \"# Function to impute missing values based on recommended strategies\",\n",
    "        \"def impute_missing_values(df, grouping_cols=None):\",\n",
    "        \"    \\\"\\\"\\\"\",\n",
    "        \"    Impute missing values using recommended strategies\",\n",
    "        \"    \",\n",
    "        \"    Args:\",\n",
    "        \"        df: DataFrame to process\",\n",
    "        \"        grouping_cols: Columns to group by for group-based imputation\",\n",
    "        \"    \",\n",
    "        \"    Returns:\",\n",
    "        \"        DataFrame with imputed values\",\n",
    "        \"    \\\"\\\"\\\"\",\n",
    "        \"    # Make a copy to avoid modifying the original\",\n",
    "        \"    imputed_df = df.copy()\",\n",
    "        \"\",\n",
    "        \"    # Use default grouping columns if none provided\",\n",
    "        \"    if grouping_cols is None:\",\n",
    "        \"        # Check if these columns exist in the dataframe\",\n",
    "        \"        possible_groups = ['fields.issuetype.name', 'fields.priority.name', 'fields.project.key']\",\n",
    "        \"        grouping_cols = [col for col in possible_groups if col in df.columns]\",\n",
    "        \"\",\n",
    "        \"    # If no grouping columns are available, use median/mode without grouping\",\n",
    "        \"    has_groups = len(grouping_cols) > 0\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    # Add drop column code if needed\n",
    "    if 'drop_column' in strategy_groups and strategy_groups['drop_column']:\n",
    "        cols_to_drop = [col for col, _ in strategy_groups['drop_column']]\n",
    "        code_lines.extend([\n",
    "            \"    # 1. Drop columns with too many missing values\",\n",
    "            f\"    cols_to_drop = {cols_to_drop}\",\n",
    "            \"    print(f\\\"Dropping {len(cols_to_drop)} columns with >35% missing values\\\")\",\n",
    "            \"    imputed_df = imputed_df.drop(columns=[col for col in cols_to_drop if col in imputed_df.columns])\",\n",
    "            \"\",\n",
    "        ])\n",
    "    \n",
    "    # Add median imputation code if needed\n",
    "    if 'median' in strategy_groups and strategy_groups['median']:\n",
    "        numeric_cols = [col for col, dtype in strategy_groups['median'] if dtype == 'numeric']\n",
    "        if numeric_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 2. Simple median imputation for numeric columns\",\n",
    "                f\"    median_cols = {numeric_cols}\",\n",
    "                \"    existing_median_cols = [col for col in median_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_median_cols:\",\n",
    "                \"        print(f\\\"Applying median imputation to {len(existing_median_cols)} columns\\\")\",\n",
    "                \"        imputer = SimpleImputer(strategy='median')\",\n",
    "                \"        imputed_df[existing_median_cols] = imputer.fit_transform(imputed_df[existing_median_cols])\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add mode imputation code if needed\n",
    "    if 'mode' in strategy_groups and strategy_groups['mode']:\n",
    "        categorical_cols = [col for col, dtype in strategy_groups['mode'] \n",
    "                          if dtype in ['categorical', 'categorical_high_cardinality', 'boolean']]\n",
    "        if categorical_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 3. Simple mode imputation for categorical columns\",\n",
    "                f\"    mode_cols = {categorical_cols}\",\n",
    "                \"    existing_mode_cols = [col for col in mode_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_mode_cols:\",\n",
    "                \"        print(f\\\"Applying mode imputation to {len(existing_mode_cols)} columns\\\")\",\n",
    "                \"        for col in existing_mode_cols:\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add grouped median imputation code if needed\n",
    "    if 'median_by_group' in strategy_groups and strategy_groups['median_by_group']:\n",
    "        grouped_numeric_cols = [col for col, dtype in strategy_groups['median_by_group'] if dtype == 'numeric']\n",
    "        if grouped_numeric_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 4. Grouped median imputation for numeric columns\",\n",
    "                f\"    grouped_median_cols = {grouped_numeric_cols}\",\n",
    "                \"    existing_grouped_median_cols = [col for col in grouped_median_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_grouped_median_cols and has_groups:\",\n",
    "                \"        print(f\\\"Applying grouped median imputation to {len(existing_grouped_median_cols)} columns\\\")\",\n",
    "                \"        for col in existing_grouped_median_cols:\",\n",
    "                \"            # Calculate medians by group\",\n",
    "                \"            group_medians = imputed_df.groupby(grouping_cols)[col].median()\",\n",
    "                \"            # For each combination of grouping values, fill with the group median\",\n",
    "                \"            for group_values, median_value in group_medians.items():\",\n",
    "                \"                if not isinstance(group_values, tuple):\",\n",
    "                \"                    group_values = (group_values,)\",\n",
    "                \"                if pd.notna(median_value):\",\n",
    "                \"                    # Create a mask for this group\",\n",
    "                \"                    mask = pd.Series(True, index=imputed_df.index)\",\n",
    "                \"                    for i, group_col in enumerate(grouping_cols):\",\n",
    "                \"                        mask = mask & (imputed_df[group_col] == group_values[i])\",\n",
    "                \"                    # Apply the group median to missing values in this group\",\n",
    "                \"                    mask = mask & imputed_df[col].isna()\",\n",
    "                \"                    imputed_df.loc[mask, col] = median_value\",\n",
    "                \"            # For any remaining NaNs, use overall median\",\n",
    "                \"            overall_median = imputed_df[col].median()\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(overall_median)\",\n",
    "                \"    elif existing_grouped_median_cols:\",\n",
    "                \"        # Fall back to simple median if no grouping columns\",\n",
    "                \"        imputer = SimpleImputer(strategy='median')\",\n",
    "                \"        imputed_df[existing_grouped_median_cols] = imputer.fit_transform(imputed_df[existing_grouped_median_cols])\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add grouped mode imputation code if needed\n",
    "    if 'mode_by_group' in strategy_groups and strategy_groups['mode_by_group']:\n",
    "        grouped_cat_cols = [col for col, dtype in strategy_groups['mode_by_group'] \n",
    "                           if dtype in ['categorical', 'categorical_high_cardinality', 'boolean']]\n",
    "        if grouped_cat_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 5. Grouped mode imputation for categorical columns\",\n",
    "                f\"    grouped_mode_cols = {grouped_cat_cols}\",\n",
    "                \"    existing_grouped_mode_cols = [col for col in grouped_mode_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_grouped_mode_cols and has_groups:\",\n",
    "                \"        print(f\\\"Applying grouped mode imputation to {len(existing_grouped_mode_cols)} columns\\\")\",\n",
    "                \"        for col in existing_grouped_mode_cols:\",\n",
    "                \"            # Calculate modes by group\",\n",
    "                \"            for group_values, group_df in imputed_df.groupby(grouping_cols):\",\n",
    "                \"                if not isinstance(group_values, tuple):\",\n",
    "                \"                    group_values = (group_values,)\",\n",
    "                \"                # Get mode for this group\",\n",
    "                \"                mode_series = group_df[col].mode()\",\n",
    "                \"                if not mode_series.empty:\",\n",
    "                \"                    mode_value = mode_series[0]\",\n",
    "                \"                    # Create a mask for this group\",\n",
    "                \"                    mask = pd.Series(True, index=imputed_df.index)\",\n",
    "                \"                    for i, group_col in enumerate(grouping_cols):\",\n",
    "                \"                        mask = mask & (imputed_df[group_col] == group_values[i])\",\n",
    "                \"                    # Apply the group mode to missing values in this group\",\n",
    "                \"                    mask = mask & imputed_df[col].isna()\",\n",
    "                \"                    imputed_df.loc[mask, col] = mode_value\",\n",
    "                \"            # For any remaining NaNs, use overall mode\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"    elif existing_grouped_mode_cols:\",\n",
    "                \"        # Fall back to simple mode if no grouping columns\",\n",
    "                \"        for col in existing_grouped_mode_cols:\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add new category imputation code if needed\n",
    "    if 'new_category' in strategy_groups and strategy_groups['new_category']:\n",
    "        new_cat_cols = [col for col, dtype in strategy_groups['new_category'] \n",
    "                       if dtype in ['categorical', 'boolean']]\n",
    "        if new_cat_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 6. New category imputation for categorical columns\",\n",
    "                f\"    new_category_cols = {new_cat_cols}\",\n",
    "                \"    existing_new_cat_cols = [col for col in new_category_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_new_cat_cols:\",\n",
    "                \"        print(f\\\"Applying new category imputation to {len(existing_new_cat_cols)} columns\\\")\",\n",
    "                \"        for col in existing_new_cat_cols:\",\n",
    "                \"            # Fill missing with a new category 'Unknown'\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna('Unknown')\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add code to drop rows with remaining NaNs in key columns\n",
    "    code_lines.extend([\n",
    "        \"    # 7. Finally, drop rows with remaining NaNs in essential columns\",\n",
    "        \"    essential_columns = ['fields.issuetype.name', 'fields.created', 'key']\",\n",
    "        \"    existing_essential = [col for col in essential_columns if col in imputed_df.columns]\",\n",
    "        \"    if existing_essential:\",\n",
    "        \"        before_rows = len(imputed_df)\",\n",
    "        \"        imputed_df = imputed_df.dropna(subset=existing_essential)\",\n",
    "        \"        dropped_rows = before_rows - len(imputed_df)\",\n",
    "        \"        print(f\\\"Dropped {dropped_rows} rows with missing values in essential columns\\\")\",\n",
    "        \"\",\n",
    "        \"    return imputed_df\",\n",
    "        \"\",\n",
    "        \"# Example usage:\",\n",
    "        \"# df = pd.read_csv('your_file.csv')\",\n",
    "        \"# imputed_df = impute_missing_values(df)\",\n",
    "        \"# imputed_df.to_csv('imputed_data.csv', index=False)\",\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(code_lines)\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"Error: File not found at {csv_file_path}\")\n",
    "    else:\n",
    "        # Analyze full dataset\n",
    "        missing_analysis, total_rows = analyze_full_dataset(csv_file_path, chunk_size)\n",
    "        \n",
    "        # Determine imputation strategies\n",
    "        missing_analysis = determine_imputation_strategies(missing_analysis)\n",
    "        \n",
    "        # Save missing value analysis\n",
    "        analysis_output = os.path.join(output_dir, 'missing_value_analysis.csv')\n",
    "        missing_analysis.to_csv(analysis_output, index=False)\n",
    "        print(f\"Missing value analysis saved to {analysis_output}\")\n",
    "        \n",
    "        # Generate imputation code\n",
    "        imputation_code = generate_imputation_code(missing_analysis)\n",
    "        code_output = os.path.join(output_dir, 'imputation_code.py')\n",
    "        with open(code_output, 'w') as f:\n",
    "            f.write(imputation_code)\n",
    "        print(f\"Imputation code generated and saved to {code_output}\")\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram of missing value percentages\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.histplot(missing_analysis['missing_percent'], bins=20)\n",
    "        plt.axvline(x=missing_threshold, color='red', linestyle='--', \n",
    "                    label=f'Drop threshold ({missing_threshold}%)')\n",
    "        plt.title('Distribution of Missing Value Percentages')\n",
    "        plt.xlabel('Missing Percentage')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Create bar chart of action counts\n",
    "        plt.subplot(2, 1, 2)\n",
    "        action_counts = missing_analysis['action'].value_counts()\n",
    "        sns.barplot(x=action_counts.index, y=action_counts.values)\n",
    "        plt.title('Recommended Actions for Columns')\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'missing_value_summary.png'))\n",
    "        \n",
    "        # Create pie chart of data types\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        type_counts = missing_analysis['data_type'].value_counts()\n",
    "        plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Column Data Types')\n",
    "        plt.savefig(os.path.join(output_dir, 'data_type_distribution.png'))\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\n=== MISSING VALUE ANALYSIS SUMMARY ===\")\n",
    "        print(f\"Total rows in dataset: {total_rows}\")\n",
    "        print(f\"Total columns: {len(missing_analysis)}\")\n",
    "        print(f\"Columns to drop (>{missing_threshold}% missing): {missing_analysis['action'].value_counts()['drop']}\")\n",
    "        print(f\"Columns to impute: {missing_analysis['action'].value_counts()['impute']}\")\n",
    "        \n",
    "        # Print data type distribution\n",
    "        print(\"\\nData type distribution:\")\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  - {dtype}: {count} columns\")\n",
    "        \n",
    "        # Print top 10 columns with highest missing percentages\n",
    "        print(\"\\nTop 10 columns with highest missing percentages:\")\n",
    "        for _, row in missing_analysis.head(10).iterrows():\n",
    "            print(f\"  - {row['column']}: {row['missing_percent']:.2f}% missing ({row['data_type']})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data analysis from ./TaskLevel/data_analysis_report.json\n",
      "Reading 1000 rows from ./TaskLevel/consolidated_task_data_filtered.csv\n",
      "Loaded 1000 rows\n",
      "Dropping columns with high missing rates or URL data\n",
      "Dropped 9 columns\n",
      "Converting date columns to datetime...\n",
      "Creating features for effort estimation...\n",
      "Creating temporal features...\n",
      "Calculating resolution hours...\n",
      "Creating issue type encodings...\n",
      "Creating priority encodings...\n",
      "Performing imputation for missing values...\n",
      "Imputing resolution hours for 274 unresolved issues\n",
      "Saving processed dataset with 55 columns to ./TaskLevel/processed_task_data.csv\n",
      "\n",
      "Created 19 new features:\n",
      "- created_day_of_week: 1000 non-null values (100.0%)\n",
      "- created_hour: 1000 non-null values (100.0%)\n",
      "- created_is_weekend: 1000 non-null values (100.0%)\n",
      "- created_month: 1000 non-null values (100.0%)\n",
      "- created_year: 1000 non-null values (100.0%)\n",
      "- is_priority_blocker: 1000 non-null values (100.0%)\n",
      "- is_priority_critical: 1000 non-null values (100.0%)\n",
      "- is_priority_major: 1000 non-null values (100.0%)\n",
      "- is_priority_minor: 1000 non-null values (100.0%)\n",
      "- is_priority_trivial: 1000 non-null values (100.0%)\n",
      "- is_type_bug: 1000 non-null values (100.0%)\n",
      "- is_type_epic: 1000 non-null values (100.0%)\n",
      "- is_type_improvement: 1000 non-null values (100.0%)\n",
      "- is_type_new_feature: 1000 non-null values (100.0%)\n",
      "- is_type_story: 1000 non-null values (100.0%)\n",
      "- is_type_sub-task: 1000 non-null values (100.0%)\n",
      "- is_type_task: 1000 non-null values (100.0%)\n",
      "- log_resolution_hours: 1000 non-null values (100.0%)\n",
      "- resolution_hours: 1000 non-null values (100.0%)\n",
      "\n",
      "Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_8203/2793032469.py:57: FutureWarning:\n",
      "\n",
      "use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "input_file = \"./TaskLevel/consolidated_task_data_filtered.csv\"\n",
    "output_file = \"./TaskLevel/processed_task_data.csv\"\n",
    "analysis_file = \"./TaskLevel/data_analysis_report.json\"  # Adjusted path\n",
    "sample_size = None  # Set this to None to process all rows\n",
    "\n",
    "# Load analysis results with error handling\n",
    "try:\n",
    "    with open(analysis_file, 'r') as f:\n",
    "        analysis = json.load(f)\n",
    "    print(f\"Loaded data analysis from {analysis_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Analysis file {analysis_file} not found. Using default data types.\")\n",
    "    analysis = {'columns': {}, 'missing_value_columns': []}\n",
    "\n",
    "# Define data types (with fallback if analysis file wasn't found)\n",
    "dtype_map = {}\n",
    "if analysis['columns']:\n",
    "    for col, info in analysis['columns'].items():\n",
    "        if info['dtype'] == 'categorical' or info['dtype'] == 'text':\n",
    "            dtype_map[col] = 'string'\n",
    "        elif info['dtype'] == 'float64' or info['dtype'] == 'int64':\n",
    "            dtype_map[col] = 'float64'  # Use float64 for all numeric to handle NaN\n",
    "else:\n",
    "    # Fallback type mapping for critical columns\n",
    "    critical_columns = {\n",
    "        'fields.issuetype.name': 'string',\n",
    "        'fields.priority.name': 'string',\n",
    "        'fields.project.key': 'string',\n",
    "        'fields.status.name': 'string',\n",
    "        'fields.creator.name': 'string',\n",
    "        'is_completed': 'float64',\n",
    "        'is_resolved': 'float64',\n",
    "        'type_task': 'float64',\n",
    "        'type_bug': 'float64',\n",
    "        'resolution_time_days': 'float64',\n",
    "        'age_days': 'float64',\n",
    "        'type_sub_task': 'float64'\n",
    "    }\n",
    "    dtype_map.update(critical_columns)\n",
    "\n",
    "# Define columns to drop\n",
    "columns_to_drop = []\n",
    "if 'missing_value_columns' in analysis:\n",
    "    columns_to_drop = [col['column'] for col in analysis['missing_value_columns'] \n",
    "                      if col['missing_percentage'] > 35]\n",
    "columns_to_drop.extend(['fields.creator.avatarUrls.48x48', 'fields.creator.avatarUrls.24x24', \n",
    "                        'fields.creator.avatarUrls.16x16', 'fields.creator.avatarUrls.32x32', \n",
    "                        'fields.creator.self'])\n",
    "\n",
    "# Set pandas options for safer processing\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "\n",
    "try:\n",
    "    # Read data with appropriate types and error handling\n",
    "    if sample_size:\n",
    "        print(f\"Reading {sample_size} rows from {input_file}\")\n",
    "        df = pd.read_csv(input_file, dtype=dtype_map, nrows=sample_size, low_memory=False)\n",
    "    else:\n",
    "        print(f\"Reading all rows from {input_file}\")\n",
    "        df = pd.read_csv(input_file, dtype=dtype_map, low_memory=False)\n",
    "\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "    # Drop high-missing columns\n",
    "    print(f\"Dropping columns with high missing rates or URL data\")\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    if columns_to_drop:\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Dropped {len(columns_to_drop)} columns\")\n",
    "\n",
    "    # Explicitly convert date columns\n",
    "    date_columns = ['fields.created', 'fields.updated', 'fields.resolutiondate']\n",
    "    print(\"Converting date columns to datetime...\")\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    print(\"Creating features for effort estimation...\")\n",
    "\n",
    "    # Process each feature carefully with try/except blocks\n",
    "    try:\n",
    "        # 1. Process datetime fields for temporal features\n",
    "        if 'fields.created' in df.columns:\n",
    "            print(\"Creating temporal features...\")\n",
    "            df['created_day_of_week'] = df['fields.created'].dt.dayofweek\n",
    "            df['created_is_weekend'] = (df['fields.created'].dt.dayofweek >= 5).astype(float)\n",
    "            df['created_hour'] = df['fields.created'].dt.hour\n",
    "            df['created_month'] = df['fields.created'].dt.month\n",
    "            df['created_year'] = df['fields.created'].dt.year\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating temporal features: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        # 2. Calculate resolution hours\n",
    "        if all(col in df.columns for col in ['fields.created', 'fields.resolutiondate']):\n",
    "            print(\"Calculating resolution hours...\")\n",
    "            df['resolution_hours'] = np.nan\n",
    "            mask = df['fields.resolutiondate'].notna()\n",
    "            if mask.any():\n",
    "                df.loc[mask, 'resolution_hours'] = (\n",
    "                    (df.loc[mask, 'fields.resolutiondate'] - df.loc[mask, 'fields.created']).dt.total_seconds() / 3600\n",
    "                )\n",
    "            df['log_resolution_hours'] = np.log1p(df['resolution_hours'].fillna(0).clip(lower=0))\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating resolution hours: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        # 3. Create one-hot encodings\n",
    "        if 'fields.issuetype.name' in df.columns:\n",
    "            print(\"Creating issue type encodings...\")\n",
    "            for issue_type in ['Bug', 'Task', 'Story', 'Improvement', 'New Feature', 'Epic', 'Sub-task']:\n",
    "                col_name = f'is_type_{issue_type.lower().replace(\" \", \"_\")}'\n",
    "                df[col_name] = (df['fields.issuetype.name'].str.lower() == issue_type.lower()).astype(float)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating issue type encodings: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        if 'fields.priority.name' in df.columns:\n",
    "            print(\"Creating priority encodings...\")\n",
    "            for priority in ['Blocker', 'Critical', 'Major', 'Minor', 'Trivial']:\n",
    "                col_name = f'is_priority_{priority.lower()}'\n",
    "                df[col_name] = df['fields.priority.name'].str.lower().str.contains(priority.lower(), na=False).astype(float)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating priority encodings: {str(e)}\")\n",
    "\n",
    "    # Imputation section with more robust error handling\n",
    "    print(\"Performing imputation for missing values...\")\n",
    "\n",
    "    # Calculate age in days for all issues (needed for imputation)\n",
    "    try:\n",
    "        if 'fields.created' in df.columns and 'age_days' not in df.columns:\n",
    "            print(\"Calculating age_days...\")\n",
    "            current_time = pd.Timestamp.now()\n",
    "            if 'fields.created' in df.columns and len(df) > 0 and df['fields.created'].iloc[0] is not None:\n",
    "                if hasattr(df['fields.created'].iloc[0], 'tzinfo') and df['fields.created'].iloc[0].tzinfo is not None:\n",
    "                    current_time = current_time.tz_localize('UTC')\n",
    "            df['age_days'] = (current_time - df['fields.created']).dt.total_seconds() / (24 * 3600)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating age_days: {str(e)}\")\n",
    "        df['age_days'] = 30.0  # Fallback value\n",
    "\n",
    "    # For unresolved issues, impute resolution hours\n",
    "    try:\n",
    "        unresolved_mask = df['fields.resolutiondate'].isna()\n",
    "        if unresolved_mask.any():\n",
    "            print(f\"Imputing resolution hours for {unresolved_mask.sum()} unresolved issues\")\n",
    "            \n",
    "            # First get global_median as fallback\n",
    "            resolved_mask = ~unresolved_mask\n",
    "            if resolved_mask.any() and 'resolution_hours' in df.columns:\n",
    "                global_median = df.loc[resolved_mask, 'resolution_hours'].median()\n",
    "                if pd.isna(global_median):  # If still no valid median\n",
    "                    global_median = 24.0  # Default to 24 hours\n",
    "            else:\n",
    "                global_median = 24.0  # Default value\n",
    "                \n",
    "            # Try group-based imputation\n",
    "            try:\n",
    "                if all(col in df.columns for col in ['fields.issuetype.name', 'fields.priority.name']):\n",
    "                    # Group by issue type and priority\n",
    "                    for name, group in df[resolved_mask].groupby(['fields.issuetype.name', 'fields.priority.name']):\n",
    "                        if len(group) > 0 and 'resolution_hours' in group.columns:\n",
    "                            median_hours = group['resolution_hours'].median()\n",
    "                            if pd.isna(median_hours):\n",
    "                                median_hours = global_median\n",
    "                                \n",
    "                            # Create safe mask for this group\n",
    "                            if isinstance(name, tuple) and len(name) == 2:\n",
    "                                issue_type, priority = name\n",
    "                                type_mask = (df['fields.issuetype.name'] == issue_type)\n",
    "                                prio_mask = (df['fields.priority.name'] == priority)\n",
    "                                group_mask = type_mask & prio_mask & unresolved_mask\n",
    "                                \n",
    "                                if group_mask.any():\n",
    "                                    # Apply age-based adjustment\n",
    "                                    age_factor = 1.0 + 0.1 * (df.loc[group_mask, 'age_days'] / 30.0).clip(0, 10.0)\n",
    "                                    df.loc[group_mask, 'resolution_hours'] = median_hours * age_factor\n",
    "            except Exception as e:\n",
    "                print(f\"Error in group-based imputation: {str(e)}\")\n",
    "                \n",
    "            # For any remaining missing values, use global median\n",
    "            missing_hours = df['resolution_hours'].isna() & unresolved_mask\n",
    "            if missing_hours.any():\n",
    "                age_factor = 1.0 + 0.1 * (df.loc[missing_hours, 'age_days'] / 30.0).clip(0, 10.0)\n",
    "                df.loc[missing_hours, 'resolution_hours'] = global_median * age_factor\n",
    "                \n",
    "            # Update log transform\n",
    "            if 'log_resolution_hours' in df.columns:\n",
    "                imputed_mask = unresolved_mask & df['resolution_hours'].notna() & (df['resolution_hours'] > 0)\n",
    "                if imputed_mask.any():\n",
    "                    df.loc[imputed_mask, 'log_resolution_hours'] = np.log1p(df.loc[imputed_mask, 'resolution_hours'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error imputing unresolved issues: {str(e)}\")\n",
    "\n",
    "    # Save processed dataset\n",
    "    print(f\"Saving processed dataset with {len(df.columns)} columns to {output_file}\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print summary of new features\n",
    "    try:\n",
    "        original_columns = set(pd.read_csv(input_file, nrows=0).columns)\n",
    "        new_columns = [col for col in df.columns if col not in original_columns]\n",
    "        \n",
    "        print(f\"\\nCreated {len(new_columns)} new features:\")\n",
    "        for col in sorted(new_columns):\n",
    "            non_null = df[col].notna().sum()\n",
    "            print(f\"- {col}: {non_null} non-null values ({non_null/len(df)*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating feature summary: {str(e)}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Critical error in processing: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./TaskLevel/processed_task_data.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
