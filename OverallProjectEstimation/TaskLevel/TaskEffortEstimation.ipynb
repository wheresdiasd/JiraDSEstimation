{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.cluster import KMeans\nimport pickle\nimport time\nimport os\nimport warnings\nimport glob\nfrom joblib import Parallel, delayed\nwarnings.filterwarnings('ignore')\n\n# Create directory for results\nresults_dir = 'task_estimation_results'\nos.makedirs(results_dir, exist_ok=True)\n\n# 1. Load the project-level metrics\nprint(\"Loading project-level metrics...\")\n# Load the project classification model results\ntry:\n    with open('../Models/Results/project_classification_results/results/cluster_profiles.json', 'r') as f:\n        import json\n        project_clusters = json.load(f)\n    print(\"Loaded project cluster profiles\")\n    \n    # Find all project models\n    model_files = glob.glob('../Models/*_*.pkl')\n    project_models = [os.path.basename(f) for f in model_files]\n    print(f\"Found {len(project_models)} project-level estimation models\")\nexcept Exception as e:\n    print(f\"Error loading project models: {e}\")\n    project_clusters = {}\n    project_models = []\n\n# 2. Load the pre-processed scaled feature data\nprint(\"\\nLoading scaled feature data...\")\ntry:\n    # Load the preprocessed scaled features file\n    task_data = pd.read_csv('../prepared_processed_data/common_features_scaled_with_original_targets.csv')\n    print(f\"Task data loaded: {task_data.shape[0]} tasks, {task_data.shape[1]} features\")\nexcept FileNotFoundError:\n    raise ValueError(\"Could not find common_features_scaled_with_original_targets.csv file\")\n\n# Check for any remaining non-numeric columns\nnon_numeric_cols = task_data.select_dtypes(exclude=np.number).columns.tolist()\nprint(f\"Found {len(non_numeric_cols)} non-numeric columns that need handling\")\n\n# 3. Identify project information from the data\nprint(\"\\nIdentifying project information...\")\nif 'remainder__project_id' in task_data.columns:\n    # Extract unique project IDs\n    project_ids = task_data['remainder__project_id'].unique()\n    print(f\"Found {len(project_ids)} unique projects in the data\")\n    \n    # Map projects to their cluster assignments if available\n    if project_clusters:\n        # Map project IDs to their cluster assignments\n        project_to_cluster = {}\n        for cluster_id, cluster_info in project_clusters.items():\n            if 'projects' in cluster_info:\n                for project in cluster_info['projects']:\n                    project_to_cluster[project] = cluster_id\n        \n        # Add cluster information to the task data\n        task_data['project_cluster'] = task_data['remainder__project_id'].map(\n            lambda x: project_to_cluster.get(str(x), -1)\n        )\n        \n        # Count tasks by cluster\n        cluster_counts = task_data['project_cluster'].value_counts()\n        print(\"Task distribution by project cluster:\")\n        print(cluster_counts)\n    else:\n        print(\"No project cluster information available\")\nelse:\n    print(\"No project ID column found in the data\")\n    project_ids = []\n\n# 4. Prepare the target variable - resolution time in hours\nprint(\"\\nPreparing target variable...\")\n\n# Select the appropriate target variable from scaled dataset\ntarget_variable = 'avg_resolution_hours'\nif target_variable not in task_data.columns:\n    # Try alternative target variables\n    target_candidates = ['median_resolution_hours', 'resolution_hours', 'total_resolution_hours']\n    for candidate in target_candidates:\n        if candidate in task_data.columns:\n            target_variable = candidate\n            break\n    else:\n        raise ValueError(\"No suitable resolution time target found in the dataset\")\n\n# Filter out invalid target values\ntask_data = task_data.dropna(subset=[target_variable])\ntask_data = task_data[task_data[target_variable] >= 0]\ntask_data = task_data[task_data[target_variable] <= 10000]  # Cap at ~417 days\n\nprint(f\"\\nTarget variable statistics:\")\nprint(f\"  Mean: {task_data[target_variable].mean():.2f} hours\")\nprint(f\"  Median: {task_data[target_variable].median():.2f} hours\")\nprint(f\"  Min: {task_data[target_variable].min():.2f} hours\")\nprint(f\"  Max: {task_data[target_variable].max():.2f} hours\")\n\n# 5. Feature preparation\nprint(\"\\nPreparing features...\")\nfeature_data = task_data.copy()\n\n# 5.1 Split features into categories\nscaled_features = [col for col in feature_data.columns if any(prefix in col for prefix in \n                                                             ['time_power__', 'pct_minmax__', 'count_std__', \n                                                              'stat_robust__', 'link_std__'])]\n\nremainder_features = [col for col in feature_data.columns if 'remainder__' in col]\n\n# List all numeric features for modeling (excluding target variables)\ntarget_vars = ['avg_resolution_hours', 'median_resolution_hours', 'min_resolution_hours', \n               'max_resolution_hours', 'resolution_hours_std', 'total_resolution_hours']\n\n# Combine all features\nnumeric_cols = []\nnumeric_cols.extend(scaled_features)\n\n# Add remainder features excluding target variables and project ID\nfor col in remainder_features:\n    if col not in target_vars and col != 'remainder__project_id':\n        numeric_cols.append(col)\n\n# Add project cluster if available\nif 'project_cluster' in feature_data.columns:\n    numeric_cols.append('project_cluster')\n\nprint(f\"Using {len(numeric_cols)} features\")\nprint(f\"Final dataset shape: {feature_data.shape}\")\n\n# 6. Split features and target\nprint(\"\\nPreparing data for modeling...\")\nX = feature_data[numeric_cols]\ny = feature_data[target_variable]\n\n# Apply log transformation to handle skew in target\nuse_log_transform = True\nif use_log_transform:\n    print(\"Applying log transformation to target variable\")\n    y = np.log1p(y)  # log(1+x) to handle zeros\n\n# 7. Data partitioning: 50% training, 25% validation, 25% testing\n# First split: 50% training, 50% remaining\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Second split: divide the remaining 50% into equal parts for validation and testing\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\nprint(\"Data partitioning complete:\")\nprint(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n\n# 8. Exploratory visualization\nplt.figure(figsize=(12, 4))\n\n# Training set target distribution\nplt.subplot(131)\nplt.hist(y_train, bins=30, alpha=0.7)\nplt.title('Training Target Distribution')\nplt.xlabel('Log(Resolution Hours)')\nplt.ylabel('Frequency')\n\n# Validation set target distribution\nplt.subplot(132)\nplt.hist(y_val, bins=30, alpha=0.7)\nplt.title('Validation Target Distribution')\nplt.xlabel('Log(Resolution Hours)')\n\n# Test set target distribution\nplt.subplot(133)\nplt.hist(y_test, bins=30, alpha=0.7)\nplt.title('Test Target Distribution')\nplt.xlabel('Log(Resolution Hours)')\n\nplt.tight_layout()\nplt.savefig(f'{results_dir}/target_distributions.png')\nplt.show()\n\nprint(f\"\\nTarget variable statistics:\")\nprint(f\"  Training: mean={y_train.mean():.2f}, median={y_train.median():.2f}, min={y_train.min():.2f}, max={y_train.max():.2f}\")\nprint(f\"  Validation: mean={y_val.mean():.2f}, median={y_val.median():.2f}, min={y_val.min():.2f}, max={y_val.max():.2f}\")\nprint(f\"  Test: mean={y_test.mean():.2f}, median={y_test.median():.2f}, min={y_test.min():.2f}, max={y_test.max():.2f}\")\n\n# Save the splits for reproducibility\nsplits = {\n    'X_train': X_train,\n    'y_train': y_train,\n    'X_val': X_val,\n    'y_val': y_val,\n    'X_test': X_test,\n    'y_test': y_test,\n    'log_transform': use_log_transform\n}\n\nwith open(f'{results_dir}/data_splits.pkl', 'wb') as f:\n    pickle.dump(splits, f)\n\nprint(\"\\nInitial setup complete. Ready for model training.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 9. Model Training and Evaluation\nprint(\"Training and evaluating regression models...\")\n\n# Create a stratified model approach based on project clusters if available\nif 'project_cluster' in X.columns and len(X['project_cluster'].unique()) > 1:\n    cluster_based_modeling = True\n    print(f\"Using cluster-based modeling approach with {len(X['project_cluster'].unique())} clusters\")\n    \n    # Store models per cluster\n    cluster_models = {}\nelse:\n    cluster_based_modeling = False\n    print(\"Using global modeling approach (no cluster stratification)\")\n\n# Skip standardization - features are already scaled in the dataset\nX_train_scaled = X_train\nX_val_scaled = X_val\n\n# Define a function to evaluate models\ndef evaluate_model(model, X_val, y_val, model_name, cluster_id=None):\n    # Make predictions\n    y_pred = model.predict(X_val)\n    \n    # Convert from log space if necessary\n    if use_log_transform:\n        y_val_orig = np.expm1(y_val)\n        y_pred_orig = np.expm1(y_pred)\n    else:\n        y_val_orig = y_val\n        y_pred_orig = y_pred\n    \n    # Calculate metrics\n    mae = mean_absolute_error(y_val_orig, y_pred_orig)\n    rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig))\n    r2 = r2_score(y_val_orig, y_pred_orig)\n    \n    # Calculate median absolute error and mean absolute percentage error\n    median_ae = np.median(np.abs(y_val_orig - y_pred_orig))\n    mape = np.mean(np.abs((y_val_orig - y_pred_orig) / (y_val_orig + 1))) * 100  # Adding 1 to avoid division by zero\n    \n    suffix = f\" (Cluster {cluster_id})\" if cluster_id is not None else \"\"\n    print(f\"\\n{model_name}{suffix}:\")\n    print(f\"  MAE: {mae:.2f} hours\")\n    print(f\"  RMSE: {rmse:.2f} hours\")\n    print(f\"  MedianAE: {median_ae:.2f} hours\")\n    print(f\"  MAPE: {mape:.2f}%\")\n    print(f\"  R²: {r2:.4f}\")\n    \n    # Create a scatter plot of actual vs. predicted values\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_val_orig, y_pred_orig, alpha=0.5)\n    plt.plot([0, y_val_orig.max()], [0, y_val_orig.max()], 'r--')\n    model_file_name = model_name.replace(\" \", \"_\")\n    if cluster_id is not None:\n        model_file_name += f\"_cluster_{cluster_id}\"\n    plt.title(f'{model_name}{suffix}: Actual vs. Predicted')\n    plt.xlabel('Actual Resolution Hours')\n    plt.ylabel('Predicted Resolution Hours')\n    plt.savefig(f'{results_dir}/{model_file_name}_predictions.png')\n    plt.show()\n    \n    # Return the metrics\n    return {\n        'model': model,\n        'name': f\"{model_name}{suffix}\",\n        'cluster': cluster_id,\n        'mae': mae,\n        'rmse': rmse,\n        'median_ae': median_ae,\n        'mape': mape,\n        'r2': r2\n    }\n\n# Initialize models\nbase_models = {\n    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n    'Extra Trees': ExtraTreesRegressor(n_estimators=100, random_state=42),\n    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n}\n\n# Train and evaluate models\nresults = []\n\n# Define function to train models for a specific cluster or globally\ndef train_models_for_data(X_train, y_train, X_val, y_val, models, cluster_id=None):\n    local_results = []\n    suffix = f\" (Cluster {cluster_id})\" if cluster_id is not None else \"\"\n    \n    for name, model in models.items():\n        print(f\"\\nTraining {name}{suffix}...\")\n        start_time = time.time()\n        model.fit(X_train, y_train)\n        train_time = time.time() - start_time\n        print(f\"Training completed in {train_time:.2f} seconds\")\n        \n        # Evaluate the model\n        result = evaluate_model(model, X_val, y_val, name, cluster_id)\n        result['train_time'] = train_time\n        local_results.append(result)\n        \n        # Save the model\n        model_file_name = name.replace(\" \", \"_\")\n        if cluster_id is not None:\n            model_file_name += f\"_cluster_{cluster_id}\"\n            \n        with open(f'{results_dir}/{model_file_name}_model.pkl', 'wb') as f:\n            pickle.dump(model, f)\n    \n    return local_results\n\n# If using cluster-based modeling, train separate models per cluster\nif cluster_based_modeling:\n    for cluster_id in sorted(X_train['project_cluster'].unique()):\n        if cluster_id == -1:  # Skip unknown cluster\n            continue\n            \n        print(f\"\\n=== Training models for Cluster {cluster_id} ===\")\n        \n        # Get data for this cluster\n        X_train_cluster = X_train[X_train['project_cluster'] == cluster_id]\n        y_train_cluster = y_train.loc[X_train_cluster.index]\n        \n        X_val_cluster = X_val[X_val['project_cluster'] == cluster_id]\n        y_val_cluster = y_val.loc[X_val_cluster.index]\n        \n        print(f\"Cluster {cluster_id} train size: {len(X_train_cluster)}, validation size: {len(X_val_cluster)}\")\n        \n        if len(X_train_cluster) < 50 or len(X_val_cluster) < 20:\n            print(f\"Skipping Cluster {cluster_id} due to insufficient data\")\n            continue\n        \n        # Train models for this cluster\n        cluster_results = train_models_for_data(\n            X_train_cluster, y_train_cluster, \n            X_val_cluster, y_val_cluster,\n            base_models, cluster_id\n        )\n        \n        # Add cluster results to overall results\n        results.extend(cluster_results)\n        \n        # Store best model for this cluster\n        best_model_idx = max(range(len(cluster_results)), key=lambda i: cluster_results[i]['r2'])\n        cluster_models[cluster_id] = cluster_results[best_model_idx]['model']\n\n# Train global model for all data\nprint(\"\\n=== Training global models ===\")\nglobal_results = train_models_for_data(X_train_scaled, y_train, X_val_scaled, y_val, base_models)\nresults.extend(global_results)\n\n# Determine if we should use global model for unknown clusters\nif cluster_based_modeling:\n    # Check if we have data with unknown cluster\n    X_train_unknown = X_train[X_train['project_cluster'] == -1]\n    y_train_unknown = y_train.loc[X_train_unknown.index]\n    \n    X_val_unknown = X_val[X_val['project_cluster'] == -1]\n    y_val_unknown = y_val.loc[X_val_unknown.index]\n    \n    if len(X_train_unknown) > 50 and len(X_val_unknown) > 20:\n        print(\"\\n=== Training models for Unknown Cluster ===\")\n        print(f\"Unknown cluster train size: {len(X_train_unknown)}, validation size: {len(X_val_unknown)}\")\n        \n        # Train models for unknown cluster\n        unknown_results = train_models_for_data(\n            X_train_unknown, y_train_unknown, \n            X_val_unknown, y_val_unknown,\n            base_models, -1\n        )\n        \n        # Add unknown cluster results to overall results\n        results.extend(unknown_results)\n        \n        # Store best model for unknown cluster\n        best_model_idx = max(range(len(unknown_results)), key=lambda i: unknown_results[i]['r2'])\n        cluster_models[-1] = unknown_results[best_model_idx]['model']\n    else:\n        print(\"\\nInsufficient data for Unknown Cluster. Will use global model.\")\n        \n        # Use best global model for unknown cluster\n        best_global_idx = max(range(len(global_results)), key=lambda i: global_results[i]['r2'])\n        cluster_models[-1] = global_results[best_global_idx]['model']\n\n# Summarize model performance\nsummary = pd.DataFrame([(r['name'], r.get('cluster', 'Global'), r['mae'], r['rmse'], r['r2'], r['mape'], r['train_time']) \n                       for r in results],\n                      columns=['Model', 'Cluster', 'MAE (hours)', 'RMSE (hours)', 'R²', 'MAPE (%)', 'Train Time (s)'])\n\n# Sort by R² (descending)\nsummary = summary.sort_values('R²', ascending=False)\nprint(\"\\nModel Performance Summary:\")\nprint(summary)\n\n# Save the summary\nsummary.to_csv(f'{results_dir}/model_performance_summary.csv', index=False)\n\n# Identify the best model\nbest_model_idx = summary['R²'].idxmax()\nbest_model_row = summary.loc[best_model_idx]\nbest_model_name = best_model_row['Model']\nbest_cluster = best_model_row['Cluster']\n\nprint(f\"\\nBest model overall: {best_model_name} with R² = {best_model_row['R²']:.4f}\")\n\n# Find the actual model object\nif best_cluster == 'Global':\n    best_model_idx = [i for i, r in enumerate(global_results) if r['name'] == best_model_name][0]\n    best_model = global_results[best_model_idx]['model']\nelse:\n    cluster_specific_results = [r for r in results if r.get('cluster') == best_cluster and r['name'] == best_model_name]\n    best_model = cluster_specific_results[0]['model']\n\n# Save the best model separately\nwith open(f'{results_dir}/best_model.pkl', 'wb') as f:\n    pickle.dump(best_model, f)\n\n# If using cluster-based approach, save the cluster models dictionary\nif cluster_based_modeling:\n    with open(f'{results_dir}/cluster_models.pkl', 'wb') as f:\n        pickle.dump(cluster_models, f)\n    print(f\"Saved {len(cluster_models)} cluster-specific models\")\n\nprint(\"Model training complete.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 10. Feature Importance Analysis\nprint(\"Analyzing feature importance...\")\n\n# Create function to analyze feature importance for a model\ndef analyze_feature_importance(model, feature_names, title=\"Feature Importance\", output_prefix=\"feature_importance\"):\n    # Extract feature importances if model supports it\n    if hasattr(model, 'feature_importances_'):\n        # Get feature importances\n        importances = model.feature_importances_\n        \n        # Create feature importance DataFrame\n        feature_importance = pd.DataFrame({\n            'Feature': feature_names,\n            'Importance': importances\n        })\n        \n        # Sort by importance\n        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n        \n        # Display top 20 most important features\n        print(f\"\\nTop 20 Most Important Features for {title}:\")\n        print(feature_importance.head(20))\n        \n        # Visualize feature importance\n        plt.figure(figsize=(12, 8))\n        # Plot top 20 features\n        top_features = feature_importance.head(20)\n        \n        # Create horizontal bar plot\n        plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])\n        plt.title(f'Top 20 Feature Importance - {title}')\n        plt.xlabel('Importance')\n        plt.tight_layout()\n        plt.savefig(f'{results_dir}/{output_prefix}.png')\n        plt.show()\n        \n        # Save feature importance\n        feature_importance.to_csv(f'{results_dir}/{output_prefix}.csv', index=False)\n        \n        # Group features by categories (if they follow a pattern in their naming)\n        if any('__' in feat for feat in feature_names):\n            print(f\"\\nFeature importance by category for {title}:\")\n            # Extract category from feature name (assuming format like 'category__feature_name')\n            feature_importance['Category'] = feature_importance['Feature'].apply(\n                lambda x: x.split('__')[0] if '__' in x else 'Other'\n            )\n            \n            # Aggregate importance by category\n            category_importance = feature_importance.groupby('Category')['Importance'].sum().reset_index()\n            category_importance = category_importance.sort_values('Importance', ascending=False)\n            \n            print(category_importance)\n            \n            # Visualize category importance\n            plt.figure(figsize=(10, 6))\n            plt.bar(category_importance['Category'], category_importance['Importance'])\n            plt.title(f'Feature Category Importance - {title}')\n            plt.xlabel('Category')\n            plt.ylabel('Importance')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.savefig(f'{results_dir}/{output_prefix}_by_category.png')\n            plt.show()\n            \n            # Save category importance\n            category_importance.to_csv(f'{results_dir}/{output_prefix}_by_category.csv', index=False)\n        \n        return feature_importance\n    else:\n        print(f\"The model does not provide feature importances.\")\n        return None\n\n# Analyze the best overall model\ntry:\n    with open(f'{results_dir}/best_model.pkl', 'rb') as f:\n        best_model = pickle.load(f)\n    \n    # Get best model name from model_performance_summary.csv\n    summary = pd.read_csv(f'{results_dir}/model_performance_summary.csv')\n    best_model_idx = summary['R²'].idxmax()\n    best_model_row = summary.loc[best_model_idx]\n    best_model_name = best_model_row['Model']\n    best_cluster = best_model_row['Cluster']\n    \n    # Analyze feature importance for best model\n    feature_names = X.columns\n    title = f\"Best Model ({best_model_name})\"\n    if best_cluster != 'Global':\n        title += f\" - Cluster {best_cluster}\"\n    \n    overall_importance = analyze_feature_importance(best_model, feature_names, title, \"best_model_feature_importance\")\n    \n    # If we have cluster-specific models, analyze each cluster\n    try:\n        with open(f'{results_dir}/cluster_models.pkl', 'rb') as f:\n            cluster_models = pickle.load(f)\n        \n        # Compare feature importance across clusters\n        if len(cluster_models) > 1:\n            print(\"\\nComparing feature importance across clusters...\")\n            \n            # Collect top 10 features from each cluster\n            top_features_by_cluster = {}\n            all_top_features = set()\n            \n            for cluster_id, model in cluster_models.items():\n                if hasattr(model, 'feature_importances_'):\n                    importances = model.feature_importances_\n                    imp_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n                    top10 = imp_df.sort_values('Importance', ascending=False).head(10)\n                    top_features_by_cluster[cluster_id] = top10\n                    all_top_features.update(top10['Feature'].tolist())\n                    \n                    # Analyze this cluster's model\n                    analyze_feature_importance(\n                        model, X.columns, \n                        f\"Cluster {cluster_id}\", \n                        f\"cluster_{cluster_id}_feature_importance\"\n                    )\n            \n            # Create comparison chart of top features across clusters\n            if all_top_features:\n                all_top_features = list(all_top_features)\n                comparison_data = []\n                \n                for feature in all_top_features:\n                    row = {'Feature': feature}\n                    for cluster_id, top10 in top_features_by_cluster.items():\n                        feat_importance = top10[top10['Feature'] == feature]['Importance'].values\n                        row[f'Cluster {cluster_id}'] = feat_importance[0] if len(feat_importance) > 0 else 0\n                    comparison_data.append(row)\n                \n                comparison_df = pd.DataFrame(comparison_data)\n                comparison_df = comparison_df.sort_values('Feature')\n                \n                # Save comparison\n                comparison_df.to_csv(f'{results_dir}/feature_importance_by_cluster.csv', index=False)\n                \n                # Visualize comparison (top 15)\n                top15_comparison = comparison_df.head(15)\n                plt.figure(figsize=(14, 8))\n                \n                cluster_cols = [col for col in comparison_df.columns if 'Cluster' in col]\n                \n                # Plot as grouped bar chart\n                bar_width = 0.8 / len(cluster_cols)\n                for i, col in enumerate(cluster_cols):\n                    plt.bar(\n                        [x + i * bar_width for x in range(len(top15_comparison))],\n                        top15_comparison[col],\n                        width=bar_width,\n                        label=col\n                    )\n                \n                plt.xlabel('Feature')\n                plt.ylabel('Importance')\n                plt.title('Feature Importance Comparison Across Clusters')\n                plt.xticks(\n                    [x + bar_width * (len(cluster_cols) - 1) / 2 for x in range(len(top15_comparison))],\n                    top15_comparison['Feature'],\n                    rotation=90\n                )\n                plt.legend()\n                plt.tight_layout()\n                plt.savefig(f'{results_dir}/feature_importance_cluster_comparison.png')\n                plt.show()\n                    \n    except Exception as e:\n        print(f\"Error analyzing cluster models: {e}\")\n            \nexcept Exception as e:\n    print(f\"Error loading models: {e}\")    \n    \n# If available, check if any feature categories are particularly important for certain clusters\nif 'overall_importance' in locals() and overall_importance is not None:\n    # Check which feature categories are most important\n    if 'Category' in overall_importance.columns:\n        top_categories = overall_importance.groupby('Category')['Importance'].sum().sort_values(ascending=False)\n        \n        print(\"\\nTop feature categories for task estimation:\")\n        print(top_categories.head(5))\n        \n        # Analyze correlation between top features and resolution time\n        try:\n            # Get the top 20 features\n            top_features = overall_importance['Feature'].head(20).tolist()\n            \n            # Get original target (non-log transformed)\n            if use_log_transform:\n                y_orig = np.expm1(y)\n            else:\n                y_orig = y\n                \n            # Create a copy of X with just top features\n            top_features_df = X[top_features].copy()\n            top_features_df['target'] = y_orig\n            \n            # Calculate correlations\n            correlations = top_features_df.corr()['target'].sort_values(ascending=False)\n            \n            print(\"\\nCorrelation of top features with resolution time:\")\n            print(correlations)\n            \n            # Plot correlation heatmap of top features\n            plt.figure(figsize=(10, 8))\n            corr_matrix = top_features_df.corr()\n            sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n            plt.title('Correlation Matrix of Top Features')\n            plt.tight_layout()\n            plt.savefig(f'{results_dir}/top_features_correlation.png')\n            plt.show()\n            \n        except Exception as e:\n            print(f\"Error analyzing correlations: {e}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 11. Final Model Evaluation on Test Set\nprint(\"Final model evaluation on test set...\")\n\n# Load the best model\nwith open(f'{results_dir}/best_model.pkl', 'rb') as f:\n    best_model = pickle.load(f)\n\n# Check if we're using cluster-based modeling\ntry:\n    with open(f'{results_dir}/cluster_models.pkl', 'rb') as f:\n        cluster_models = pickle.load(f)\n    cluster_based_modeling = True\n    print(f\"Using cluster-based modeling with {len(cluster_models)} clusters\")\nexcept:\n    cluster_based_modeling = False\n    print(\"Using global model for all test data\")\n\n# Define function to make predictions using cluster-based or global approach\ndef predict_with_appropriate_model(X_data):\n    if not cluster_based_modeling or 'project_cluster' not in X_data.columns:\n        # Use global model for all predictions\n        return best_model.predict(X_data)\n    else:\n        # Make predictions using appropriate cluster model\n        predictions = np.zeros(len(X_data))\n        \n        for cluster_id, model in cluster_models.items():\n            # Get data for this cluster\n            cluster_mask = X_data['project_cluster'] == cluster_id\n            if cluster_mask.sum() > 0:\n                # Predict with the appropriate model\n                predictions[cluster_mask] = model.predict(X_data[cluster_mask])\n        \n        return predictions\n\n# Apply to test data\nX_test_scaled = X_test  # No scaling needed - features already scaled\n\n# Make predictions using cluster-based approach if available\ny_pred = predict_with_appropriate_model(X_test_scaled)\n\n# Convert predictions back from log space\nif use_log_transform:\n    y_test_orig = np.expm1(y_test)\n    y_pred_orig = np.expm1(y_pred)\nelse:\n    y_test_orig = y_test\n    y_pred_orig = y_pred\n\n# Calculate metrics\nmae = mean_absolute_error(y_test_orig, y_pred_orig)\nrmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\nr2 = r2_score(y_test_orig, y_pred_orig)\nmedian_ae = np.median(np.abs(y_test_orig - y_pred_orig))\nmape = np.mean(np.abs((y_test_orig - y_pred_orig) / (y_test_orig + 1))) * 100  # Adding 1 to avoid division by zero\n\nprint(f\"\\nFinal Model Test Results:\")\nprint(f\"  MAE: {mae:.2f} hours\")\nprint(f\"  RMSE: {rmse:.2f} hours\")\nprint(f\"  MedianAE: {median_ae:.2f} hours\")\nprint(f\"  MAPE: {mape:.2f}%\")\nprint(f\"  R²: {r2:.4f}\")\n\n# Create a scatter plot of actual vs. predicted values\nplt.figure(figsize=(10, 8))\nplt.scatter(y_test_orig, y_pred_orig, alpha=0.5)\nplt.plot([0, y_test_orig.max()], [0, y_test_orig.max()], 'r--')\nplt.title('Final Model: Actual vs. Predicted')\nplt.xlabel('Actual Resolution Hours')\nplt.ylabel('Predicted Resolution Hours')\nplt.savefig(f'{results_dir}/final_model_predictions.png')\nplt.show()\n\n# If we have cluster models, evaluate each cluster separately\nif cluster_based_modeling and 'project_cluster' in X_test.columns:\n    print(\"\\nAnalyzing performance by cluster:\")\n    \n    cluster_metrics = []\n    for cluster_id in sorted(X_test['project_cluster'].unique()):\n        # Skip clusters with very few samples\n        cluster_mask = X_test['project_cluster'] == cluster_id\n        if cluster_mask.sum() < 20:\n            print(f\"  Skipping Cluster {cluster_id} - insufficient test data ({cluster_mask.sum()} samples)\")\n            continue\n            \n        # Get data for this cluster\n        X_test_cluster = X_test[cluster_mask]\n        y_test_cluster = y_test.loc[X_test_cluster.index]\n        \n        # Get appropriate model for this cluster\n        if cluster_id in cluster_models:\n            model = cluster_models[cluster_id]\n        else:\n            # Use global model for unknown clusters\n            model = best_model\n            \n        # Make predictions\n        y_pred_cluster = model.predict(X_test_cluster)\n        \n        # Convert back from log space\n        if use_log_transform:\n            y_test_cluster_orig = np.expm1(y_test_cluster)\n            y_pred_cluster_orig = np.expm1(y_pred_cluster)\n        else:\n            y_test_cluster_orig = y_test_cluster\n            y_pred_cluster_orig = y_pred_cluster\n            \n        # Calculate metrics\n        cluster_mae = mean_absolute_error(y_test_cluster_orig, y_pred_cluster_orig)\n        cluster_rmse = np.sqrt(mean_squared_error(y_test_cluster_orig, y_pred_cluster_orig))\n        cluster_r2 = r2_score(y_test_cluster_orig, y_pred_cluster_orig)\n        cluster_median_ae = np.median(np.abs(y_test_cluster_orig - y_pred_cluster_orig))\n        cluster_mape = np.mean(np.abs((y_test_cluster_orig - y_pred_cluster_orig) / (y_test_cluster_orig + 1))) * 100\n        \n        print(f\"\\n  Cluster {cluster_id} ({cluster_mask.sum()} test samples):\")\n        print(f\"    MAE: {cluster_mae:.2f} hours\")\n        print(f\"    RMSE: {cluster_rmse:.2f} hours\")\n        print(f\"    MedianAE: {cluster_median_ae:.2f} hours\")\n        print(f\"    MAPE: {cluster_mape:.2f}%\")\n        print(f\"    R²: {cluster_r2:.4f}\")\n        \n        # Store metrics\n        cluster_metrics.append({\n            'cluster_id': cluster_id,\n            'test_samples': cluster_mask.sum(),\n            'mae': cluster_mae,\n            'rmse': cluster_rmse,\n            'median_ae': cluster_median_ae,\n            'mape': cluster_mape,\n            'r2': cluster_r2\n        })\n        \n        # Plot cluster-specific actual vs. predicted\n        plt.figure(figsize=(8, 6))\n        plt.scatter(y_test_cluster_orig, y_pred_cluster_orig, alpha=0.5)\n        plt.plot([0, y_test_cluster_orig.max()], [0, y_test_cluster_orig.max()], 'r--')\n        plt.title(f'Cluster {cluster_id}: Actual vs. Predicted')\n        plt.xlabel('Actual Resolution Hours')\n        plt.ylabel('Predicted Resolution Hours')\n        plt.savefig(f'{results_dir}/cluster_{cluster_id}_final_predictions.png')\n        plt.show()\n    \n    # Save cluster metrics\n    if cluster_metrics:\n        cluster_metrics_df = pd.DataFrame(cluster_metrics)\n        cluster_metrics_df.to_csv(f'{results_dir}/cluster_performance_metrics.csv', index=False)\n        \n        # Plot comparative bar chart of R² by cluster\n        plt.figure(figsize=(10, 6))\n        cluster_ids = cluster_metrics_df['cluster_id'].tolist()\n        r2_values = cluster_metrics_df['r2'].tolist()\n        \n        plt.bar(cluster_ids, r2_values)\n        plt.axhline(y=r2, color='r', linestyle='--', label=f'Overall R² = {r2:.4f}')\n        plt.title('Model Performance (R²) by Project Cluster')\n        plt.xlabel('Cluster')\n        plt.ylabel('R²')\n        plt.ylim(0, 1)  # R² typically ranges from 0 to 1\n        plt.legend()\n        plt.savefig(f'{results_dir}/cluster_performance_comparison.png')\n        plt.show()\n\n# Save the test results\nfinal_metrics = {\n    'mae': mae,\n    'rmse': rmse,\n    'r2': r2,\n    'median_ae': median_ae,\n    'mape': mape,\n    'cluster_based': cluster_based_modeling\n}\n\nwith open(f'{results_dir}/final_test_metrics.json', 'w') as f:\n    import json\n    json.dump(final_metrics, f, indent=4)\n\nprint(\"\\nFinal model evaluation complete.\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 12. Example: Predict Effort for New Tasks\nprint(\"Example: Predicting effort for new tasks\")\n\n# Check if we're using cluster-based modeling\ntry:\n    with open(f'{results_dir}/cluster_models.pkl', 'rb') as f:\n        cluster_models = pickle.load(f)\n    cluster_based_modeling = True\n    print(f\"Using cluster-based prediction with {len(cluster_models)} clusters\")\nexcept:\n    cluster_based_modeling = False\n    with open(f'{results_dir}/best_model.pkl', 'rb') as f:\n        best_model = pickle.load(f)\n    print(\"Using global model for all predictions\")\n\n# Define function to predict task effort\ndef predict_task_effort(task_features, project_cluster=None):\n    \"\"\"\n    Predict resolution hours for a new task using the pre-scaled feature space.\n    \n    Parameters:\n    -----------\n    task_features : dict\n        Dictionary of task features (must match the feature names in the scaled dataset)\n    project_cluster : int, optional\n        Project cluster ID, if known\n    \n    Returns:\n    --------\n    float\n        Predicted resolution hours\n    \"\"\"\n    # Add cluster information if provided\n    if project_cluster is not None and cluster_based_modeling:\n        task_features['project_cluster'] = project_cluster\n    \n    # Convert to DataFrame\n    task_df = pd.DataFrame([task_features])\n    \n    # Ensure all required features are present\n    for col in X.columns:\n        if col not in task_df.columns:\n            task_df[col] = 0  # Default to 0 for missing features\n    \n    # Keep only the features used in training\n    task_df = task_df[X.columns]\n    \n    # Choose appropriate model based on cluster\n    if cluster_based_modeling and project_cluster is not None and project_cluster in cluster_models:\n        model = cluster_models[project_cluster]\n        print(f\"Using cluster {project_cluster} specific model\")\n    elif cluster_based_modeling and project_cluster is not None and -1 in cluster_models:\n        model = cluster_models[-1]  # Use unknown cluster model\n        print(\"Using unknown cluster model\")\n    else:\n        # Use best global model\n        with open(f'{results_dir}/best_model.pkl', 'rb') as f:\n            model = pickle.load(f)\n        print(\"Using global model\")\n    \n    # Make prediction (no scaling needed as we're already in the scaled feature space)\n    pred_log = model.predict(task_df)[0]\n    \n    # Transform prediction back from log space if needed\n    if use_log_transform:\n        prediction = np.expm1(pred_log)\n    else:\n        prediction = pred_log\n        \n    return prediction\n\n# Get a list of available project clusters\navailable_clusters = []\nif cluster_based_modeling:\n    available_clusters = sorted([cid for cid in cluster_models.keys() if cid != -1])\n    print(f\"Available project clusters: {available_clusters}\")\n\nprint(\"\\nWARNING: These are example predictions. In practice, you would need to:\")\nprint(\"1. Transform new raw data into the same scaled feature space\")\nprint(\"2. Ensure feature names match those in the training dataset exactly\")\nprint(\"3. Apply the same preprocessing steps used on the original data\\n\")\n\n# Get column names from dataset to create meaningful examples\n# Get the most influential features based on importance\ntop_feature_names = []\ntry:\n    with open(f'{results_dir}/best_model_feature_importance.csv', 'r') as f:\n        feature_importance = pd.read_csv(f)\n        top_feature_names = feature_importance.head(10)['Feature'].tolist()\nexcept:\n    # If no feature importance file, just use X columns\n    if hasattr(best_model, 'feature_importances_'):\n        importances = best_model.feature_importances_\n        feat_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n        top_feature_names = feat_importance.sort_values('Importance', ascending=False).head(10)['Feature'].tolist()\n    else:\n        top_feature_names = X.columns[:10].tolist()\n\nprint(f\"Top 10 influential features: {top_feature_names}\")\n\n# Create example predictions using values in the range of the training data\nexample_tasks = []\n\n# Task 1: Low complexity task (use values from the 25th percentile of the dataset for key features)\ntask1 = {}\nfor feat in top_feature_names:\n    task1[feat] = X[feat].quantile(0.25)\nexample_tasks.append({\"name\": \"Low complexity task\", \"features\": task1, \"cluster\": None})\n\n# Task 2: Medium complexity task (use median values)\ntask2 = {}\nfor feat in top_feature_names:\n    task2[feat] = X[feat].median()\nexample_tasks.append({\"name\": \"Medium complexity task\", \"features\": task2, \"cluster\": None})\n\n# Task 3: High complexity task (use values from the 75th percentile)\ntask3 = {}\nfor feat in top_feature_names:\n    task3[feat] = X[feat].quantile(0.75)\nexample_tasks.append({\"name\": \"High complexity task\", \"features\": task3, \"cluster\": None})\n\n# Add cluster-specific examples if clusters are available\nif available_clusters:\n    # Choose a cluster for demonstration\n    demo_cluster = available_clusters[0]\n    \n    # Get data specific to this cluster\n    if 'project_cluster' in X.columns:\n        X_cluster = X[X['project_cluster'] == demo_cluster]\n        \n        if len(X_cluster) > 0:\n            # Cluster-specific task\n            task_cluster = {}\n            for feat in top_feature_names:\n                task_cluster[feat] = X_cluster[feat].median()\n            \n            example_tasks.append({\n                \"name\": f\"Cluster {demo_cluster} specific task\", \n                \"features\": task_cluster,\n                \"cluster\": demo_cluster\n            })\n\nprint(\"\\nPredicted resolution times:\")\nfor task in example_tasks:\n    hours = predict_task_effort(task[\"features\"], task[\"cluster\"])\n    days = hours / 24\n    workdays = hours / 8  # Assuming 8-hour workdays\n    \n    cluster_info = f\" (Cluster {task['cluster']})\" if task[\"cluster\"] is not None else \"\"\n    print(f\"{task['name']}{cluster_info}: {hours:.2f} hours ({days:.2f} calendar days, {workdays:.2f} work days)\")\n\n# Save the prediction function\nwith open(f'{results_dir}/task_effort_predictor.py', 'w') as f:\n    f.write(\"\"\"\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n\ndef predict_task_effort(task_features, model_path='best_model.pkl', cluster_models_path='cluster_models.pkl', project_cluster=None):\n    \\\"\\\"\\\"\n    Predict resolution hours for a new task using pre-scaled features.\n    \n    Parameters:\n    -----------\n    task_features : dict\n        Dictionary of task features in the scaled feature space\n    model_path : str\n        Path to the trained model pickle file\n    cluster_models_path : str\n        Path to the cluster models pickle file (optional)\n    project_cluster : int, optional\n        Project cluster ID, if known\n    \n    Returns:\n    --------\n    float\n        Predicted resolution hours\n    \\\"\\\"\\\"\n    # Check if cluster models are available\n    cluster_based_modeling = False\n    if os.path.exists(cluster_models_path):\n        try:\n            with open(cluster_models_path, 'rb') as f:\n                cluster_models = pickle.load(f)\n            cluster_based_modeling = True\n        except:\n            cluster_models = {}\n    \n    # Add cluster information if provided\n    if project_cluster is not None and cluster_based_modeling:\n        task_features['project_cluster'] = project_cluster\n    \n    # Get list of expected features from model\n    with open('data_splits.pkl', 'rb') as f:\n        splits = pickle.load(f)\n        expected_features = splits['X_train'].columns\n        use_log_transform = splits.get('log_transform', True)\n    \n    # Convert to DataFrame\n    task_df = pd.DataFrame([task_features])\n    \n    # Ensure all required features are present\n    for col in expected_features:\n        if col not in task_df.columns:\n            task_df[col] = 0  # Default to 0 for missing features\n    \n    # Keep only the features used in training\n    task_df = task_df[expected_features]\n    \n    # Choose appropriate model based on cluster\n    if cluster_based_modeling and project_cluster is not None and project_cluster in cluster_models:\n        model = cluster_models[project_cluster]\n    elif cluster_based_modeling and -1 in cluster_models:\n        model = cluster_models[-1]  # Use unknown cluster model\n    else:\n        # Use best global model\n        with open(model_path, 'rb') as f:\n            model = pickle.load(f)\n    \n    # Make prediction (no scaling needed for pre-scaled features)\n    pred_log = model.predict(task_df)[0]\n    \n    # Transform prediction back from log space if needed\n    if use_log_transform:\n        prediction = np.expm1(pred_log)\n    else:\n        prediction = pred_log\n        \n    return prediction\n\ndef estimate_project_task_effort(project_features, task_features):\n    \\\"\\\"\\\"\n    Combine project-level and task-level features to estimate task effort.\n    \n    Parameters:\n    -----------\n    project_features : dict\n        Dictionary of project features to determine cluster and project context\n    task_features : dict\n        Dictionary of task-specific features\n    \n    Returns:\n    --------\n    dict\n        Dictionary containing prediction and confidence interval\n    \\\"\\\"\\\"\n    # This is a placeholder for a more sophisticated function that would:\n    # 1. Determine project cluster from project features\n    # 2. Adjust task features based on project context\n    # 3. Generate prediction with confidence intervals\n    \n    # For now, we'll do a simple implementation\n    all_features = {}\n    all_features.update(task_features)\n    \n    # Detect project cluster (placeholder)\n    project_cluster = None\n    \n    # Make prediction\n    hours = predict_task_effort(all_features, project_cluster=project_cluster)\n    \n    # Return results with placeholder confidence interval\n    return {\n        'prediction': hours,\n        'days': hours / 24,\n        'work_days': hours / 8,\n        'confidence_low': hours * 0.7,  # Placeholder - would be calculated from model\n        'confidence_high': hours * 1.3  # Placeholder - would be calculated from model\n    }\n\"\"\")\n\nprint(\"\\nPrediction function saved to task_effort_predictor.py\")\nprint(\"Model training and evaluation complete.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}