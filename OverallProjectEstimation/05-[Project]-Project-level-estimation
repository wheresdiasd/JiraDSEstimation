{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (971, 185)\n",
      "Missing values: 71310\n",
      "\n",
      "=== DATA TYPE ANALYSIS ===\n",
      "float64    169\n",
      "int64        9\n",
      "object       7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Non-numeric columns (7):\n",
      "  - project_key: object, 958 unique values\n",
      "  - project_name: object, 970 unique values\n",
      "  - project_start_date: object, 970 unique values\n",
      "  - project_latest_resolved_date: object, 969 unique values\n",
      "  - project_latest_update_date: object, 962 unique values\n",
      "  - repository: object, 7 unique values\n",
      "    Values: ['MariaDB' 'Hyperledger' 'MongoDB' 'Jira' 'Mojang' 'RedHat' 'Apache']\n",
      "  - source_file: object, 971 unique values\n",
      "\n",
      "=== HANDLING NON-NUMERIC COLUMNS ===\n",
      "Dropping identifier columns: ['project_key', 'project_name', 'project_start_date', 'project_latest_resolved_date', 'project_latest_update_date', 'repository', 'source_file']\n",
      "\n",
      "=== IDENTIFYING TARGET VARIABLES ===\n",
      "Target variables: ['avg_resolution_hours', 'median_resolution_hours', 'min_resolution_hours', 'max_resolution_hours', 'resolution_hours_std', 'total_resolution_hours']\n",
      "\n",
      "=== CATEGORIZING FEATURES ===\n",
      "\n",
      "=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\n",
      "Capped 181 lower and 3 upper outliers in project_id\n",
      "Capped 0 lower and 117 upper outliers in total_issues\n",
      "Capped 0 lower and 1 upper outliers in project_duration_days\n",
      "Capped 0 lower and 78 upper outliers in avg_resolution_hours\n",
      "Capped 0 lower and 126 upper outliers in median_resolution_hours\n",
      "Capped 3 lower and 169 upper outliers in min_resolution_hours\n",
      "Capped 0 lower and 2 upper outliers in max_resolution_hours\n",
      "Filled 18 missing values in resolution_hours_std with median: 5566.3916\n",
      "Capped 0 lower and 21 upper outliers in resolution_hours_std\n",
      "Capped 0 lower and 136 upper outliers in total_resolution_hours\n",
      "Filled 37 missing values in resolution_time_skewness with median: 2.8143\n",
      "Capped 0 lower and 23 upper outliers in resolution_time_skewness\n",
      "Filled 37 missing values in resolution_time_kurtosis with median: 8.5848\n",
      "Capped 0 lower and 69 upper outliers in resolution_time_kurtosis\n",
      "Capped 0 lower and 128 upper outliers in resolution_time_p25\n",
      "Capped 0 lower and 133 upper outliers in resolution_time_p75\n",
      "Capped 0 lower and 90 upper outliers in resolution_time_p90\n",
      "Capped 0 lower and 128 upper outliers in resolution_time_iqr\n",
      "Capped 0 lower and 32 upper outliers in pct_resolved_within_24h\n",
      "Capped 0 lower and 25 upper outliers in pct_resolved_within_week\n",
      "Capped 23 lower and 0 upper outliers in pct_resolved_within_month\n",
      "Capped 0 lower and 30 upper outliers in pct_issues_created_on_weekend\n",
      "Capped 0 lower and 40 upper outliers in pct_issues_resolved_on_weekend\n",
      "Capped 0 lower and 85 upper outliers in max_issues_per_month\n",
      "Capped 0 lower and 96 upper outliers in avg_issues_per_month\n",
      "Capped 0 lower and 6 upper outliers in months_with_activity\n",
      "Filled 7 missing values in issue_creation_volatility with median: 0.8586\n",
      "Capped 13 lower and 30 upper outliers in issue_creation_volatility\n",
      "Filled 156 missing values in priority_major_count with median: 258.0000\n",
      "Capped 0 lower and 92 upper outliers in priority_major_count\n",
      "Filled 156 missing values in priority_major_pct with median: 72.7273\n",
      "Capped 31 lower and 0 upper outliers in priority_major_pct\n",
      "Filled 196 missing values in priority_minor_count with median: 62.0000\n",
      "Capped 0 lower and 94 upper outliers in priority_minor_count\n",
      "Filled 196 missing values in priority_minor_pct with median: 16.1622\n",
      "Capped 0 lower and 21 upper outliers in priority_minor_pct\n",
      "Filled 264 missing values in priority_critical_count with median: 16.0000\n",
      "Capped 0 lower and 96 upper outliers in priority_critical_count\n",
      "Filled 264 missing values in priority_critical_pct with median: 3.5452\n",
      "Capped 0 lower and 42 upper outliers in priority_critical_pct\n",
      "Filled 336 missing values in priority_trivial_count with median: 12.0000\n",
      "Capped 0 lower and 87 upper outliers in priority_trivial_count\n",
      "Filled 336 missing values in priority_trivial_pct with median: 2.4590\n",
      "Capped 0 lower and 37 upper outliers in priority_trivial_pct\n",
      "Filled 34 missing values in type_bug_count with median: 153.0000\n",
      "Capped 0 lower and 131 upper outliers in type_bug_count\n",
      "Filled 34 missing values in type_bug_pct with median: 43.1579\n",
      "Filled 96 missing values in type_task_count with median: 30.0000\n",
      "Capped 0 lower and 123 upper outliers in type_task_count\n",
      "Filled 96 missing values in type_task_pct with median: 8.6022\n",
      "Capped 0 lower and 62 upper outliers in type_task_pct\n",
      "Filled 362 missing values in type_new_feature_count with median: 35.0000\n",
      "Capped 0 lower and 59 upper outliers in type_new_feature_count\n",
      "Filled 362 missing values in type_new_feature_pct with median: 8.2696\n",
      "Capped 0 lower and 31 upper outliers in type_new_feature_pct\n",
      "Filled 251 missing values in priority_major_type_task_count with median: 20.0000\n",
      "Capped 0 lower and 96 upper outliers in priority_major_type_task_count\n",
      "Filled 251 missing values in priority_major_type_task_avg_resolution_hours with median: 2001.7835\n",
      "Capped 0 lower and 68 upper outliers in priority_major_type_task_avg_resolution_hours\n",
      "Filled 206 missing values in priority_major_type_bug_count with median: 100.0000\n",
      "Capped 0 lower and 101 upper outliers in priority_major_type_bug_count\n",
      "Filled 206 missing values in priority_major_type_bug_avg_resolution_hours with median: 2162.1244\n",
      "Capped 0 lower and 60 upper outliers in priority_major_type_bug_avg_resolution_hours\n",
      "Filled 451 missing values in priority_major_type_new_feature_count with median: 19.0000\n",
      "Capped 0 lower and 58 upper outliers in priority_major_type_new_feature_count\n",
      "Filled 451 missing values in priority_major_type_new_feature_avg_resolution_hours with median: 4221.5146\n",
      "Capped 0 lower and 49 upper outliers in priority_major_type_new_feature_avg_resolution_hours\n",
      "Filled 432 missing values in priority_minor_type_task_count with median: 5.0000\n",
      "Capped 0 lower and 69 upper outliers in priority_minor_type_task_count\n",
      "Filled 432 missing values in priority_minor_type_task_avg_resolution_hours with median: 2301.8082\n",
      "Capped 0 lower and 48 upper outliers in priority_minor_type_task_avg_resolution_hours\n",
      "Filled 289 missing values in priority_minor_type_bug_count with median: 23.5000\n",
      "Capped 0 lower and 93 upper outliers in priority_minor_type_bug_count\n",
      "Filled 289 missing values in priority_minor_type_bug_avg_resolution_hours with median: 3192.1343\n",
      "Capped 0 lower and 41 upper outliers in priority_minor_type_bug_avg_resolution_hours\n",
      "Filled 331 missing values in priority_critical_type_bug_count with median: 12.0000\n",
      "Capped 0 lower and 82 upper outliers in priority_critical_type_bug_count\n",
      "Filled 331 missing values in priority_critical_type_bug_avg_resolution_hours with median: 2145.9509\n",
      "Capped 0 lower and 66 upper outliers in priority_critical_type_bug_avg_resolution_hours\n",
      "Filled 449 missing values in priority_trivial_type_bug_count with median: 6.0000\n",
      "Capped 0 lower and 67 upper outliers in priority_trivial_type_bug_count\n",
      "Filled 449 missing values in priority_trivial_type_bug_avg_resolution_hours with median: 1586.5446\n",
      "Capped 0 lower and 45 upper outliers in priority_trivial_type_bug_avg_resolution_hours\n",
      "Capped 0 lower and 39 upper outliers in avg_inward_links\n",
      "Capped 0 lower and 33 upper outliers in avg_outward_links\n",
      "Capped 0 lower and 37 upper outliers in avg_total_links\n",
      "Capped 0 lower and 159 upper outliers in total_inward_links\n",
      "Capped 0 lower and 160 upper outliers in total_outward_links\n",
      "Capped 0 lower and 163 upper outliers in total_links\n",
      "Capped 0 lower and 1 upper outliers in pct_issues_with_high_dependencies\n",
      "Capped 0 lower and 37 upper outliers in link_density\n",
      "Capped 0 lower and 113 upper outliers in num_resolved_issues\n",
      "Capped 55 lower and 0 upper outliers in pct_resolved_issues\n",
      "Filled 1 missing values in resolution_rate_per_day with median: 0.1182\n",
      "Capped 0 lower and 99 upper outliers in resolution_rate_per_day\n",
      "Filled 96 missing values in type_task_resolution_rate with median: 86.9565\n",
      "Capped 49 lower and 0 upper outliers in type_task_resolution_rate\n",
      "Filled 34 missing values in type_bug_resolution_rate with median: 87.1486\n",
      "Capped 67 lower and 0 upper outliers in type_bug_resolution_rate\n",
      "Filled 362 missing values in type_new_feature_resolution_rate with median: 75.0000\n",
      "Capped 38 lower and 0 upper outliers in type_new_feature_resolution_rate\n",
      "Capped 0 lower and 114 upper outliers in weekly_efficiency_ratio\n",
      "Capped 0 lower and 83 upper outliers in complexity_weighted_resolution_time\n",
      "Filled 165 missing values in high_to_low_priority_ratio with median: 4.4334\n",
      "Capped 0 lower and 69 upper outliers in high_to_low_priority_ratio\n",
      "Filled 34 missing values in bug_ratio with median: 0.7692\n",
      "Capped 0 lower and 5 upper outliers in bug_ratio\n",
      "Filled 1 missing values in creation_resolution_balance with median: 0.5387\n",
      "Filled 86 missing values in weighted_priority_score with median: 5.1793\n",
      "Capped 82 lower and 27 upper outliers in weighted_priority_score\n",
      "Capped 25 lower and 0 upper outliers in issue_type_entropy\n",
      "Filled 7 missing values in monthly_velocity with median: 5.1989\n",
      "Capped 0 lower and 101 upper outliers in monthly_velocity\n",
      "Filled 105 missing values in team_size_creators with median: 71.0000\n",
      "Capped 0 lower and 116 upper outliers in team_size_creators\n",
      "Filled 105 missing values in team_size_assignees with median: 0.0000\n",
      "Filled 105 missing values in team_size_combined with median: 71.0000\n",
      "Capped 0 lower and 116 upper outliers in team_size_combined\n",
      "Filled 105 missing values in core_team_ratio with median: 0.7327\n",
      "Capped 3 lower and 0 upper outliers in core_team_ratio\n",
      "Filled 105 missing values in creator_workload_gini with median: 0.6596\n",
      "Capped 19 lower and 0 upper outliers in creator_workload_gini\n",
      "Filled 105 missing values in creator_diversity with median: 4.2505\n",
      "Capped 0 lower and 11 upper outliers in creator_diversity\n",
      "Filled 105 missing values in avg_issues_per_creator with median: 5.4148\n",
      "Capped 0 lower and 57 upper outliers in avg_issues_per_creator\n",
      "Filled 105 missing values in top_creator_contribution with median: 0.2308\n",
      "Capped 0 lower and 24 upper outliers in top_creator_contribution\n",
      "Filled 112 missing values in creator_activity_variance with median: 209.6838\n",
      "Capped 0 lower and 110 upper outliers in creator_activity_variance\n",
      "Filled 112 missing values in creator_activity_std with median: 14.4805\n",
      "Capped 0 lower and 64 upper outliers in creator_activity_std\n",
      "Filled 124 missing values in team_type_specialization_index with median: 0.8873\n",
      "Capped 31 lower and 0 upper outliers in team_type_specialization_index\n",
      "Filled 105 missing values in bug_creation_ratio with median: 0.4379\n",
      "Filled 105 missing values in feature_request_ratio with median: 0.2941\n",
      "Capped 0 lower and 4 upper outliers in feature_request_ratio\n",
      "Filled 135 missing values in bug_creator_concentration with median: 0.5041\n",
      "Filled 135 missing values in bug_developer_ratio with median: 1.9115\n",
      "Capped 0 lower and 82 upper outliers in bug_developer_ratio\n",
      "Filled 245 missing values in feature_developer_ratio with median: 1.5074\n",
      "Capped 0 lower and 51 upper outliers in feature_developer_ratio\n",
      "Filled 105 missing values in weekend_activity_ratio with median: 0.2241\n",
      "Capped 0 lower and 40 upper outliers in weekend_activity_ratio\n",
      "Filled 110 missing values in creation_rate_stability with median: 0.1431\n",
      "Capped 26 lower and 12 upper outliers in creation_rate_stability\n",
      "Filled 110 missing values in avg_new_creators_per_month with median: 1.7791\n",
      "Capped 0 lower and 113 upper outliers in avg_new_creators_per_month\n",
      "Filled 110 missing values in creator_onboarding_volatility with median: 0.5612\n",
      "Capped 69 lower and 24 upper outliers in creator_onboarding_volatility\n",
      "Filled 105 missing values in creator_link_density_mean with median: 0.1969\n",
      "Capped 0 lower and 41 upper outliers in creator_link_density_mean\n",
      "Filled 105 missing values in creator_link_density_std with median: 0.4140\n",
      "Capped 0 lower and 24 upper outliers in creator_link_density_std\n",
      "Filled 198 missing values in complex_issue_distribution with median: 0.4167\n",
      "Filled 198 missing values in team_complexity_capacity with median: 70.4000\n",
      "Capped 0 lower and 101 upper outliers in team_complexity_capacity\n",
      "Filled 105 missing values in work_hour_creation_ratio with median: 0.4425\n",
      "Capped 8 lower and 14 upper outliers in work_hour_creation_ratio\n",
      "Filled 105 missing values in creation_hour_entropy with median: 4.1876\n",
      "Capped 79 lower and 0 upper outliers in creation_hour_entropy\n",
      "Filled 105 missing values in creator_resolution_time_variability with median: 1.4155\n",
      "Capped 28 lower and 20 upper outliers in creator_resolution_time_variability\n",
      "Filled 105 missing values in team_resolution_predictability with median: -0.8716\n",
      "Capped 22 lower and 0 upper outliers in team_resolution_predictability\n",
      "Filled 105 missing values in avg_creator_experience_days with median: 308.0861\n",
      "Capped 0 lower and 40 upper outliers in avg_creator_experience_days\n",
      "Filled 105 missing values in avg_creator_issue_count with median: 21.5082\n",
      "Capped 0 lower and 85 upper outliers in avg_creator_issue_count\n",
      "Filled 105 missing values in avg_creator_specialization with median: 0.7458\n",
      "Filled 268 missing values in priority_blocker_count with median: 10.0000\n",
      "Capped 0 lower and 97 upper outliers in priority_blocker_count\n",
      "Filled 268 missing values in priority_blocker_pct with median: 2.4450\n",
      "Capped 0 lower and 42 upper outliers in priority_blocker_pct\n",
      "Filled 746 missing values in type_epic_count with median: 19.0000\n",
      "Capped 0 lower and 15 upper outliers in type_epic_count\n",
      "Filled 746 missing values in type_epic_pct with median: 2.9007\n",
      "Capped 0 lower and 20 upper outliers in type_epic_pct\n",
      "Filled 871 missing values in priority_major_type_epic_count with median: 9.0000\n",
      "Capped 0 lower and 13 upper outliers in priority_major_type_epic_count\n",
      "Filled 871 missing values in priority_major_type_epic_avg_resolution_hours with median: 6215.5621\n",
      "Capped 0 lower and 5 upper outliers in priority_major_type_epic_avg_resolution_hours\n",
      "Filled 934 missing values in priority_minor_type_epic_count with median: 1.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_minor_type_epic_count\n",
      "Filled 934 missing values in priority_minor_type_epic_avg_resolution_hours with median: 6831.7895\n",
      "Capped 0 lower and 2 upper outliers in priority_minor_type_epic_avg_resolution_hours\n",
      "Filled 604 missing values in priority_trivial_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 46 upper outliers in priority_trivial_type_task_count\n",
      "Filled 604 missing values in priority_trivial_type_task_avg_resolution_hours with median: 861.1744\n",
      "Capped 0 lower and 30 upper outliers in priority_trivial_type_task_avg_resolution_hours\n",
      "Filled 346 missing values in priority_blocker_type_bug_count with median: 9.0000\n",
      "Capped 0 lower and 84 upper outliers in priority_blocker_type_bug_count\n",
      "Filled 346 missing values in priority_blocker_type_bug_avg_resolution_hours with median: 995.3753\n",
      "Capped 0 lower and 83 upper outliers in priority_blocker_type_bug_avg_resolution_hours\n",
      "Filled 640 missing values in priority_critical_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 46 upper outliers in priority_critical_type_task_count\n",
      "Filled 640 missing values in priority_critical_type_task_avg_resolution_hours with median: 1703.8983\n",
      "Capped 0 lower and 28 upper outliers in priority_critical_type_task_avg_resolution_hours\n",
      "Filled 746 missing values in type_epic_resolution_rate with median: 44.8276\n",
      "Filled 948 missing values in type_technical_task_count with median: 6.0000\n",
      "Capped 0 lower and 4 upper outliers in type_technical_task_count\n",
      "Filled 948 missing values in type_technical_task_pct with median: 0.4292\n",
      "Capped 0 lower and 2 upper outliers in type_technical_task_pct\n",
      "Filled 952 missing values in priority_major_type_technical_task_count with median: 6.0000\n",
      "Capped 0 lower and 4 upper outliers in priority_major_type_technical_task_count\n",
      "Filled 952 missing values in priority_major_type_technical_task_avg_resolution_hours with median: 712.1675\n",
      "Capped 0 lower and 2 upper outliers in priority_major_type_technical_task_avg_resolution_hours\n",
      "Filled 640 missing values in priority_blocker_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 48 upper outliers in priority_blocker_type_task_count\n",
      "Filled 640 missing values in priority_blocker_type_task_avg_resolution_hours with median: 984.4468\n",
      "Capped 0 lower and 41 upper outliers in priority_blocker_type_task_avg_resolution_hours\n",
      "Filled 948 missing values in type_technical_task_resolution_rate with median: 100.0000\n",
      "Capped 4 lower and 0 upper outliers in type_technical_task_resolution_rate\n",
      "Filled 330 missing values in type_improvement_count with median: 92.0000\n",
      "Capped 0 lower and 80 upper outliers in type_improvement_count\n",
      "Filled 330 missing values in type_improvement_pct with median: 26.2782\n",
      "Capped 0 lower and 12 upper outliers in type_improvement_pct\n",
      "Filled 930 missing values in type_release_count with median: 4.0000\n",
      "Capped 0 lower and 5 upper outliers in type_release_count\n",
      "Filled 930 missing values in type_release_pct with median: 0.3648\n",
      "Capped 0 lower and 4 upper outliers in type_release_pct\n",
      "Filled 794 missing values in type_story_count with median: 36.0000\n",
      "Capped 0 lower and 12 upper outliers in type_story_count\n",
      "Filled 794 missing values in type_story_pct with median: 8.6747\n",
      "Filled 890 missing values in priority_major_type_story_count with median: 10.0000\n",
      "Capped 0 lower and 6 upper outliers in priority_major_type_story_count\n",
      "Filled 890 missing values in priority_major_type_story_avg_resolution_hours with median: 2647.0180\n",
      "Capped 0 lower and 6 upper outliers in priority_major_type_story_avg_resolution_hours\n",
      "Filled 930 missing values in type_release_resolution_rate with median: 100.0000\n",
      "Capped 7 lower and 0 upper outliers in type_release_resolution_rate\n",
      "Filled 794 missing values in type_story_resolution_rate with median: 71.4286\n",
      "Filled 330 missing values in type_improvement_resolution_rate with median: 80.0000\n",
      "Capped 39 lower and 0 upper outliers in type_improvement_resolution_rate\n",
      "Filled 564 missing values in priority_minor_type_new_feature_count with median: 6.0000\n",
      "Capped 0 lower and 58 upper outliers in priority_minor_type_new_feature_count\n",
      "Filled 564 missing values in priority_minor_type_new_feature_avg_resolution_hours with median: 5050.6199\n",
      "Capped 0 lower and 34 upper outliers in priority_minor_type_new_feature_avg_resolution_hours\n",
      "Filled 786 missing values in priority_critical_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 18 upper outliers in priority_critical_type_new_feature_count\n",
      "Filled 786 missing values in priority_critical_type_new_feature_avg_resolution_hours with median: 3163.5037\n",
      "Capped 0 lower and 17 upper outliers in priority_critical_type_new_feature_avg_resolution_hours\n",
      "Filled 862 missing values in priority_medium_count with median: 83.0000\n",
      "Capped 0 lower and 13 upper outliers in priority_medium_count\n",
      "Filled 862 missing values in priority_medium_pct with median: 20.9091\n",
      "Filled 853 missing values in priority_high_count with median: 48.5000\n",
      "Capped 0 lower and 14 upper outliers in priority_high_count\n",
      "Filled 853 missing values in priority_high_pct with median: 9.1847\n",
      "Capped 0 lower and 4 upper outliers in priority_high_pct\n",
      "Filled 861 missing values in priority_low_count with median: 23.0000\n",
      "Capped 0 lower and 24 upper outliers in priority_low_count\n",
      "Filled 861 missing values in priority_low_pct with median: 5.8824\n",
      "Capped 0 lower and 2 upper outliers in priority_low_pct\n",
      "Filled 879 missing values in type_documentation_count with median: 6.5000\n",
      "Capped 0 lower and 12 upper outliers in type_documentation_count\n",
      "Filled 879 missing values in type_documentation_pct with median: 0.8067\n",
      "Capped 0 lower and 6 upper outliers in type_documentation_pct\n",
      "Filled 889 missing values in priority_medium_type_bug_count with median: 31.5000\n",
      "Capped 0 lower and 14 upper outliers in priority_medium_type_bug_count\n",
      "Filled 889 missing values in priority_medium_type_bug_avg_resolution_hours with median: 1549.7276\n",
      "Capped 0 lower and 4 upper outliers in priority_medium_type_bug_avg_resolution_hours\n",
      "Filled 915 missing values in priority_medium_type_task_count with median: 10.0000\n",
      "Capped 0 lower and 6 upper outliers in priority_medium_type_task_count\n",
      "Filled 915 missing values in priority_medium_type_task_avg_resolution_hours with median: 1238.7529\n",
      "Capped 0 lower and 2 upper outliers in priority_medium_type_task_avg_resolution_hours\n",
      "Filled 892 missing values in priority_high_type_bug_count with median: 14.0000\n",
      "Capped 0 lower and 10 upper outliers in priority_high_type_bug_count\n",
      "Filled 892 missing values in priority_high_type_bug_avg_resolution_hours with median: 1072.9892\n",
      "Capped 0 lower and 2 upper outliers in priority_high_type_bug_avg_resolution_hours\n",
      "Filled 919 missing values in priority_high_type_task_count with median: 7.5000\n",
      "Capped 0 lower and 5 upper outliers in priority_high_type_task_count\n",
      "Filled 919 missing values in priority_high_type_task_avg_resolution_hours with median: 1167.7849\n",
      "Capped 0 lower and 3 upper outliers in priority_high_type_task_avg_resolution_hours\n",
      "Filled 908 missing values in priority_low_type_bug_count with median: 24.0000\n",
      "Capped 0 lower and 11 upper outliers in priority_low_type_bug_count\n",
      "Filled 908 missing values in priority_low_type_bug_avg_resolution_hours with median: 2982.1633\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_bug_avg_resolution_hours\n",
      "Filled 933 missing values in priority_low_type_task_count with median: 3.0000\n",
      "Capped 0 lower and 5 upper outliers in priority_low_type_task_count\n",
      "Filled 933 missing values in priority_low_type_task_avg_resolution_hours with median: 1652.2908\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_task_avg_resolution_hours\n",
      "Filled 879 missing values in type_documentation_resolution_rate with median: 77.7971\n",
      "Capped 5 lower and 0 upper outliers in type_documentation_resolution_rate\n",
      "Filled 956 missing values in priority_high_type_improvement_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_improvement_count\n",
      "Filled 956 missing values in priority_high_type_improvement_avg_resolution_hours with median: 330.3733\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_improvement_avg_resolution_hours\n",
      "Filled 960 missing values in priority_high_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_new_feature_count\n",
      "Filled 960 missing values in priority_high_type_new_feature_avg_resolution_hours with median: 2542.1649\n",
      "Capped 0 lower and 1 upper outliers in priority_high_type_new_feature_avg_resolution_hours\n",
      "Filled 961 missing values in priority_low_type_improvement_count with median: 10.0000\n",
      "Capped 0 lower and 2 upper outliers in priority_low_type_improvement_count\n",
      "Filled 961 missing values in priority_low_type_improvement_avg_resolution_hours with median: 2729.6553\n",
      "Filled 960 missing values in priority_low_type_new_feature_count with median: 2.0000\n",
      "Capped 0 lower and 1 upper outliers in priority_low_type_new_feature_count\n",
      "Filled 960 missing values in priority_low_type_new_feature_avg_resolution_hours with median: 3787.2931\n",
      "Filled 873 missing values in type_question_count with median: 5.0000\n",
      "Capped 0 lower and 13 upper outliers in type_question_count\n",
      "Filled 873 missing values in type_question_pct with median: 0.8336\n",
      "Capped 0 lower and 10 upper outliers in type_question_pct\n",
      "Filled 873 missing values in type_question_resolution_rate with median: 77.7778\n",
      "Filled 946 missing values in priority_normal_count with median: 18.0000\n",
      "Capped 0 lower and 4 upper outliers in priority_normal_count\n",
      "Filled 946 missing values in priority_normal_pct with median: 12.4189\n",
      "Filled 955 missing values in priority_normal_type_bug_count with median: 5.0000\n",
      "Capped 0 lower and 3 upper outliers in priority_normal_type_bug_count\n",
      "Filled 955 missing values in priority_normal_type_bug_avg_resolution_hours with median: 520.7106\n",
      "Capped 0 lower and 1 upper outliers in priority_normal_type_bug_avg_resolution_hours\n",
      "\n",
      "=== SEPARATING FEATURES FROM TARGETS ===\n",
      "\n",
      "=== CREATING FEATURE PREPROCESSING PIPELINE ===\n",
      "Scaling features (target variables will remain unchanged)...\n",
      "\n",
      "=== SAVING RESULTS ===\n",
      "1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned.csv\n",
      "2. Saved feature preprocessor: jira_feature_preprocessor.pkl\n",
      "3. Saved scaled features with original targets: common_features_scaled_with_original_targets.csv\n",
      "\n",
      "=== TARGET VARIABLES (PRESERVED IN ORIGINAL UNITS) ===\n",
      "avg_resolution_hours: Mean = 3930.69, Median = 2614.45, Min = 0.01, Max = 11742.86\n",
      "median_resolution_hours: Mean = 668.63, Median = 402.17, Min = 0.00, Max = 2116.74\n",
      "min_resolution_hours: Mean = 0.02, Median = 0.01, Min = -0.04, Max = 0.07\n",
      "max_resolution_hours: Mean = 45387.55, Median = 36556.54, Min = 0.01, Max = 154639.88\n",
      "resolution_hours_std: Mean = 7299.05, Median = 5566.39, Min = 0.00, Max = 23406.80\n",
      "total_resolution_hours: Mean = 3205638.91, Median = 880454.55, Min = 0.01, Max = 11880816.27\n",
      "\n",
      "=== PROCESSING COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "results_dir = 'prepared_processed_data'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "\n",
    "# 2. Basic exploration\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "# 3. Check data types and identify non-numeric columns\n",
    "print(\"\\n=== DATA TYPE ANALYSIS ===\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "non_numeric_columns = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nNon-numeric columns ({len(non_numeric_columns)}):\")\n",
    "for col in non_numeric_columns:\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"  - {col}: {df[col].dtype}, {unique_values} unique values\")\n",
    "    if unique_values < 10:  # Show examples if not too many\n",
    "        print(f\"    Values: {df[col].unique()}\")\n",
    "\n",
    "# 4. Handle non-numeric columns\n",
    "print(\"\\n=== HANDLING NON-NUMERIC COLUMNS ===\")\n",
    "# Identify columns to drop (identifiers) and columns to encode (categorical)\n",
    "cols_to_drop = []\n",
    "cols_to_encode = []\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    # Check if it's an identifier column\n",
    "    if any(keyword in col.lower() for keyword in ['project', 'key', 'name', 'id', 'source', 'repository', 'file']):\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        # Must be a categorical column\n",
    "        cols_to_encode.append(col)\n",
    "\n",
    "# Drop identifier columns\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping identifier columns: {cols_to_drop}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "if cols_to_encode:\n",
    "    print(f\"Encoding categorical columns: {cols_to_encode}\")\n",
    "    # Use pandas get_dummies for one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)\n",
    "    print(f\"Expanded to {df.shape[1]} columns after encoding\")\n",
    "\n",
    "# 5. Function for interquartile-based imputation and outlier capping\n",
    "def impute_and_cap_using_iqr(df, columns=None, fill_missing=True, cap_outliers=True, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Impute missing values and cap outliers using interquartile range method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    columns : list or None\n",
    "        Columns to process. If None, will process all numeric columns.\n",
    "    fill_missing : bool\n",
    "        Whether to fill missing values with median\n",
    "    cap_outliers : bool\n",
    "        Whether to cap outliers based on IQR\n",
    "    iqr_multiplier : float\n",
    "        Multiplier for IQR to define outlier boundaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate quartiles and IQR\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define lower and upper bounds\n",
    "        lower_bound = q1 - iqr_multiplier * iqr\n",
    "        upper_bound = q3 + iqr_multiplier * iqr\n",
    "        \n",
    "        # Fill missing values with median if requested\n",
    "        if fill_missing and df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"Filled {df[col].isna().sum()} missing values in {col} with median: {median_val:.4f}\")\n",
    "        \n",
    "        # Cap outliers if requested\n",
    "        if cap_outliers:\n",
    "            # Count outliers before capping\n",
    "            n_lower_outliers = (df_clean[col] < lower_bound).sum()\n",
    "            n_upper_outliers = (df_clean[col] > upper_bound).sum()\n",
    "            \n",
    "            if n_lower_outliers > 0 or n_upper_outliers > 0:\n",
    "                # Apply capping\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"Capped {n_lower_outliers} lower and {n_upper_outliers} upper outliers in {col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 6. Identify potential target variables\n",
    "print(\"\\n=== IDENTIFYING TARGET VARIABLES ===\")\n",
    "potential_target_variables = [\n",
    "    'avg_resolution_hours',\n",
    "    'median_resolution_hours',\n",
    "    'min_resolution_hours',\n",
    "    'max_resolution_hours',\n",
    "    'resolution_hours_std',\n",
    "    'total_resolution_hours'\n",
    "]\n",
    "\n",
    "# Check if all target variables exist\n",
    "missing_targets = [var for var in potential_target_variables if var not in df.columns]\n",
    "if missing_targets:\n",
    "    print(f\"Warning: Some target variables are missing: {missing_targets}\")\n",
    "    # Update the list to only include existing columns\n",
    "    potential_target_variables = [var for var in potential_target_variables if var in df.columns]\n",
    "\n",
    "print(f\"Target variables: {potential_target_variables}\")\n",
    "\n",
    "# 7. Categorize features based on their type and expected distribution\n",
    "print(\"\\n=== CATEGORIZING FEATURES ===\")\n",
    "\n",
    "# Time-based features (excluding target variables)\n",
    "time_features = [\n",
    "    'resolution_time_p25', \n",
    "    'resolution_time_p75', \n",
    "    'resolution_time_p90', \n",
    "    'resolution_time_iqr', \n",
    "    'project_duration_days'\n",
    "]\n",
    "\n",
    "# Percentage features\n",
    "pct_features = [\n",
    "    'pct_resolved_within_24h', 'pct_resolved_within_week', 'pct_resolved_within_month',\n",
    "    'pct_issues_created_on_weekend', 'pct_issues_resolved_on_weekend',\n",
    "    'pct_resolved_issues', 'pct_issues_with_high_dependencies',\n",
    "    'priority_critical_pct', 'priority_blocker_pct', 'type_bug_pct', \n",
    "    'type_task_pct', 'type_new_feature_pct', 'type_epic_pct', \n",
    "    'type_improvement_pct', 'type_story_pct', 'type_documentation_pct',\n",
    "    'priority_high_pct', 'priority_low_pct'\n",
    "]\n",
    "\n",
    "# Count features\n",
    "count_features = [\n",
    "    'total_issues', 'max_issues_per_month', 'months_with_activity',\n",
    "    'priority_critical_count', 'type_bug_count', 'type_task_count', \n",
    "    'type_new_feature_count', 'priority_critical_type_bug_count',\n",
    "    'total_inward_links', 'total_outward_links', 'total_links',\n",
    "    'num_resolved_issues', 'priority_blocker_count', 'type_epic_count',\n",
    "    'priority_blocker_type_bug_count', 'type_improvement_count',\n",
    "    'type_story_count', 'priority_high_count', 'priority_low_count',\n",
    "    'type_documentation_count', 'priority_high_type_bug_count',\n",
    "    'priority_low_type_bug_count'\n",
    "]\n",
    "\n",
    "# Statistical/rate features\n",
    "stat_features = [\n",
    "    'resolution_time_skewness', 'resolution_time_kurtosis',\n",
    "    'avg_issues_per_month', 'issue_creation_volatility',\n",
    "    'resolution_rate_per_day', 'type_task_resolution_rate', \n",
    "    'type_bug_resolution_rate', 'type_new_feature_resolution_rate',\n",
    "    'weekly_efficiency_ratio', 'complexity_weighted_resolution_time',\n",
    "    'high_to_low_priority_ratio', 'bug_ratio', 'creation_resolution_balance',\n",
    "    'weighted_priority_score', 'issue_type_entropy', 'monthly_velocity',\n",
    "    'type_epic_resolution_rate', 'type_story_resolution_rate',\n",
    "    'type_improvement_resolution_rate', 'type_documentation_resolution_rate'\n",
    "]\n",
    "\n",
    "# Link-related features\n",
    "link_features = [\n",
    "    'avg_inward_links', 'avg_outward_links', 'avg_total_links',\n",
    "    'link_density'\n",
    "]\n",
    "\n",
    "# Resolution hour features - these will be excluded from scaling as they're related to target variables\n",
    "avg_resolution_features = [\n",
    "    'priority_critical_type_bug_avg_resolution_hours',\n",
    "    'priority_blocker_type_bug_avg_resolution_hours',\n",
    "    'priority_high_type_bug_avg_resolution_hours',\n",
    "    'priority_low_type_bug_avg_resolution_hours'\n",
    "]\n",
    "\n",
    "# Combine all feature categories\n",
    "feature_categories = {\n",
    "    'time_features': time_features,\n",
    "    'pct_features': pct_features,\n",
    "    'count_features': count_features,\n",
    "    'stat_features': stat_features,\n",
    "    'link_features': link_features,\n",
    "    'avg_resolution_features': avg_resolution_features\n",
    "}\n",
    "\n",
    "# Filter to keep only features that exist in the dataframe\n",
    "for category, features in feature_categories.items():\n",
    "    filtered_features = [f for f in features if f in df.columns]\n",
    "    if len(filtered_features) != len(features):\n",
    "        print(f\"Warning: Some features in {category} are missing from the dataset\")\n",
    "        print(f\"  Missing: {set(features) - set(filtered_features)}\")\n",
    "    feature_categories[category] = filtered_features\n",
    "\n",
    "# Get a flat list of all features being processed\n",
    "all_features_to_process = []\n",
    "for category, features in feature_categories.items():\n",
    "    all_features_to_process.extend(features)\n",
    "\n",
    "# 8. Apply interquartile-based imputation to all numeric features\n",
    "print(\"\\n=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\")\n",
    "numeric_columns = df.select_dtypes(include=np.number).columns.tolist()\n",
    "df_clean = impute_and_cap_using_iqr(df, columns=numeric_columns, fill_missing=True, cap_outliers=True)\n",
    "\n",
    "# 9. Separate features from targets\n",
    "print(\"\\n=== SEPARATING FEATURES FROM TARGETS ===\")\n",
    "# Include all columns except the target variables\n",
    "feature_columns = [col for col in df_clean.columns if col not in potential_target_variables]\n",
    "feature_df = df_clean[feature_columns]\n",
    "target_df = df_clean[potential_target_variables]\n",
    "\n",
    "# 10. Create the column transformer for feature scaling\n",
    "print(\"\\n=== CREATING FEATURE PREPROCESSING PIPELINE ===\")\n",
    "# Get the actual feature lists based on what's available in the dataframe\n",
    "time_feats = feature_categories['time_features']\n",
    "pct_feats = feature_categories['pct_features']\n",
    "count_feats = feature_categories['count_features']\n",
    "stat_feats = feature_categories['stat_features']\n",
    "link_feats = feature_categories['link_features']\n",
    "\n",
    "# Check if any feature categories are empty\n",
    "for category, features in {'time': time_feats, 'pct': pct_feats, 'count': count_feats, \n",
    "                          'stat': stat_feats, 'link': link_feats}.items():\n",
    "    if not features:\n",
    "        print(f\"Warning: No {category} features available for scaling\")\n",
    "\n",
    "# Create transformers only for non-empty feature lists\n",
    "transformers = []\n",
    "if time_feats:\n",
    "    transformers.append(('time_power', PowerTransformer(method='yeo-johnson'), time_feats))\n",
    "if pct_feats:\n",
    "    transformers.append(('pct_minmax', MinMaxScaler(), pct_feats))\n",
    "if count_feats:\n",
    "    transformers.append(('count_std', StandardScaler(), count_feats))\n",
    "if stat_feats:\n",
    "    transformers.append(('stat_robust', RobustScaler(), stat_feats))\n",
    "if link_feats:\n",
    "    transformers.append(('link_std', StandardScaler(), link_feats))\n",
    "\n",
    "feature_preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough'  # Non-numeric columns will pass through\n",
    ")\n",
    "\n",
    "# 11. Apply the preprocessing to features only\n",
    "print(\"Scaling features (target variables will remain unchanged)...\")\n",
    "# Check if feature_df has any data\n",
    "if feature_df.empty:\n",
    "    print(\"Error: No features available for scaling!\")\n",
    "else:\n",
    "    scaled_features = feature_preprocessor.fit_transform(feature_df)\n",
    "\n",
    "    # 12. Create DataFrame with scaled features\n",
    "    feature_names = feature_preprocessor.get_feature_names_out()\n",
    "    df_scaled_features = pd.DataFrame(scaled_features, columns=feature_names, index=df_clean.index)\n",
    "\n",
    "    # 13. Check if we've introduced any NaN values during scaling\n",
    "    nan_count = df_scaled_features.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: {nan_count} NaN values introduced during scaling\")\n",
    "        # Replace NaNs with 0\n",
    "        df_scaled_features = df_scaled_features.fillna(0)\n",
    "        print(\"NaN values have been replaced with 0\")\n",
    "\n",
    "    # 14. Combine scaled features with original target variables\n",
    "    final_df = pd.concat([df_scaled_features, target_df], axis=1)\n",
    "\n",
    "    # 15. Save the results\n",
    "    print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "    # Save the clean but unscaled dataset (after IQR imputation and outlier capping)\n",
    "    df_clean.to_csv(f'{results_dir}/common_features_iqr_cleaned.csv', index=False)\n",
    "    print(\"1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned.csv\")\n",
    "\n",
    "    # Save the preprocessor for later use\n",
    "    with open(f'{results_dir}/jira_feature_preprocessor.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_preprocessor, f)\n",
    "    print(\"2. Saved feature preprocessor: jira_feature_preprocessor.pkl\")\n",
    "\n",
    "    # Save the dataset with scaled features and original target variables\n",
    "    final_df.to_csv(f'{results_dir}/common_features_scaled_with_original_targets.csv', index=False)\n",
    "    print(\"3. Saved scaled features with original targets: common_features_scaled_with_original_targets.csv\")\n",
    "\n",
    "    # 16. Print information about target variables (not scaled)\n",
    "    print(\"\\n=== TARGET VARIABLES (PRESERVED IN ORIGINAL UNITS) ===\")\n",
    "    for target in potential_target_variables:\n",
    "        print(f\"{target}: Mean = {df_clean[target].mean():.2f}, Median = {df_clean[target].median():.2f}, Min = {df_clean[target].min():.2f}, Max = {df_clean[target].max():.2f}\")\n",
    "\n",
    "print(\"\\n=== PROCESSING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./prepared_processed_data/common_features_scaled_with_original_targets.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = 'resolution_hours_analysis'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. TARGET VARIABLE VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "target = 'total_resolution_hours'\n",
    "df['log_total_resolution_hours'] = np.log1p(df[target])\n",
    "\n",
    "# Create distribution plots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[target], kde=True, color='steelblue')\n",
    "plt.title('Distribution of Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['log_total_resolution_hours'], kde=True, color='forestgreen')\n",
    "plt.title('Distribution of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_distribution.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create boxplots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[target], color='steelblue')\n",
    "plt.title('Boxplot of Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['log_total_resolution_hours'], color='forestgreen')\n",
    "plt.title('Boxplot of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_boxplots.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. PLANNING-TIME FEATURES ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define features available at planning time\n",
    "planning_features = [\n",
    "    # Project scope indicators\n",
    "    'total_issues',                    # Estimated during planning\n",
    "    \n",
    "    # Project composition estimates\n",
    "    'priority_critical_pct',           # Expected critical issues\n",
    "    'priority_high_pct',               # Expected high priority issues\n",
    "    'priority_medium_pct',             # Expected medium priority issues\n",
    "    'priority_low_pct',                # Expected low priority issues\n",
    "    'priority_blocker_pct',            # Expected blocker issues\n",
    "    \n",
    "    # Issue type distribution (estimated from similar projects)\n",
    "    'type_bug_pct',                    # Expected bug percentage\n",
    "    'type_task_pct',                   # Expected task percentage\n",
    "    'type_new_feature_pct',            # Expected feature work\n",
    "    'type_improvement_pct',            # Expected improvements\n",
    "    'type_documentation_pct',          # Expected documentation work\n",
    "    \n",
    "    # Team composition\n",
    "    'team_size_creators',              # Planned team size\n",
    "    'team_size_assignees',             # Planned assignees\n",
    "    'team_size_combined',              # Overall team size\n",
    "    \n",
    "    # Complexity indicators\n",
    "    'weighted_priority_score',         # Expected priority complexity\n",
    "    'issue_type_entropy',              # Expected variety of issues\n",
    "    \n",
    "    # Historical indicators that could be estimated\n",
    "    'high_to_low_priority_ratio',      # Expected priority distribution\n",
    "    'bug_ratio',                       # Expected bug ratio\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataframe\n",
    "planning_features = [f for f in planning_features if f in df.columns]\n",
    "\n",
    "# Create a correlation matrix of planning features with the target\n",
    "planning_correlations = df[planning_features].corrwith(df['log_total_resolution_hours'])\n",
    "planning_correlations = planning_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=planning_correlations.values, y=planning_correlations.index, palette='viridis')\n",
    "plt.title('Planning-Time Features: Correlation with Log-Transformed Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/planning_feature_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plots for top features\n",
    "top_features = planning_correlations.abs().sort_values(ascending=False).head(6).index\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "               scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = valid_data[feature].corr(valid_data['log_total_resolution_hours'])\n",
    "    \n",
    "    plt.title(f'{feature} vs. Log-Transformed Resolution Hours', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=10)\n",
    "    plt.annotate(f'r = {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/top_feature_relationships.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. FEATURE CATEGORY ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Project Scope': ['total_issues', 'project_duration_days'],\n",
    "    'Priority Distribution': [col for col in planning_features if 'priority_' in col and '_pct' in col],\n",
    "    'Issue Types': [col for col in planning_features if 'type_' in col and '_pct' in col],\n",
    "    'Team Composition': [col for col in planning_features if 'team_' in col],\n",
    "    'Complexity Metrics': ['weighted_priority_score', 'issue_type_entropy', 'high_to_low_priority_ratio', 'bug_ratio']\n",
    "}\n",
    "\n",
    "# Calculate average correlation by category\n",
    "category_correlations = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Get features that exist in the dataframe\n",
    "    existing_features = [f for f in features if f in df.columns]\n",
    "    if existing_features:\n",
    "        # Get absolute correlations\n",
    "        abs_corrs = df[existing_features].corrwith(df['log_total_resolution_hours']).abs()\n",
    "        category_correlations[category] = abs_corrs.mean()\n",
    "\n",
    "# Visualize category correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = list(category_correlations.keys())\n",
    "correlations = list(category_correlations.values())\n",
    "\n",
    "# Sort by correlation\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "categories = [categories[i] for i in sorted_indices]\n",
    "correlations = [correlations[i] for i in sorted_indices]\n",
    "\n",
    "sns.barplot(x=categories, y=correlations, palette='viridis')\n",
    "plt.title('Average Correlation by Feature Category', fontsize=14)\n",
    "plt.xlabel('Feature Category', fontsize=12)\n",
    "plt.ylabel('Average Absolute Correlation', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/category_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. PRIORITY & ISSUE TYPE IMPACT VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create visualizations for priority distribution impact\n",
    "priority_features = [col for col in df.columns if 'priority_' in col and '_pct' in col]\n",
    "priority_features = [f for f in priority_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if priority_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(priority_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/priority_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations for issue type impact\n",
    "issue_type_features = [col for col in df.columns if 'type_' in col and '_pct' in col]\n",
    "issue_type_features = [f for f in issue_type_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if issue_type_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(issue_type_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/issue_type_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. MODEL TRAINING & FEATURE IMPORTANCE\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Prepare the data for modeling\n",
    "df_planning = df[planning_features + ['log_total_resolution_hours']].copy()\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_planning[planning_features] = imputer.fit_transform(df_planning[planning_features])\n",
    "\n",
    "# Check for and replace infinite values\n",
    "for col in planning_features:\n",
    "    mask = np.isinf(df_planning[col])\n",
    "    if mask.any():\n",
    "        print(f\"Replacing {mask.sum()} infinite values in {col}\")\n",
    "        df_planning.loc[mask, col] = df_planning[col].median()\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_planning[planning_features]\n",
    "y = df_planning['log_total_resolution_hours']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest - RMSE: {rf_rmse:.4f}, R²: {rf_r2:.4f}\")\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': planning_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
    "plt.title('Feature Importance for Predicting Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/feature_importance.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. ACTUAL VS PREDICTED VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create scatter plot of actual vs predicted\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Log scale comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, rf_pred, alpha=0.6, c='steelblue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title('Actual vs Predicted: Log Scale', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Predicted Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.annotate(f'R² = {rf_r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "            fontsize=12, fontweight='bold')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Original scale comparison \n",
    "plt.subplot(1, 2, 2)\n",
    "y_test_original = np.expm1(y_test)\n",
    "rf_pred_original = np.expm1(rf_pred)\n",
    "\n",
    "plt.scatter(y_test_original, rf_pred_original, alpha=0.6, c='forestgreen')\n",
    "plt.title('Actual vs Predicted: Original Scale', fontsize=14)\n",
    "plt.xlabel('Actual Total Resolution Hours', fontsize=12) \n",
    "plt.ylabel('Predicted Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Draw reference line\n",
    "max_val = max(y_test_original.max(), rf_pred_original.max())\n",
    "plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/actual_vs_predicted.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. ERROR ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Calculate prediction errors\n",
    "errors = y_test - rf_pred\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "# Create error distribution plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(errors, kde=True, color='steelblue')\n",
    "plt.title('Distribution of Prediction Errors (Log Scale)', fontsize=14)\n",
    "plt.xlabel('Prediction Error', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=y_test, y=abs_errors, alpha=0.6, color='forestgreen')\n",
    "plt.title('Error Magnitude vs Actual Value', fontsize=14)\n",
    "plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/error_analysis.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. SUMMARY HEATMAP\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a correlation heatmap of top planning features\n",
    "top_planning_features = feature_importances.head(10)['Feature'].tolist()\n",
    "correlation_matrix = df[top_planning_features + ['log_total_resolution_hours']].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(correlation_matrix)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f', \n",
    "           linewidths=0.5, mask=mask)\n",
    "plt.title('Correlation Matrix of Top Planning Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/correlation_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations saved to {viz_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (971, 185)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:120: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=planning_correlations.values, y=planning_correlations.index, palette='viridis')\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:188: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=categories, y=correlations, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODELS WITHOUT HYPERPARAMETER TUNING ===\n",
      "Random Forest Performance:\n",
      "  Train - RMSE: 0.5080, MAE: 0.3215, R²: 0.9728, Spearman: 0.9914\n",
      "  Test  - RMSE: 1.2537, MAE: 0.8328, R²: 0.8122, Spearman: 0.9499\n",
      "Gradient Boosting Performance:\n",
      "  Train - RMSE: 0.6963, MAE: 0.5247, R²: 0.9489, Spearman: 0.9671\n",
      "  Test  - RMSE: 1.1784, MAE: 0.8141, R²: 0.8341, Spearman: 0.9501\n",
      "XGBoost Performance:\n",
      "  Train - RMSE: 0.0381, MAE: 0.0269, R²: 0.9998, Spearman: 0.9999\n",
      "  Test  - RMSE: 1.4407, MAE: 0.9122, R²: 0.7520, Spearman: 0.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HYPERPARAMETER TUNING ===\n",
      "Tuning Random Forest...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Tuning Gradient Boosting...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.8}\n",
      "Tuning XGBoost...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "\n",
      "=== TUNED MODEL EVALUATION ===\n",
      "Tuned Random Forest Performance:\n",
      "  Train - RMSE: 0.6437, MAE: 0.4084, R²: 0.9563, Spearman: 0.9839\n",
      "  Test  - RMSE: 1.2425, MAE: 0.8261, R²: 0.8155, Spearman: 0.9515\n",
      "Tuned Gradient Boosting Performance:\n",
      "  Train - RMSE: 0.7074, MAE: 0.5426, R²: 0.9473, Spearman: 0.9669\n",
      "  Test  - RMSE: 1.1984, MAE: 0.8215, R²: 0.8284, Spearman: 0.9508\n",
      "Tuned XGBoost Performance:\n",
      "  Train - RMSE: 0.7797, MAE: 0.5722, R²: 0.9359, Spearman: 0.9645\n",
      "  Test  - RMSE: 1.2148, MAE: 0.8216, R²: 0.8237, Spearman: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:393: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL COMPARISON ===\n",
      "Model Performance Comparison:\n",
      "                     Model      RMSE       MAE        R²  Spearman\n",
      "0            Random Forest  1.253662  0.832786  0.812227  0.949854\n",
      "1        Gradient Boosting  1.178394  0.814112  0.834097  0.950106\n",
      "2                  XGBoost  1.440680  0.912166  0.752025  0.944276\n",
      "3      Tuned Random Forest  1.242549  0.826101  0.815541  0.951488\n",
      "4  Tuned Gradient Boosting  1.198441  0.821500  0.828404  0.950752\n",
      "5            Tuned XGBoost  1.214805  0.821568  0.823686  0.954328\n",
      "\n",
      "Improvement from Hyperparameter Tuning (%):\n",
      "               Model       RMSE       MAE\n",
      "0      Random Forest   0.886458  0.802712\n",
      "1  Gradient Boosting  -1.701163 -0.907483\n",
      "2            XGBoost  15.678339  9.932202\n",
      "\n",
      "Best Overall Model: Gradient Boosting\n",
      "  RMSE: 1.1784\n",
      "  MAE: 0.8141\n",
      "  R²: 0.8341\n",
      "  Spearman: 0.9501\n",
      "\n",
      "Analysis complete! All visualizations saved to resolution_hours_analysis_enhanced/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_27860/2957063538.py:724: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='Total Importance', y='Category', data=category_imp_df, palette='viridis')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Create output directory for visualizations\n",
    "viz_dir = 'resolution_hours_analysis_enhanced'\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. TARGET VARIABLE VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "target = 'total_resolution_hours'\n",
    "df['log_total_resolution_hours'] = np.log1p(df[target])\n",
    "\n",
    "# Create distribution plots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[target], kde=True, color='steelblue')\n",
    "plt.title('Distribution of Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['log_total_resolution_hours'], kde=True, color='forestgreen')\n",
    "plt.title('Distribution of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_distribution.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create boxplots\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df[target], color='steelblue')\n",
    "plt.title('Boxplot of Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Total Resolution Hours', fontsize=12)\n",
    "plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df['log_total_resolution_hours'], color='forestgreen')\n",
    "plt.title('Boxplot of Log-Transformed Total Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/target_boxplots.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. PLANNING-TIME FEATURES ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Define features available at planning time\n",
    "planning_features = [\n",
    "    # Project scope indicators\n",
    "    # 'total_issues',                    # Estimated during planning\n",
    "    \n",
    "    # Project composition estimates\n",
    "    'priority_critical_pct',           # Expected critical issues\n",
    "    'priority_high_pct',               # Expected high priority issues\n",
    "    'priority_medium_pct',             # Expected medium priority issues\n",
    "    'priority_low_pct',                # Expected low priority issues\n",
    "    'priority_blocker_pct',            # Expected blocker issues\n",
    "    \n",
    "    # Issue type distribution (estimated from similar projects)\n",
    "    'type_bug_pct',                    # Expected bug percentage\n",
    "    'type_task_pct',                   # Expected task percentage\n",
    "    'type_new_feature_pct',            # Expected feature work\n",
    "    'type_improvement_pct',            # Expected improvements\n",
    "    'type_documentation_pct',          # Expected documentation work\n",
    "    \n",
    "    # Team composition\n",
    "    'team_size_creators',              # Planned team size\n",
    "    'team_size_assignees',             # Planned assignees\n",
    "    'team_size_combined',              # Overall team size\n",
    "    \n",
    "    # Complexity indicators\n",
    "    'weighted_priority_score',         # Expected priority complexity\n",
    "    'issue_type_entropy',              # Expected variety of issues\n",
    "    \n",
    "    # Historical indicators that could be estimated\n",
    "    'high_to_low_priority_ratio',      # Expected priority distribution\n",
    "    'bug_ratio',                       # Expected bug ratio\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the dataframe\n",
    "planning_features = [f for f in planning_features if f in df.columns]\n",
    "\n",
    "# Create a correlation matrix of planning features with the target\n",
    "planning_correlations = df[planning_features].corrwith(df['log_total_resolution_hours'])\n",
    "planning_correlations = planning_correlations.sort_values(ascending=False)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x=planning_correlations.values, y=planning_correlations.index, palette='viridis')\n",
    "plt.title('Planning-Time Features: Correlation with Log-Transformed Resolution Hours', fontsize=14)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/planning_feature_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create scatter plots for top features\n",
    "top_features = planning_correlations.abs().sort_values(ascending=False).head(6).index\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "    \n",
    "    # Create scatter plot\n",
    "    sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "               scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = valid_data[feature].corr(valid_data['log_total_resolution_hours'])\n",
    "    \n",
    "    plt.title(f'{feature} vs. Log-Transformed Resolution Hours', fontsize=12)\n",
    "    plt.xlabel(feature, fontsize=10)\n",
    "    plt.ylabel('Log(Total Resolution Hours + 1)', fontsize=10)\n",
    "    plt.annotate(f'r = {corr:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/top_feature_relationships.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. FEATURE CATEGORY ANALYSIS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Project Scope': ['total_issues', 'project_duration_days'],\n",
    "    'Priority Distribution': [col for col in planning_features if 'priority_' in col and '_pct' in col],\n",
    "    'Issue Types': [col for col in planning_features if 'type_' in col and '_pct' in col],\n",
    "    'Team Composition': [col for col in planning_features if 'team_' in col],\n",
    "    'Complexity Metrics': ['weighted_priority_score', 'issue_type_entropy', 'high_to_low_priority_ratio', 'bug_ratio']\n",
    "}\n",
    "\n",
    "# Calculate average correlation by category\n",
    "category_correlations = {}\n",
    "for category, features in feature_categories.items():\n",
    "    # Get features that exist in the dataframe\n",
    "    existing_features = [f for f in features if f in df.columns]\n",
    "    if existing_features:\n",
    "        # Get absolute correlations\n",
    "        abs_corrs = df[existing_features].corrwith(df['log_total_resolution_hours']).abs()\n",
    "        category_correlations[category] = abs_corrs.mean()\n",
    "\n",
    "# Visualize category correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "categories = list(category_correlations.keys())\n",
    "correlations = list(category_correlations.values())\n",
    "\n",
    "# Sort by correlation\n",
    "sorted_indices = np.argsort(correlations)[::-1]\n",
    "categories = [categories[i] for i in sorted_indices]\n",
    "correlations = [correlations[i] for i in sorted_indices]\n",
    "\n",
    "sns.barplot(x=categories, y=correlations, palette='viridis')\n",
    "plt.title('Average Correlation by Feature Category', fontsize=14)\n",
    "plt.xlabel('Feature Category', fontsize=12)\n",
    "plt.ylabel('Average Absolute Correlation', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{viz_dir}/category_correlations.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. PRIORITY & ISSUE TYPE IMPACT VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create visualizations for priority distribution impact\n",
    "priority_features = [col for col in df.columns if 'priority_' in col and '_pct' in col]\n",
    "priority_features = [f for f in priority_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if priority_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(priority_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/priority_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations for issue type impact\n",
    "issue_type_features = [col for col in df.columns if 'type_' in col and '_pct' in col]\n",
    "issue_type_features = [f for f in issue_type_features if f in df.columns][:5]  # Limit to top 5\n",
    "\n",
    "if issue_type_features:\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    for i, feature in enumerate(issue_type_features):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        valid_data = df[[feature, 'log_total_resolution_hours']].dropna()\n",
    "        \n",
    "        sns.regplot(x=feature, y='log_total_resolution_hours', data=valid_data, \n",
    "                   scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "        \n",
    "        plt.title(f'Impact of {feature}', fontsize=12)\n",
    "        plt.xlabel(feature, fontsize=10)\n",
    "        plt.ylabel('Log(Total Hours)' if i == 0 else '', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{viz_dir}/issue_type_impact.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. DATA PREPARATION FOR MODELING\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Prepare the data for modeling\n",
    "df_planning = df[planning_features + ['log_total_resolution_hours']].copy()\n",
    "\n",
    "# Handle missing values with imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_planning[planning_features] = imputer.fit_transform(df_planning[planning_features])\n",
    "\n",
    "# Check for and replace infinite values\n",
    "for col in planning_features:\n",
    "    mask = np.isinf(df_planning[col])\n",
    "    if mask.any():\n",
    "        print(f\"Replacing {mask.sum()} infinite values in {col}\")\n",
    "        df_planning.loc[mask, col] = df_planning[col].median()\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_planning[planning_features]\n",
    "y = df_planning['log_total_resolution_hours']\n",
    "\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model performance on train and test sets\"\"\"\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Calculate Spearman correlation\n",
    "    train_spearman = stats.spearmanr(y_train, y_pred_train)[0]\n",
    "    test_spearman = stats.spearmanr(y_test, y_pred_test)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"  Train - RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}, Spearman: {train_spearman:.4f}\")\n",
    "    print(f\"  Test  - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}, Spearman: {test_spearman:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'predictions': y_pred_test,\n",
    "        'rmse': test_rmse,\n",
    "        'mae': test_mae,\n",
    "        'r2': test_r2,\n",
    "        'spearman': test_spearman\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. MODEL TRAINING - WITHOUT HYPERPARAMETER TUNING\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== MODELS WITHOUT HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "\n",
    "# Train Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_results = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting\")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. VISUALIZATION OF BASELINE MODELS' PREDICTIONS\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create directory for baseline model predictions\n",
    "baseline_dir = f'{viz_dir}/baseline_models'\n",
    "os.makedirs(baseline_dir, exist_ok=True)\n",
    "\n",
    "# Function to create prediction plots\n",
    "def create_prediction_plots(y_true, y_pred, model_name, output_dir):\n",
    "    # Log scale comparison\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Predictions plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, c='steelblue')\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.title(f'{model_name}: Actual vs Predicted (Log Scale)', fontsize=14)\n",
    "    plt.xlabel('Actual Log(Total Resolution Hours)', fontsize=12)\n",
    "    plt.ylabel('Predicted Log(Total Resolution Hours)', fontsize=12)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    plt.annotate(f'R² = {r2:.4f}', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Residuals plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    residuals = y_true - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6, c='forestgreen')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(f'{model_name}: Residuals Plot', fontsize=14)\n",
    "    plt.xlabel('Predicted Log(Total Resolution Hours)', fontsize=12)\n",
    "    plt.ylabel('Residuals', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/{model_name.lower().replace(\" \", \"_\")}_predictions.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Create prediction plots for each model\n",
    "create_prediction_plots(y_test, rf_results['predictions'], 'Random Forest', baseline_dir)\n",
    "create_prediction_plots(y_test, gb_results['predictions'], 'Gradient Boosting', baseline_dir)\n",
    "create_prediction_plots(y_test, xgb_results['predictions'], 'XGBoost', baseline_dir)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8. FEATURE IMPORTANCE VISUALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Function to plot feature importances\n",
    "def plot_feature_importance(model, feature_names, model_name, output_dir):\n",
    "    # Get feature importances\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not have feature_importances_ attribute.\")\n",
    "        return\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')\n",
    "    plt.title(f'{model_name}: Feature Importance', fontsize=14)\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/{model_name.lower().replace(\" \", \"_\")}_feature_importance.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return feature_importances\n",
    "\n",
    "# Plot feature importances for all models\n",
    "rf_importance = plot_feature_importance(rf_results['model'], planning_features, 'Random Forest', baseline_dir)\n",
    "gb_importance = plot_feature_importance(gb_results['model'], planning_features, 'Gradient Boosting', baseline_dir)\n",
    "xgb_importance = plot_feature_importance(xgb_results['model'], planning_features, 'XGBoost', baseline_dir)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9. HYPERPARAMETER TUNING\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Create directory for tuned models\n",
    "tuned_dir = f'{viz_dir}/tuned_models'\n",
    "os.makedirs(tuned_dir, exist_ok=True)\n",
    "\n",
    "# Define parameter grids for each model\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Function for hyperparameter tuning\n",
    "def tune_model(model, param_grid, X_train, y_train, model_name):\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        scoring='neg_root_mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Tune each model\n",
    "tuned_rf = tune_model(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    rf_param_grid,\n",
    "    X_train, y_train,\n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "tuned_gb = tune_model(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gb_param_grid,\n",
    "    X_train, y_train,\n",
    "    \"Gradient Boosting\"\n",
    ")\n",
    "\n",
    "tuned_xgb = tune_model(\n",
    "    XGBRegressor(random_state=42),\n",
    "    xgb_param_grid,\n",
    "    X_train, y_train,\n",
    "    \"XGBoost\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 10. EVALUATE TUNED MODELS\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== TUNED MODEL EVALUATION ===\")\n",
    "\n",
    "# Evaluate tuned models\n",
    "tuned_rf_results = evaluate_model(tuned_rf, X_train, X_test, y_train, y_test, \"Tuned Random Forest\")\n",
    "tuned_gb_results = evaluate_model(tuned_gb, X_train, X_test, y_train, y_test, \"Tuned Gradient Boosting\")\n",
    "tuned_xgb_results = evaluate_model(tuned_xgb, X_train, X_test, y_train, y_test, \"Tuned XGBoost\")\n",
    "\n",
    "# Create prediction plots for tuned models\n",
    "create_prediction_plots(y_test, tuned_rf_results['predictions'], 'Tuned_Random_Forest', tuned_dir)\n",
    "create_prediction_plots(y_test, tuned_gb_results['predictions'], 'Tuned_Gradient_Boosting', tuned_dir)\n",
    "create_prediction_plots(y_test, tuned_xgb_results['predictions'], 'Tuned_XGBoost', tuned_dir)\n",
    "\n",
    "# Plot feature importances for tuned models\n",
    "tuned_rf_importance = plot_feature_importance(tuned_rf_results['model'], planning_features, 'Tuned_Random_Forest', tuned_dir)\n",
    "tuned_gb_importance = plot_feature_importance(tuned_gb_results['model'], planning_features, 'Tuned_Gradient_Boosting', tuned_dir)\n",
    "tuned_xgb_importance = plot_feature_importance(tuned_xgb_results['model'], planning_features, 'Tuned_XGBoost', tuned_dir)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 11. MODEL COMPARISON\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "\n",
    "# Create directory for comparison\n",
    "comparison_dir = f'{viz_dir}/comparison'\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "# Collect all model results\n",
    "all_models = [\n",
    "    ('Random Forest', rf_results),\n",
    "    ('Gradient Boosting', gb_results),\n",
    "    ('XGBoost', xgb_results),\n",
    "    ('Tuned Random Forest', tuned_rf_results),\n",
    "    ('Tuned Gradient Boosting', tuned_gb_results),\n",
    "    ('Tuned XGBoost', tuned_xgb_results)\n",
    "]\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [m[0] for m in all_models],\n",
    "    'RMSE': [m[1]['rmse'] for m in all_models],\n",
    "    'MAE': [m[1]['mae'] for m in all_models],\n",
    "    'R²': [m[1]['r2'] for m in all_models],\n",
    "    'Spearman': [m[1]['spearman'] for m in all_models]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 12. VISUALIZATION OF MODEL COMPARISON\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create bar plots for each metric\n",
    "metrics = ['RMSE', 'MAE', 'R²', 'Spearman']\n",
    "colors = ['steelblue', 'forestgreen', 'purple', 'orange']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # For R² and Spearman, higher is better\n",
    "    if metric in ['R²', 'Spearman']:\n",
    "        sorted_df = comparison_df.sort_values(metric, ascending=False)\n",
    "        plt.barh(sorted_df['Model'], sorted_df[metric], color=colors[i])\n",
    "    else:  # For RMSE and MAE, lower is better\n",
    "        sorted_df = comparison_df.sort_values(metric)\n",
    "        plt.barh(sorted_df['Model'], sorted_df[metric], color=colors[i])\n",
    "    \n",
    "    plt.title(f'Model Comparison by {metric}', fontsize=14)\n",
    "    plt.xlabel(metric, fontsize=12)\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{comparison_dir}/{metric.lower().replace(\"²\", \"2\")}_comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 13. CALCULATE IMPROVEMENT FROM HYPERPARAMETER TUNING\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a heatmap showing improvement percentage\n",
    "baseline_models = ['Random Forest', 'Gradient Boosting', 'XGBoost']\n",
    "tuned_models = ['Tuned Random Forest', 'Tuned Gradient Boosting', 'Tuned XGBoost']\n",
    "metrics_for_improvement = ['RMSE', 'MAE']\n",
    "\n",
    "# Calculate improvement percentages\n",
    "improvement_data = []\n",
    "for baseline, tuned in zip(baseline_models, tuned_models):\n",
    "    row = [baseline]\n",
    "    for metric in metrics_for_improvement:\n",
    "        baseline_value = comparison_df.loc[comparison_df['Model'] == baseline, metric].values[0]\n",
    "        tuned_value = comparison_df.loc[comparison_df['Model'] == tuned, metric].values[0]\n",
    "        \n",
    "        # For RMSE and MAE, lower is better, so improvement is (baseline - tuned) / baseline\n",
    "        improvement_pct = (baseline_value - tuned_value) / baseline_value * 100\n",
    "        row.append(improvement_pct)\n",
    "    improvement_data.append(row)\n",
    "\n",
    "improvement_df = pd.DataFrame(improvement_data, columns=['Model'] + metrics_for_improvement)\n",
    "print(\"\\nImprovement from Hyperparameter Tuning (%):\")\n",
    "print(improvement_df)\n",
    "\n",
    "# Create a heatmap of improvement percentages\n",
    "plt.figure(figsize=(10, 6))\n",
    "improvement_matrix = improvement_df.set_index('Model')\n",
    "sns.heatmap(improvement_matrix, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Improvement from Hyperparameter Tuning (%)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{comparison_dir}/improvement_heatmap.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 14. IDENTIFY BEST MODEL OVERALL\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Rank models by each metric\n",
    "ranking_df = comparison_df.copy()\n",
    "\n",
    "# For RMSE and MAE (lower is better), rank in ascending order\n",
    "for metric in ['RMSE', 'MAE']:\n",
    "    ranking_df[f'{metric}_rank'] = ranking_df[metric].rank()\n",
    "\n",
    "# For R² and Spearman (higher is better), rank in descending order\n",
    "for metric in ['R²', 'Spearman']:\n",
    "    ranking_df[f'{metric}_rank'] = ranking_df[metric].rank(ascending=False)\n",
    "\n",
    "# Calculate average rank\n",
    "rank_columns = [col for col in ranking_df.columns if col.endswith('_rank')]\n",
    "ranking_df['Average_Rank'] = ranking_df[rank_columns].mean(axis=1)\n",
    "ranking_df = ranking_df.sort_values('Average_Rank')\n",
    "\n",
    "best_model_name = ranking_df.iloc[0]['Model']\n",
    "best_model_idx = [m[0] for m in all_models].index(best_model_name)\n",
    "best_model = all_models[best_model_idx][1]['model']\n",
    "best_predictions = all_models[best_model_idx][1]['predictions']\n",
    "\n",
    "print(f\"\\nBest Overall Model: {best_model_name}\")\n",
    "for metric in metrics:\n",
    "    value = ranking_df.loc[ranking_df['Model'] == best_model_name, metric].values[0]\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 15. CREATE FINAL VISUALIZATIONS FOR BEST MODEL\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Create a final comparison plot showing all models' performance\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot R² vs RMSE for all models\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(\n",
    "    comparison_df['RMSE'], comparison_df['R²'], \n",
    "    s=100, alpha=0.7, c=sns.color_palette(\"viridis\", len(comparison_df))\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, model in enumerate(comparison_df['Model']):\n",
    "    plt.annotate(\n",
    "        model, \n",
    "        (comparison_df['RMSE'].iloc[i], comparison_df['R²'].iloc[i]),\n",
    "        xytext=(5, 5), textcoords='offset points'\n",
    "    )\n",
    "\n",
    "plt.title('Model Comparison: R² vs RMSE', fontsize=14)\n",
    "plt.xlabel('RMSE (lower is better)', fontsize=12)\n",
    "plt.ylabel('R² (higher is better)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Highlight best model in metrics\n",
    "plt.subplot(2, 1, 2)\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "n_bars = len(all_models)\n",
    "\n",
    "for i, (model_name, model_results) in enumerate(all_models):\n",
    "    values = [model_results[metric.lower().replace('²', '2')] for metric in metrics]\n",
    "    plt.bar(x + (i - n_bars/2 + 0.5) * width, values, width, label=model_name)\n",
    "\n",
    "plt.title('Comparison of All Models Across Metrics', fontsize=14)\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{comparison_dir}/best_models_comparison.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Create a summary document\n",
    "with open(f'{comparison_dir}/model_summary.txt', 'w') as f:\n",
    "    f.write(\"=== MODEL COMPARISON SUMMARY ===\\n\\n\")\n",
    "    f.write(\"Performance Metrics:\\n\")\n",
    "    f.write(comparison_df.to_string(index=False))\n",
    "    f.write(\"\\n\\nImprovement from Hyperparameter Tuning (%):\\n\")\n",
    "    f.write(improvement_df.to_string(index=False))\n",
    "    f.write(f\"\\n\\nBest Overall Model: {best_model_name}\\n\")\n",
    "    for metric in metrics:\n",
    "        value = ranking_df.loc[ranking_df['Model'] == best_model_name, metric].values[0]\n",
    "        f.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "    \n",
    "    f.write(\"\\nBest Parameters:\\n\")\n",
    "    if 'Tuned' in best_model_name:\n",
    "        f.write(f\"  {str(best_model.get_params())}\\n\")\n",
    "    \n",
    "    f.write(\"\\nTop 10 Features (by importance):\\n\")\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "        feature_imp = pd.DataFrame({\n",
    "            'Feature': planning_features,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        for i, row in feature_imp.head(10).iterrows():\n",
    "            f.write(f\"  {row['Feature']}: {row['Importance']:.4f}\\n\")\n",
    "\n",
    "# Create feature importance by category visualization for best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importance by category\n",
    "    importance_by_category = {}\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        # Filter features that exist in planning_features\n",
    "        cat_features = [f for f in features if f in planning_features]\n",
    "        if not cat_features:\n",
    "            continue\n",
    "            \n",
    "        # Get importance values\n",
    "        feature_indices = [planning_features.index(f) for f in cat_features]\n",
    "        importances = best_model.feature_importances_[feature_indices]\n",
    "        \n",
    "        # Calculate total importance for category\n",
    "        importance_by_category[category] = np.sum(importances)\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    category_imp_df = pd.DataFrame({\n",
    "        'Category': list(importance_by_category.keys()),\n",
    "        'Total Importance': list(importance_by_category.values())\n",
    "    }).sort_values('Total Importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Total Importance', y='Category', data=category_imp_df, palette='viridis')\n",
    "    plt.title(f'Feature Importance by Category for {best_model_name}', fontsize=14)\n",
    "    plt.xlabel('Total Importance', fontsize=12)\n",
    "    plt.ylabel('Feature Category', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{comparison_dir}/best_tuned_model_feature_importance_by_category.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"\\nAnalysis complete! All visualizations saved to {viz_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
