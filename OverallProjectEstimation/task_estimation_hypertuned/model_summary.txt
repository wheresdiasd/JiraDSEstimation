# Resolution Hours Ensemble Model with Hyperparameter Tuning

## Performance Comparison

| Model | Metric | Original | Tuned | Improvement (%) |
|-------|--------|----------|-------|----------------|
| Random_Forest | MAE | 696252.7982 | 795.4357 | 99.89 |
| Random_Forest | RMSE | 1011883.1847 | 10810.0806 | 98.93 |
| Random_Forest | R2 | 0.9545 | 1.0000 | 4.76 |
| Random_Forest | Spearman | 0.9303 | 0.9420 | 1.27 |
| Gradient_Boosting | MAE | 493179.5558 | 55036.7658 | 88.84 |
| Gradient_Boosting | RMSE | 793477.5745 | 104439.2886 | 86.84 |
| Gradient_Boosting | R2 | 0.9721 | 0.9995 | 2.83 |
| Gradient_Boosting | Spearman | 0.9326 | 0.9416 | 0.96 |
| XGBoost | MAE | 540165.1681 | 109830.7301 | 79.67 |
| XGBoost | RMSE | 853762.8692 | 213102.3863 | 75.04 |
| XGBoost | R2 | 0.9676 | 0.9980 | 3.14 |
| XGBoost | Spearman | 0.9312 | 0.9405 | 1.00 |
| Ensemble | MAE | 561397.4735 | 52590.8548 | 90.63 |
| Ensemble | RMSE | 846977.9712 | 101449.1967 | 88.02 |
| Ensemble | R2 | 0.9682 | 0.9995 | 3.24 |
| Ensemble | Spearman | 0.9325 | 0.9416 | 0.97 |

## Hyperparameter Changes

### Random_Forest

| Parameter | Original | Tuned |
|-----------|----------|-------|
| n_estimators | 100 | 50 |
| min_samples_leaf | 5 | 1 |
| max_features | sqrt | sqrt |
| max_depth | 10 | None |

### Gradient_Boosting

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 1.0 | 1.0 |
| n_estimators | 100 | 200 |
| min_samples_leaf | 5 | 3 |
| max_depth | 5 | 7 |
| learning_rate | 0.05 | 0.05 |

### XGBoost

| Parameter | Original | Tuned |
|-----------|----------|-------|
| subsample | 0.8 | 1.0 |
| n_estimators | 100 | 200 |
| min_child_weight | 5 | 3 |
| max_depth | 5 | 5 |
| learning_rate | 0.05 | 0.1 |
| colsample_bytree | 0.8 | 0.8 |


## Conclusion

Hyperparameter tuning resulted in an average MAE improvement of 89.76% and an average RÂ² improvement of 3.49%.

Before tuning, Gradient_Boosting was the best performing model. After tuning, Random_Forest became the best performing model.
