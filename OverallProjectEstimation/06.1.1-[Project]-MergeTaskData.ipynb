{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_12374/3384569829.py:22: DtypeWarning: Columns (7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  task_df = pd.read_csv(task_data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Dataset Shape: (2259837, 40)\n",
      "Project Dataset Shape: (971, 178)\n",
      "Using Task Project ID Column: fields.project.id\n",
      "Using Project ID Column: remainder__project_id\n",
      "\n",
      "Merged Dataset Shape: (1575833, 63)\n",
      "Number of Unique Projects: 711\n",
      "\n",
      "Merged Dataset Columns:\n",
      "['id', 'fields.issuetype.id', 'fields.issuetype.name', 'project_id', 'fields.project.key', 'fields.project.name', 'fields.created', 'fields.priority.name', 'fields.priority.id', 'fields.updated', 'fields.status.name', 'fields.creator.active', 'priority_id', 'issue_type_id', 'is_completed', 'type_task', 'type_bug', 'inward_count', 'outward_count', 'is_resolved', 'age_days', 'type_sub_task', 'created_is_weekend', 'created_hour', 'created_month', 'created_year', 'resolution_hours', 'log_resolution_hours', 'is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', 'is_type_new_feature', 'is_type_epic', 'is_type_sub-task', 'is_priority_blocker', 'is_priority_critical', 'is_priority_major', 'is_priority_minor', 'is_priority_trivial', 'time_power__project_duration_days', 'count_std__total_issues', 'pct_minmax__type_bug_pct', 'pct_minmax__type_task_pct', 'pct_minmax__type_new_feature_pct', 'pct_minmax__type_epic_pct', 'pct_minmax__type_improvement_pct', 'pct_minmax__type_story_pct', 'pct_minmax__type_documentation_pct', 'pct_minmax__priority_critical_pct', 'pct_minmax__priority_blocker_pct', 'pct_minmax__priority_high_pct', 'pct_minmax__priority_low_pct', 'remainder__team_size_creators', 'remainder__team_size_assignees', 'remainder__team_size_combined', 'stat_robust__weighted_priority_score', 'stat_robust__issue_type_entropy', 'stat_robust__high_to_low_priority_ratio', 'stat_robust__bug_ratio', 'avg_resolution_hours', 'median_resolution_hours', 'total_resolution_hours']\n",
      "\n",
      "Merged dataset saved to: merged_task_data/merged_project_task_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def merge_task_and_project_data(task_data_path, project_data_path, output_dir='merged_task_data'):\n",
    "    \"\"\"\n",
    "    Merge task-level and project-level datasets\n",
    "    \n",
    "    Parameters:\n",
    "    - task_data_path: Path to the cleaned task-level dataset\n",
    "    - project_data_path: Path to the project-level dataset\n",
    "    - output_dir: Directory to save merged dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Merged DataFrame\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Load Datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    task_df = pd.read_csv(task_data_path)\n",
    "    project_df = pd.read_csv(project_data_path)\n",
    "    \n",
    "    # Print initial dataset info\n",
    "    print(f\"Task Dataset Shape: {task_df.shape}\")\n",
    "    print(f\"Project Dataset Shape: {project_df.shape}\")\n",
    "    \n",
    "    # 2. Identify Project ID Columns\n",
    "    # Try to find project ID columns in both datasets\n",
    "    task_project_id_candidates = [\n",
    "        'fields.project.id', \n",
    "        'project_id', \n",
    "        'remainder__project_id'\n",
    "    ]\n",
    "    \n",
    "    project_id_candidates = [\n",
    "        'project_id', \n",
    "        'remainder__project_id', \n",
    "        'fields.project.id'\n",
    "    ]\n",
    "    \n",
    "    # Find matching project ID column\n",
    "    task_project_id_col = next((col for col in task_project_id_candidates if col in task_df.columns), None)\n",
    "    project_id_col = next((col for col in project_id_candidates if col in project_df.columns), None)\n",
    "    \n",
    "    if not task_project_id_col or not project_id_col:\n",
    "        raise ValueError(f\"Could not find matching project ID columns. Task columns: {task_df.columns}, Project columns: {project_df.columns}\")\n",
    "    \n",
    "    print(f\"Using Task Project ID Column: {task_project_id_col}\")\n",
    "    print(f\"Using Project ID Column: {project_id_col}\")\n",
    "    \n",
    "    # 3. Rename columns for consistent merging\n",
    "    task_df = task_df.rename(columns={task_project_id_col: 'project_id'})\n",
    "    project_df = project_df.rename(columns={project_id_col: 'project_id'})\n",
    "    \n",
    "    # 4. Select Relevant Project Features\n",
    "    project_context_features = [\n",
    "        # Project size/scope\n",
    "        'time_power__project_duration_days',\n",
    "        'count_std__total_issues',\n",
    "        \n",
    "        # Project composition\n",
    "        'pct_minmax__type_bug_pct',\n",
    "        'pct_minmax__type_task_pct',\n",
    "        'pct_minmax__type_new_feature_pct',\n",
    "        'pct_minmax__type_epic_pct',\n",
    "        'pct_minmax__type_improvement_pct',\n",
    "        'pct_minmax__type_story_pct',\n",
    "        'pct_minmax__type_documentation_pct',\n",
    "        \n",
    "        # Priority distribution\n",
    "        'pct_minmax__priority_critical_pct',\n",
    "        'pct_minmax__priority_blocker_pct',\n",
    "        'pct_minmax__priority_high_pct',\n",
    "        'pct_minmax__priority_low_pct',\n",
    "        \n",
    "        # Team metrics\n",
    "        'remainder__team_size_creators',\n",
    "        'remainder__team_size_assignees',\n",
    "        'remainder__team_size_combined',\n",
    "        \n",
    "        # Complexity metrics\n",
    "        'stat_robust__weighted_priority_score',\n",
    "        'stat_robust__issue_type_entropy',\n",
    "        'stat_robust__high_to_low_priority_ratio',\n",
    "        'stat_robust__bug_ratio',\n",
    "        \n",
    "        # Resolution time metrics\n",
    "        'avg_resolution_hours',\n",
    "        'median_resolution_hours',\n",
    "        'total_resolution_hours'\n",
    "    ]\n",
    "    \n",
    "    # Filter to features that actually exist in the project dataframe\n",
    "    project_context_features = [f for f in project_context_features if f in project_df.columns]\n",
    "    \n",
    "    # Select features to merge\n",
    "    project_merge_df = project_df[['project_id'] + project_context_features]\n",
    "    \n",
    "    # 5. Merge Datasets\n",
    "    merged_df = pd.merge(\n",
    "        task_df, \n",
    "        project_merge_df, \n",
    "        on='project_id', \n",
    "        how='inner'  # Use inner join to ensure only matching projects\n",
    "    )\n",
    "    \n",
    "    # 6. Log-transform resolution hours if not already done\n",
    "    if 'log_resolution_hours' not in merged_df.columns:\n",
    "        if 'resolution_hours' in merged_df.columns:\n",
    "            merged_df['log_resolution_hours'] = np.log1p(merged_df['resolution_hours'])\n",
    "    \n",
    "    # 7. Basic Data Validation\n",
    "    print(f\"\\nMerged Dataset Shape: {merged_df.shape}\")\n",
    "    print(f\"Number of Unique Projects: {merged_df['project_id'].nunique()}\")\n",
    "    print(\"\\nMerged Dataset Columns:\")\n",
    "    print(merged_df.columns.tolist())\n",
    "    \n",
    "    # 8. Save Merged Dataset\n",
    "    output_path = os.path.join(output_dir, 'merged_project_task_data.csv')\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nMerged dataset saved to: {output_path}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Update these paths to match your specific dataset locations\n",
    "    task_data_path = './cleaned_jira_dataset.csv'\n",
    "    project_data_path = 'prepared_processed_data/common_features_scaled_with_original_targets.csv'\n",
    "    \n",
    "    # Merge the datasets\n",
    "    merged_data = merge_task_and_project_data(\n",
    "        task_data_path, \n",
    "        project_data_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading task dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_1771/4226598794.py:16: DtypeWarning: Columns (7,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  task_df = pd.read_csv(task_data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Regressor (Task-Only)...\n",
      "\n",
      "Task-Only Model Performance:\n",
      "Mean Squared Error: 117444005.7987\n",
      "Root Mean Squared Error: 10837.1586\n",
      "Mean Absolute Error: 4047.7512\n",
      "R-squared: 0.0145\n",
      "Mean Magnitude of Relative Error: inf\n",
      "\n",
      "Task-Only Analysis complete. Check the 'task_only_results' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./merged_task_data/merged_project_task_data.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
