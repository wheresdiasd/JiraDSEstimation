{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "def inventory_project_columns(base_dir, output_inventory_file=None):\n",
    "    \"\"\"\n",
    "    Inventories all columns across project files, analyzing naming patterns\n",
    "    to identify similar columns with different names.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing repository folders with project-level data\n",
    "    output_inventory_file : str, optional\n",
    "        Path to save the column inventory as JSON\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with column pattern information\n",
    "    \"\"\"\n",
    "    # Check if the base directory exists\n",
    "    if not os.path.exists(base_dir):\n",
    "        raise ValueError(f\"Base directory not found: {base_dir}\")\n",
    "    \n",
    "    # Get a list of all repository folders\n",
    "    repo_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "    if not repo_folders:\n",
    "        raise ValueError(f\"No repository folders found in {base_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(repo_folders)} repository folders in {base_dir}\")\n",
    "    \n",
    "    # Dictionary to track column information\n",
    "    all_columns = set()\n",
    "    column_patterns = defaultdict(set)\n",
    "    column_by_repo = defaultdict(set)\n",
    "    repo_by_column = defaultdict(set)\n",
    "    \n",
    "    # Process each repository folder\n",
    "    for repo_folder in repo_folders:\n",
    "        repo_path = os.path.join(base_dir, repo_folder)\n",
    "        \n",
    "        # Find all project-level CSV files in the repository folder\n",
    "        csv_files = glob.glob(os.path.join(repo_path, \"project_level_*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No project-level CSV files found in {repo_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing repository: {repo_folder} - Found {len(csv_files)} project-level files\")\n",
    "        \n",
    "        # Process each CSV file in the repository folder\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                # Read just the header to get column names\n",
    "                df_header = pd.read_csv(csv_file, nrows=0)\n",
    "                \n",
    "                # Add columns to the tracking dictionaries\n",
    "                for col in df_header.columns:\n",
    "                    all_columns.add(col)\n",
    "                    column_by_repo[repo_folder].add(col)\n",
    "                    repo_by_column[col].add(repo_folder)\n",
    "                    \n",
    "                    # Categorize columns by pattern\n",
    "                    if re.match(r'priority_.*_count', col):\n",
    "                        column_patterns['priority_count'].add(col)\n",
    "                    elif re.match(r'priority_.*_pct', col):\n",
    "                        column_patterns['priority_pct'].add(col)\n",
    "                    elif re.match(r'type_.*_count', col):\n",
    "                        column_patterns['type_count'].add(col)\n",
    "                    elif re.match(r'type_.*_pct', col):\n",
    "                        column_patterns['type_pct'].add(col)\n",
    "                    elif re.match(r'priority_.*_type_.*_count', col):\n",
    "                        column_patterns['priority_type_count'].add(col)\n",
    "                    elif re.match(r'priority_.*_type_.*_avg_resolution_hours', col):\n",
    "                        column_patterns['priority_type_resolution'].add(col)\n",
    "                    elif re.match(r'type_.*_resolution_rate', col):\n",
    "                        column_patterns['type_resolution_rate'].add(col)\n",
    "                    elif col.startswith('pct_'):\n",
    "                        column_patterns['percentage_metrics'].add(col)\n",
    "                    elif col.endswith('_ratio'):\n",
    "                        column_patterns['ratio_metrics'].add(col)\n",
    "                    else:\n",
    "                        column_patterns['other'].add(col)\n",
    "                \n",
    "                print(f\"  Processed column names from {os.path.basename(csv_file)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {csv_file}: {str(e)}\")\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    inventory = {\n",
    "        'total_unique_columns': len(all_columns),\n",
    "        'column_patterns': {k: sorted(list(v)) for k, v in column_patterns.items()},\n",
    "        'repositories': {\n",
    "            'count': len(repo_folders),\n",
    "            'names': repo_folders\n",
    "        },\n",
    "        'columns_by_repo': {k: sorted(list(v)) for k, v in column_by_repo.items()},\n",
    "        'repos_by_column': {k: sorted(list(v)) for k, v in repo_by_column.items()}\n",
    "    }\n",
    "    \n",
    "    # Add pattern analysis to identify similar columns with different names\n",
    "    similar_columns = defaultdict(list)\n",
    "    \n",
    "    # Analyze priority columns\n",
    "    priority_cols = column_patterns['priority_count'].union(column_patterns['priority_pct'])\n",
    "    priority_patterns = {}\n",
    "    for col in priority_cols:\n",
    "        # Extract priority level and metric\n",
    "        matches = re.match(r'priority_([\\w-]+)(?:_-_p\\d+|___p\\d+)?_(count|pct)', col)\n",
    "        if matches:\n",
    "            priority_level = matches.group(1)\n",
    "            metric = matches.group(2)\n",
    "            key = f\"priority_{priority_level}_{metric}\"\n",
    "            if key not in priority_patterns:\n",
    "                priority_patterns[key] = []\n",
    "            priority_patterns[key].append(col)\n",
    "    \n",
    "    # Add to similar columns\n",
    "    for key, cols in priority_patterns.items():\n",
    "        if len(cols) > 1:\n",
    "            similar_columns[key] = cols\n",
    "    \n",
    "    # Analyze issue type columns\n",
    "    type_cols = column_patterns['type_count'].union(column_patterns['type_pct'])\n",
    "    type_patterns = {}\n",
    "    for col in type_cols:\n",
    "        # Extract issue type and metric\n",
    "        matches = re.match(r'type_([\\w-]+)_(count|pct)', col)\n",
    "        if matches:\n",
    "            issue_type = matches.group(1).replace('-', '_')  # Standardize - vs _\n",
    "            metric = matches.group(2)\n",
    "            key = f\"type_{issue_type}_{metric}\"\n",
    "            if key not in type_patterns:\n",
    "                type_patterns[key] = []\n",
    "            type_patterns[key].append(col)\n",
    "    \n",
    "    # Add to similar columns\n",
    "    for key, cols in type_patterns.items():\n",
    "        if len(cols) > 1:\n",
    "            similar_columns[key] = cols\n",
    "    \n",
    "    # Add similar columns to inventory\n",
    "    inventory['similar_columns'] = {k: v for k, v in similar_columns.items()}\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nColumn Inventory Complete!\")\n",
    "    print(f\"Total unique columns across all repositories: {len(all_columns)}\")\n",
    "    print(\"\\nColumn pattern categories:\")\n",
    "    for pattern, cols in column_patterns.items():\n",
    "        print(f\"  {pattern}: {len(cols)} columns\")\n",
    "    \n",
    "    print(\"\\nSimilar columns with different naming patterns:\")\n",
    "    for standard_name, variants in list(similar_columns.items())[:10]:  # Show first 10\n",
    "        print(f\"  {standard_name}: {len(variants)} variants\")\n",
    "        for v in variants[:3]:  # Show first 3 variants\n",
    "            print(f\"    - {v}\")\n",
    "        if len(variants) > 3:\n",
    "            print(f\"    - ... and {len(variants) - 3} more\")\n",
    "    \n",
    "    if len(similar_columns) > 10:\n",
    "        print(f\"  ... and {len(similar_columns) - 10} more pattern groups\")\n",
    "    \n",
    "    # Save inventory to file if specified\n",
    "    if output_inventory_file:\n",
    "        output_dir = os.path.dirname(output_inventory_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(output_inventory_file, 'w') as f:\n",
    "            json.dump(inventory, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nInventory saved to {output_inventory_file}\")\n",
    "    \n",
    "    return inventory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gini(array):\n",
    "    \"\"\"\n",
    "    Calculate the Gini coefficient of a numpy array.\n",
    "    \"\"\"\n",
    "    array = np.array(array)\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array)\n",
    "    array = array + 1e-7  # small constant to avoid division by zero\n",
    "    array = np.sort(array)\n",
    "    index = np.arange(1, array.shape[0] + 1)\n",
    "    n = array.shape[0]\n",
    "    return (np.sum((2 * index - n - 1) * array)) / (n * np.sum(array))\n",
    "\n",
    "def add_team_metrics(aggregation_df, issues_df):\n",
    "    \"\"\"\n",
    "    Compute and append team metrics to the aggregation DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "      aggregation_df : pandas.DataFrame\n",
    "          Aggregated project-level DataFrame (typically a single row).\n",
    "      issues_df : pandas.DataFrame\n",
    "          DataFrame containing detailed issue/creator/assignee information from your CSVs.\n",
    "    \n",
    "    Returns:\n",
    "      aggregation_df : pandas.DataFrame\n",
    "          The input aggregation_df augmented with the new team metrics.\n",
    "    \"\"\"\n",
    "    # 1. Team Size and Composition\n",
    "    team_size_creators = issues_df['creator'].nunique() if 'creator' in issues_df.columns else 0\n",
    "    team_size_assignees = issues_df['assignee'].nunique() if 'assignee' in issues_df.columns else 0\n",
    "    team_size_combined = team_size_creators + team_size_assignees\n",
    "\n",
    "    # Assuming there is a boolean column 'is_core' to indicate core team membership\n",
    "    if 'is_core' in issues_df.columns:\n",
    "        core_team_count = issues_df.loc[issues_df['is_core'] == True, 'creator'].nunique()\n",
    "        core_team_ratio = core_team_count / team_size_combined if team_size_combined > 0 else np.nan\n",
    "    else:\n",
    "        core_team_ratio = np.nan\n",
    "\n",
    "    # 2. Creator Contribution and Workload\n",
    "    if 'creator' in issues_df.columns:\n",
    "        creator_counts = issues_df.groupby('creator').size()\n",
    "        creator_workload_gini = gini(creator_counts.values) if len(creator_counts) > 0 else np.nan\n",
    "        avg_issues_per_creator = creator_counts.mean()\n",
    "        top_creator_contribution = creator_counts.max() / len(issues_df) if len(issues_df) > 0 else np.nan\n",
    "        creator_activity_variance = creator_counts.var()\n",
    "        creator_activity_std = creator_counts.std()\n",
    "    else:\n",
    "        creator_workload_gini = avg_issues_per_creator = top_creator_contribution = creator_activity_variance = creator_activity_std = np.nan\n",
    "\n",
    "    # Diversity in contributions: average number of unique issue types per creator\n",
    "    if 'issue_type' in issues_df.columns and 'creator' in issues_df.columns:\n",
    "        creator_diversity = issues_df.groupby('creator')['issue_type'].nunique().mean()\n",
    "    else:\n",
    "        creator_diversity = np.nan\n",
    "\n",
    "    # 3. Team Specialization and Complexity\n",
    "    if 'issue_type' in issues_df.columns:\n",
    "        issue_type_counts = issues_df['issue_type'].value_counts()\n",
    "        team_type_specialization_index = issue_type_counts.max() / issue_type_counts.sum()\n",
    "    else:\n",
    "        team_type_specialization_index = np.nan\n",
    "\n",
    "    # Example: Use a 'complexity' column if available to compute team complexity capacity\n",
    "    team_complexity_capacity = issues_df['complexity'].mean() if 'complexity' in issues_df.columns else np.nan\n",
    "\n",
    "    # Team resolution predictability: Inverse of the standard deviation of resolution times\n",
    "    if 'resolution_time' in issues_df.columns:\n",
    "        std_resolution = issues_df['resolution_time'].std()\n",
    "        team_resolution_predictability = 1 / std_resolution if std_resolution and std_resolution != 0 else np.nan\n",
    "    else:\n",
    "        team_resolution_predictability = np.nan\n",
    "\n",
    "    # 4. Developer and Creator Ratios\n",
    "    if 'issue_type' in issues_df.columns and 'creator' in issues_df.columns and 'assignee' in issues_df.columns:\n",
    "        # Bug-related contributions\n",
    "        bug_issues = issues_df[issues_df['issue_type'] == 'bug']\n",
    "        if len(bug_issues) > 0:\n",
    "            bug_creator_concentration = bug_issues.groupby('creator').size().max() / len(bug_issues)\n",
    "        else:\n",
    "            bug_creator_concentration = np.nan\n",
    "\n",
    "        # Bug handling ratios (comparing average bug issue counts for creators vs. assignees)\n",
    "        avg_bug_creator = bug_issues.groupby('creator').size().mean() if len(bug_issues) > 0 else np.nan\n",
    "        avg_bug_assignee = bug_issues.groupby('assignee').size().mean() if len(bug_issues) > 0 else np.nan\n",
    "        bug_developer_ratio = avg_bug_assignee / avg_bug_creator if avg_bug_creator and avg_bug_creator != 0 else np.nan\n",
    "\n",
    "        # Feature handling ratios\n",
    "        feature_issues = issues_df[issues_df['issue_type'] == 'feature']\n",
    "        avg_feature_creator = feature_issues.groupby('creator').size().mean() if len(feature_issues) > 0 else np.nan\n",
    "        avg_feature_assignee = feature_issues.groupby('assignee').size().mean() if len(feature_issues) > 0 else np.nan\n",
    "        feature_developer_ratio = avg_feature_assignee / avg_feature_creator if avg_feature_creator and avg_feature_creator != 0 else np.nan\n",
    "    else:\n",
    "        bug_creator_concentration = bug_developer_ratio = feature_developer_ratio = np.nan\n",
    "\n",
    "    # 5. Creator Experience and Efficiency\n",
    "    avg_creator_experience_days = issues_df['creator_experience_days'].mean() if 'creator_experience_days' in issues_df.columns else np.nan\n",
    "    avg_creator_issue_count = issues_df.groupby('creator').size().mean() if 'creator' in issues_df.columns else np.nan\n",
    "    avg_creator_specialization = issues_df['creator_specialization'].mean() if 'creator_specialization' in issues_df.columns else np.nan\n",
    "    creator_link_density_mean = issues_df['creator_link_density'].mean() if 'creator_link_density' in issues_df.columns else np.nan\n",
    "    creator_link_density_std = issues_df['creator_link_density'].std() if 'creator_link_density' in issues_df.columns else np.nan\n",
    "    creator_resolution_time_variability = issues_df['resolution_time'].std() if 'resolution_time' in issues_df.columns else np.nan\n",
    "    creator_onboarding_volatility = issues_df['onboarding_time'].std() if 'onboarding_time' in issues_df.columns else np.nan\n",
    "    \n",
    "    # Average new creators per month using 'creator_join_date'\n",
    "    if 'creator_join_date' in issues_df.columns:\n",
    "        issues_df['join_month'] = pd.to_datetime(issues_df['creator_join_date']).dt.to_period('M')\n",
    "        avg_new_creators_per_month = issues_df.groupby('join_month')['creator'].nunique().mean()\n",
    "    else:\n",
    "        avg_new_creators_per_month = np.nan\n",
    "\n",
    "    # Combine all computed metrics into a dictionary\n",
    "    new_metrics = {\n",
    "        'team_size_creators': team_size_creators,\n",
    "        'team_size_assignees': team_size_assignees,\n",
    "        'team_size_combined': team_size_combined,\n",
    "        'core_team_ratio': core_team_ratio,\n",
    "        'creator_workload_gini': creator_workload_gini,\n",
    "        'creator_diversity': creator_diversity,\n",
    "        'avg_issues_per_creator': avg_issues_per_creator,\n",
    "        'top_creator_contribution': top_creator_contribution,\n",
    "        'creator_activity_variance': creator_activity_variance,\n",
    "        'creator_activity_std': creator_activity_std,\n",
    "        'team_type_specialization_index': team_type_specialization_index,\n",
    "        'team_complexity_capacity': team_complexity_capacity,\n",
    "        'team_resolution_predictability': team_resolution_predictability,\n",
    "        'bug_creator_concentration': bug_creator_concentration,\n",
    "        'bug_developer_ratio': bug_developer_ratio,\n",
    "        'feature_developer_ratio': feature_developer_ratio,\n",
    "        'avg_creator_experience_days': avg_creator_experience_days,\n",
    "        'avg_creator_issue_count': avg_creator_issue_count,\n",
    "        'avg_creator_specialization': avg_creator_specialization,\n",
    "        'creator_link_density_mean': creator_link_density_mean,\n",
    "        'creator_link_density_std': creator_link_density_std,\n",
    "        'creator_resolution_time_variability': creator_resolution_time_variability,\n",
    "        'creator_onboarding_volatility': creator_onboarding_volatility,\n",
    "        'avg_new_creators_per_month': avg_new_creators_per_month\n",
    "    }\n",
    "\n",
    "    # Append these new metrics to the aggregation DataFrame.\n",
    "    for key, value in new_metrics.items():\n",
    "        aggregation_df[key] = value\n",
    "\n",
    "    return aggregation_df\n",
    "\n",
    "# Example integration:\n",
    "# At the end of your CombineProjectsInventory.ipynb pipeline, you can add:\n",
    "# aggregated_df = add_team_metrics(aggregated_df, issues_df)\n",
    "# This will enrich your aggregated data with the additional team metrics.\n",
    "\n",
    "def combine_all_projects_raw(base_dir, output_file):\n",
    "    \"\"\"\n",
    "    Combines all project files without any column standardization,\n",
    "    keeping all original columns and populating with NaN where needed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str\n",
    "        Base directory containing repository folders with project-level data\n",
    "    output_file : str\n",
    "        Path to save the combined data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Combined DataFrame with all original columns\n",
    "    \"\"\"\n",
    "    # Check if the base directory exists\n",
    "    if not os.path.exists(base_dir):\n",
    "        raise ValueError(f\"Base directory not found: {base_dir}\")\n",
    "    \n",
    "    # Get a list of all repository folders\n",
    "    repo_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "    if not repo_folders:\n",
    "        raise ValueError(f\"No repository folders found in {base_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(repo_folders)} repository folders in {base_dir}\")\n",
    "    \n",
    "    # List to store all project data rows\n",
    "    all_projects = []\n",
    "    \n",
    "    # Process each repository folder\n",
    "    for repo_folder in repo_folders:\n",
    "        repo_path = os.path.join(base_dir, repo_folder)\n",
    "        \n",
    "        # Find all project-level CSV files in the repository folder\n",
    "        csv_files = glob.glob(os.path.join(repo_path, \"project_level_*.csv\"))\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"No project-level CSV files found in {repo_path}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing repository: {repo_folder} - Found {len(csv_files)} project-level files\")\n",
    "        \n",
    "        # Process each CSV file in the repository folder\n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                # Read the project data\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Add repository and source file information\n",
    "                df['repository'] = repo_folder\n",
    "                df['source_file'] = os.path.basename(csv_file)\n",
    "                \n",
    "                # Add to the list of projects\n",
    "                all_projects.append(df)\n",
    "                \n",
    "                print(f\"  Added project data from {os.path.basename(csv_file)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {csv_file}: {str(e)}\")\n",
    "    \n",
    "    if not all_projects:\n",
    "        print(\"No project data was found or successfully processed.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatenate all project DataFrames with all columns\n",
    "    combined_df = pd.concat(all_projects, ignore_index=True)\n",
    "    \n",
    "    # Count NaN values per column\n",
    "    nan_counts = combined_df.isna().sum().sort_values(ascending=False)\n",
    "    nan_percentages = (nan_counts / len(combined_df) * 100).round(2)\n",
    "    \n",
    "    # Print columns with highest NaN percentages\n",
    "    print(\"\\nColumns with highest NaN percentages:\")\n",
    "    for col, pct in nan_percentages.head(20).items():\n",
    "        print(f\"  {col}: {pct}% missing ({nan_counts[col]} out of {len(combined_df)} rows)\")\n",
    "    \n",
    "    # Save to file\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"Combined raw project data saved to {output_file}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    PROJECT_DATA_DIR = \"./project_level_data\"\n",
    "    COMBINED_RAW_FILE = \"./project_level_data/combined/combined_projects_raw.csv\"\n",
    "    COLUMN_INVENTORY_FILE = \"./project_level_data/combined/column_inventory.json\"\n",
    "    \n",
    "    # First, inventory all columns across projects\n",
    "    inventory = inventory_project_columns(PROJECT_DATA_DIR, COLUMN_INVENTORY_FILE)\n",
    "    \n",
    "    # Then combine all projects without standardization to see the raw data\n",
    "    combined_df = combine_all_projects_raw(PROJECT_DATA_DIR, COMBINED_RAW_FILE)\n",
    "    \n",
    "    print(f\"\\nRaw combination complete with {len(combined_df)} projects and {len(combined_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './project_level_data/combined_projects_raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./project_level_data/combined_projects_raw.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Start a D-Tale session and open it in the browser\u001b[39;00m\n\u001b[1;32m     11\u001b[0m d \u001b[38;5;241m=\u001b[39m dtale\u001b[38;5;241m.\u001b[39mshow(df, ignore_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_cell_edits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './project_level_data/combined_projects_raw.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./project_level_data/combined/combined_projects_raw.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
