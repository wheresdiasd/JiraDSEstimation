{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Task Effort Estimation Model\n",
    "\n",
    "This notebook focuses on estimating individual task effort using the cleaned Jira dataset. The goal is to build models that can predict how long it will take to complete a task based on its characteristics.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "To control how much of the dataset is used for analysis and model training, adjust the `DATA_SAMPLE_PERCENTAGE` parameter at the top of the notebook:\n",
    "\n",
    "- Set to `1.0` to use the full dataset (default)\n",
    "- Set to a lower value (e.g., `0.1` for 10%, `0.5` for 50%) to use a subset of the data\n",
    "- Smaller datasets are faster to process but may result in less accurate models\n",
    "- The sampling is stratified by issue type when possible to maintain class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from xgboost import XGBRegressor\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Parameter to control the percentage of dataset to use (between 0 and 1)\n",
    "# Adjust this parameter to control dataset size throughout the analysis\n",
    "DATA_SAMPLE_PERCENTAGE = 0.1  # Default: 100% of data\n",
    "\n",
    "# Create directory for results\n",
    "results_dir = 'individual_task_effort_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define a function to format column groups\n",
    "def format_column_group(col):\n",
    "    if col in ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', \n",
    "              'is_type_new_feature', 'is_type_epic', 'is_type_sub-task']:\n",
    "        return 'Issue Type'\n",
    "    elif col in ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', \n",
    "               'is_priority_minor', 'is_priority_trivial']:\n",
    "        return 'Priority'\n",
    "    elif col in ['inward_count', 'outward_count']:\n",
    "        return 'Relationships'\n",
    "    elif col in ['age_days', 'created_is_weekend', 'created_hour', 'created_month', 'created_year']:\n",
    "        return 'Temporal'\n",
    "    elif col in ['fields.creator.active', 'fields.issuetype.id', 'is_completed', 'is_resolved']:\n",
    "        return 'Status'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned Jira dataset...\n",
      "Dataset loaded: 2259837 tasks, 40 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fields.issuetype.id</th>\n",
       "      <th>fields.issuetype.name</th>\n",
       "      <th>fields.project.id</th>\n",
       "      <th>fields.project.key</th>\n",
       "      <th>fields.project.name</th>\n",
       "      <th>fields.created</th>\n",
       "      <th>fields.priority.name</th>\n",
       "      <th>fields.priority.id</th>\n",
       "      <th>fields.updated</th>\n",
       "      <th>...</th>\n",
       "      <th>is_type_story</th>\n",
       "      <th>is_type_improvement</th>\n",
       "      <th>is_type_new_feature</th>\n",
       "      <th>is_type_epic</th>\n",
       "      <th>is_type_sub-task</th>\n",
       "      <th>is_priority_blocker</th>\n",
       "      <th>is_priority_critical</th>\n",
       "      <th>is_priority_major</th>\n",
       "      <th>is_priority_minor</th>\n",
       "      <th>is_priority_trivial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96268.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Task</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>MDBF</td>\n",
       "      <td>MariaDB Foundation Development</td>\n",
       "      <td>2021-01-25 23:18:41+00:00</td>\n",
       "      <td>Major</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-01-25 23:21:20+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96267.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Task</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>MDBF</td>\n",
       "      <td>MariaDB Foundation Development</td>\n",
       "      <td>2021-01-25 23:10:25+00:00</td>\n",
       "      <td>Major</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-01-25 23:11:51+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96086.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bug</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>MDBF</td>\n",
       "      <td>MariaDB Foundation Development</td>\n",
       "      <td>2021-01-20 03:02:50+00:00</td>\n",
       "      <td>Major</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-01-20 05:56:39+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96055.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Task</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>MDBF</td>\n",
       "      <td>MariaDB Foundation Development</td>\n",
       "      <td>2021-01-19 15:04:26+00:00</td>\n",
       "      <td>Critical</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-01-19 15:04:26+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96032.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Task</td>\n",
       "      <td>11901.0</td>\n",
       "      <td>MDBF</td>\n",
       "      <td>MariaDB Foundation Development</td>\n",
       "      <td>2021-01-18 19:42:44+00:00</td>\n",
       "      <td>Major</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-01-25 23:07:20+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  fields.issuetype.id fields.issuetype.name  fields.project.id  \\\n",
       "0  96268.0                  3.0                  Task            11901.0   \n",
       "1  96267.0                  3.0                  Task            11901.0   \n",
       "2  96086.0                  1.0                   Bug            11901.0   \n",
       "3  96055.0                  3.0                  Task            11901.0   \n",
       "4  96032.0                  3.0                  Task            11901.0   \n",
       "\n",
       "  fields.project.key             fields.project.name  \\\n",
       "0               MDBF  MariaDB Foundation Development   \n",
       "1               MDBF  MariaDB Foundation Development   \n",
       "2               MDBF  MariaDB Foundation Development   \n",
       "3               MDBF  MariaDB Foundation Development   \n",
       "4               MDBF  MariaDB Foundation Development   \n",
       "\n",
       "              fields.created fields.priority.name  fields.priority.id  \\\n",
       "0  2021-01-25 23:18:41+00:00                Major                 3.0   \n",
       "1  2021-01-25 23:10:25+00:00                Major                 3.0   \n",
       "2  2021-01-20 03:02:50+00:00                Major                 3.0   \n",
       "3  2021-01-19 15:04:26+00:00             Critical                 2.0   \n",
       "4  2021-01-18 19:42:44+00:00                Major                 3.0   \n",
       "\n",
       "              fields.updated  ... is_type_story is_type_improvement  \\\n",
       "0  2021-01-25 23:21:20+00:00  ...           0.0                 0.0   \n",
       "1  2021-01-25 23:11:51+00:00  ...           0.0                 0.0   \n",
       "2  2021-01-20 05:56:39+00:00  ...           0.0                 0.0   \n",
       "3  2021-01-19 15:04:26+00:00  ...           0.0                 0.0   \n",
       "4  2021-01-25 23:07:20+00:00  ...           0.0                 0.0   \n",
       "\n",
       "   is_type_new_feature  is_type_epic  is_type_sub-task  is_priority_blocker  \\\n",
       "0                  0.0           0.0               0.0                  0.0   \n",
       "1                  0.0           0.0               0.0                  0.0   \n",
       "2                  0.0           0.0               0.0                  0.0   \n",
       "3                  0.0           0.0               0.0                  0.0   \n",
       "4                  0.0           0.0               0.0                  0.0   \n",
       "\n",
       "   is_priority_critical  is_priority_major  is_priority_minor  \\\n",
       "0                   0.0                1.0                0.0   \n",
       "1                   0.0                1.0                0.0   \n",
       "2                   0.0                1.0                0.0   \n",
       "3                   1.0                0.0                0.0   \n",
       "4                   0.0                1.0                0.0   \n",
       "\n",
       "   is_priority_trivial  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the cleaned Jira dataset\n",
    "print(\"Loading cleaned Jira dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv('cleaned_jira_dataset.csv')\n",
    "    print(f\"Dataset loaded: {df.shape[0]} tasks, {df.shape[1]} features\")\n",
    "\n",
    "    # Apply simple random sampling if percentage is less than 100%\n",
    "    if DATA_SAMPLE_PERCENTAGE < 1.0:\n",
    "        df = df.sample(frac=DATA_SAMPLE_PERCENTAGE, random_state=42)\n",
    "        print(f\"Applied {DATA_SAMPLE_PERCENTAGE*100:.1f}% sampling: {df.shape[0]} tasks retained\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values...\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Missing Values', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=missing_df.index[:15], y=missing_df['Percentage'][:15])\n",
    "plt.title('Top 15 Columns with Missing Values')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Percentage Missing')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/missing_values.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target variable distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Resolution hours distribution (before filtering)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['resolution_hours'], bins=50, kde=True)\n",
    "plt.title('Resolution Hours Distribution')\n",
    "plt.xlabel('Resolution Hours')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Log-transformed resolution hours\n",
    "plt.subplot(1, 2, 2)\n",
    "df['log_resolution_hours'] = np.log1p(df['resolution_hours'])\n",
    "sns.histplot(df['log_resolution_hours'], bins=50, kde=True)\n",
    "plt.title('Log(Resolution Hours) Distribution')\n",
    "plt.xlabel('Log(Resolution Hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/target_distribution_raw.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget variable statistics (before filtering):\")\n",
    "print(f\"  Mean: {df['resolution_hours'].mean():.2f} hours\")\n",
    "print(f\"  Median: {df['resolution_hours'].median():.2f} hours\")\n",
    "print(f\"  Min: {df['resolution_hours'].min():.2f} hours\")\n",
    "print(f\"  Max: {df['resolution_hours'].max():.2f} hours\")\n",
    "print(f\"  Standard Deviation: {df['resolution_hours'].std():.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "print(\"\\nCleaning the dataset...\")\n",
    "\n",
    "# 1. Filter out tasks with invalid resolution hours\n",
    "df_filtered = df.copy()\n",
    "df_filtered = df_filtered.dropna(subset=['resolution_hours'])\n",
    "df_filtered = df_filtered[df_filtered['resolution_hours'] >= 0]\n",
    "\n",
    "# 2. Cap extremely long resolution times (e.g., > 6 months)\n",
    "resolution_cap = 6 * 30 * 24  # 6 months in hours\n",
    "long_tasks = df_filtered['resolution_hours'] > resolution_cap\n",
    "print(f\"Capped {long_tasks.sum()} tasks with resolution times > {resolution_cap} hours (6 months)\")\n",
    "df_filtered.loc[long_tasks, 'resolution_hours'] = resolution_cap\n",
    "df_filtered.loc[long_tasks, 'log_resolution_hours'] = np.log1p(resolution_cap)\n",
    "\n",
    "# 3. Remove columns with high percentage of missing values (e.g., > 50%)\n",
    "high_missing_cols = missing_df[missing_df['Percentage'] > 50].index.tolist()\n",
    "df_filtered = df_filtered.drop(columns=high_missing_cols, errors='ignore')\n",
    "print(f\"Removed {len(high_missing_cols)} columns with more than 50% missing values\")\n",
    "\n",
    "# 4. Fill remaining missing values\n",
    "# For numeric columns, fill with median\n",
    "numeric_cols = df_filtered.select_dtypes(include=['number']).columns\n",
    "df_filtered[numeric_cols] = df_filtered[numeric_cols].fillna(df_filtered[numeric_cols].median())\n",
    "\n",
    "# For categorical columns, fill with mode\n",
    "categorical_cols = df_filtered.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].mode()[0])\n",
    "\n",
    "# 5. Check for remaining missing values\n",
    "remaining_missing = df_filtered.isnull().sum().sum()\n",
    "print(f\"Remaining missing values after cleaning: {remaining_missing}\")\n",
    "\n",
    "# Display cleaned dataset info\n",
    "print(f\"\\nCleaned dataset shape: {df_filtered.shape[0]} tasks, {df_filtered.shape[1]} features\")\n",
    "\n",
    "# Visualize cleaned target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Resolution hours distribution (after cleaning)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_filtered['resolution_hours'], bins=50, kde=True)\n",
    "plt.title('Resolution Hours Distribution (Cleaned)')\n",
    "plt.xlabel('Resolution Hours')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Log-transformed resolution hours\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_filtered['log_resolution_hours'], bins=50, kde=True)\n",
    "plt.title('Log(Resolution Hours) Distribution (Cleaned)')\n",
    "plt.xlabel('Log(Resolution Hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/target_distribution_cleaned.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTarget variable statistics (after cleaning):\")\n",
    "print(f\"  Mean: {df_filtered['resolution_hours'].mean():.2f} hours\")\n",
    "print(f\"  Median: {df_filtered['resolution_hours'].median():.2f} hours\")\n",
    "print(f\"  Min: {df_filtered['resolution_hours'].min():.2f} hours\")\n",
    "print(f\"  Max: {df_filtered['resolution_hours'].max():.2f} hours\")\n",
    "print(f\"  Standard Deviation: {df_filtered['resolution_hours'].std():.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relationships between task characteristics and resolution time\n",
    "print(\"\\nExploring relationships with resolution time...\")\n",
    "\n",
    "# 1. Issue type impact on resolution time\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', 'is_type_new_feature', 'is_type_epic', 'is_type_sub-task']:\n",
    "    if col in df_filtered.columns:\n",
    "        plt.subplot(2, 4, ['is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement', \n",
    "                         'is_type_new_feature', 'is_type_epic', 'is_type_sub-task'].index(col) % 8 + 1)\n",
    "        \n",
    "        # Create groups based on the issue type\n",
    "        type_true = df_filtered[df_filtered[col] == 1]['resolution_hours']\n",
    "        type_false = df_filtered[df_filtered[col] == 0]['resolution_hours']\n",
    "        \n",
    "        # Boxplot for this issue type\n",
    "        plt.boxplot([type_true, type_false], labels=['Yes', 'No'])\n",
    "        plt.title(f'{col.replace(\"is_type_\", \"\").replace(\"_\", \" \").title()}')\n",
    "        plt.ylabel('Resolution Hours')\n",
    "        \n",
    "        # Print mean resolution time for this type\n",
    "        print(f\"{col.replace('is_type_', '').replace('_', ' ').title()} - \"\n",
    "              f\"Mean resolution time: Yes={type_true.mean():.2f}, No={type_false.mean():.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/issue_type_impact.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Priority impact\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', 'is_priority_minor', 'is_priority_trivial']:\n",
    "    if col in df_filtered.columns:\n",
    "        plt.subplot(2, 3, ['is_priority_blocker', 'is_priority_critical', 'is_priority_major', \n",
    "                         'is_priority_minor', 'is_priority_trivial'].index(col) % 6 + 1)\n",
    "        \n",
    "        # Create groups based on the priority\n",
    "        priority_true = df_filtered[df_filtered[col] == 1]['resolution_hours']\n",
    "        priority_false = df_filtered[df_filtered[col] == 0]['resolution_hours']\n",
    "        \n",
    "        # Boxplot for this priority\n",
    "        plt.boxplot([priority_true, priority_false], labels=['Yes', 'No'])\n",
    "        plt.title(f'{col.replace(\"is_priority_\", \"\").replace(\"_\", \" \").title()}')\n",
    "        plt.ylabel('Resolution Hours')\n",
    "        \n",
    "        # Print mean resolution time for this priority\n",
    "        print(f\"{col.replace('is_priority_', '').replace('_', ' ').title()} - \"\n",
    "              f\"Mean resolution time: Yes={priority_true.mean():.2f}, No={priority_false.mean():.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/priority_impact.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation between numeric features and resolution time\n",
    "numeric_features = df_filtered.select_dtypes(include=['number']).columns.tolist()\n",
    "numeric_features.remove('resolution_hours')  # Remove target from features\n",
    "if 'log_resolution_hours' in numeric_features:\n",
    "    numeric_features.remove('log_resolution_hours')\n",
    "\n",
    "# Calculate correlations with resolution time\n",
    "correlations = []\n",
    "for col in numeric_features:\n",
    "    corr = df_filtered[col].corr(df_filtered['resolution_hours'])\n",
    "    correlations.append((col, corr))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display top 15 correlated features\n",
    "top_corr = pd.DataFrame(correlations[:15], columns=['Feature', 'Correlation'])\n",
    "print(\"\\nTop 15 features correlated with resolution time:\")\n",
    "print(top_corr)\n",
    "\n",
    "# Visualize top 10 correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "top10 = pd.DataFrame(correlations[:10], columns=['Feature', 'Correlation'])\n",
    "sns.barplot(x='Correlation', y='Feature', data=top10)\n",
    "plt.title('Top 10 Features Correlated with Resolution Time')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/top_correlations.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Heatmap of correlations between top features\n",
    "top_features = [x[0] for x in correlations[:15]] + ['resolution_hours']\n",
    "corr_matrix = df_filtered[top_features].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Top Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature categories and their correlations with resolution time\n",
    "print(\"\\nAnalyzing feature categories...\")\n",
    "\n",
    "# Group features by category\n",
    "category_features = {}\n",
    "for col in numeric_features:\n",
    "    category = format_column_group(col)\n",
    "    if category not in category_features:\n",
    "        category_features[category] = []\n",
    "    category_features[category].append(col)\n",
    "\n",
    "# Calculate average correlation by category\n",
    "category_correlations = []\n",
    "for category, features in category_features.items():\n",
    "    category_corr = np.mean([abs(df_filtered[feat].corr(df_filtered['resolution_hours'])) for feat in features])\n",
    "    category_correlations.append((category, category_corr, len(features)))\n",
    "\n",
    "# Sort by correlation strength\n",
    "category_correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display category correlations\n",
    "category_corr_df = pd.DataFrame(category_correlations, columns=['Category', 'Avg. Abs. Correlation', 'Feature Count'])\n",
    "print(\"Average absolute correlation by feature category:\")\n",
    "print(category_corr_df)\n",
    "\n",
    "# Visualize category correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Avg. Abs. Correlation', y='Category', data=category_corr_df)\n",
    "plt.title('Average Correlation Strength by Feature Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/category_correlations.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target for modeling\n",
    "print(\"\\nPreparing features for modeling...\")\n",
    "\n",
    "# 1. Select features for model training\n",
    "# Remove ID columns and target-related columns\n",
    "id_columns = ['id', 'fields.issuetype.id', 'fields.project.id', 'fields.priority.id']\n",
    "target_columns = ['resolution_hours', 'log_resolution_hours']\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_drop = id_columns + target_columns\n",
    "\n",
    "# Drop columns that aren't useful for modeling\n",
    "for col in df_filtered.columns:\n",
    "    # Drop date columns that have been converted to features\n",
    "    if col in ['fields.created', 'fields.updated']:\n",
    "        columns_to_drop.append(col)\n",
    "        \n",
    "    # Drop any string/object columns that haven't been encoded\n",
    "    elif df_filtered[col].dtype == 'object' and col not in id_columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "# Remove columns that don't exist\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df_filtered.columns]\n",
    "\n",
    "# Create feature matrix\n",
    "X = df_filtered.drop(columns=columns_to_drop)\n",
    "\n",
    "# Set target variable (log-transformed for better model performance)\n",
    "y = df_filtered['log_resolution_hours']\n",
    "\n",
    "# Display selected features\n",
    "print(f\"Selected {X.shape[1]} features for modeling\")\n",
    "print(\"Feature list:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# 2. Check for multicollinearity\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "# Find highly correlated pairs (|r| > 0.8)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs (|r| > 0.8):\")\n",
    "    for col1, col2, corr in high_corr_pairs:\n",
    "        print(f\"{col1} <-> {col2}: {corr:.3f}\")\n",
    "\n",
    "    # Create a set of features to drop (keep the one with higher correlation to target)\n",
    "    to_drop = set()\n",
    "    for col1, col2, _ in high_corr_pairs:\n",
    "        corr1 = abs(df_filtered[col1].corr(df_filtered['resolution_hours']))\n",
    "        corr2 = abs(df_filtered[col2].corr(df_filtered['resolution_hours']))\n",
    "        if corr1 >= corr2:\n",
    "            to_drop.add(col2)\n",
    "        else:\n",
    "            to_drop.add(col1)\n",
    "    \n",
    "    print(f\"\\nDropping {len(to_drop)} features due to multicollinearity:\")\n",
    "    print(list(to_drop))\n",
    "    X = X.drop(columns=list(to_drop))\n",
    "\n",
    "# 3. Split data into training, validation, and test sets\n",
    "print(\"\\nSplitting data into train, validation, and test sets...\")\n",
    "\n",
    "# First split: 80% train+validation, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% validation (60%/20% of original data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Save data splits for reproducibility\n",
    "splits = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'log_transform': True  # We're using log-transformed target\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump(splits, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation (Initial Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate regression models\n",
    "print(\"\\nTraining initial regression models...\")\n",
    "\n",
    "# Function to evaluate models and return metrics\n",
    "def evaluate_model(model, X_val, y_val, model_name, is_val=True):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Convert back from log space\n",
    "    y_val_orig = np.expm1(y_val)\n",
    "    y_pred_orig = np.expm1(y_pred)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_val_orig, y_pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_orig, y_pred_orig))\n",
    "    r2 = r2_score(y_val_orig, y_pred_orig)\n",
    "    \n",
    "    # Calculate Spearman rank correlation (handles non-linear relationships)\n",
    "    spearman_corr, _ = spearmanr(y_val_orig, y_pred_orig)\n",
    "    \n",
    "    # Print results\n",
    "    dataset = \"Validation\" if is_val else \"Test\"\n",
    "    print(f\"\\n{model_name} ({dataset} Set):\")\n",
    "    print(f\"  MAE: {mae:.2f} hours\")\n",
    "    print(f\"  RMSE: {rmse:.2f} hours\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    \n",
    "    # Create actual vs. predicted plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_val_orig, y_pred_orig, alpha=0.3)\n",
    "    plt.plot([0, y_val_orig.max()], [0, y_val_orig.max()], 'r--')\n",
    "    plt.title(f'{model_name}: Actual vs. Predicted')\n",
    "    plt.xlabel('Actual Resolution Hours')\n",
    "    plt.ylabel('Predicted Resolution Hours')\n",
    "    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot residuals\n",
    "    residuals = y_val_orig - y_pred_orig\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred_orig, residuals, alpha=0.3)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(f'{model_name}: Residuals')\n",
    "    plt.xlabel('Predicted Resolution Hours')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_residuals.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'name': model_name,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'spearman': spearman_corr,\n",
    "        'predictions': y_pred,\n",
    "        'original_predictions': y_pred_orig,\n",
    "        'original_values': y_val_orig\n",
    "    }\n",
    "\n",
    "# Define base models\n",
    "base_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    results = evaluate_model(model, X_val, y_val, name)\n",
    "    results['train_time'] = train_time\n",
    "    model_results[name] = results\n",
    "    \n",
    "    # Save model\n",
    "    with open(f'{results_dir}/original_{name.replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Create ensemble predictions (average of all models)\n",
    "print(\"\\nCreating ensemble prediction...\")\n",
    "y_pred_ensemble = np.zeros_like(y_val)\n",
    "for name, results in model_results.items():\n",
    "    y_pred_ensemble += results['predictions']\n",
    "y_pred_ensemble /= len(model_results)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_results = {\n",
    "    'name': 'Ensemble',\n",
    "    'predictions': y_pred_ensemble,\n",
    "    'original_predictions': np.expm1(y_pred_ensemble),\n",
    "    'original_values': np.expm1(y_val)\n",
    "}\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "ensemble_results['mae'] = mean_absolute_error(\n",
    "    ensemble_results['original_values'], \n",
    "    ensemble_results['original_predictions']\n",
    ")\n",
    "ensemble_results['rmse'] = np.sqrt(mean_squared_error(\n",
    "    ensemble_results['original_values'], \n",
    "    ensemble_results['original_predictions']\n",
    "))\n",
    "ensemble_results['r2'] = r2_score(\n",
    "    ensemble_results['original_values'], \n",
    "    ensemble_results['original_predictions']\n",
    ")\n",
    "ensemble_results['spearman'], _ = spearmanr(\n",
    "    ensemble_results['original_values'], \n",
    "    ensemble_results['original_predictions']\n",
    ")\n",
    "\n",
    "# Print ensemble results\n",
    "print(f\"\\nEnsemble (Validation Set):\")\n",
    "print(f\"  MAE: {ensemble_results['mae']:.2f} hours\")\n",
    "print(f\"  RMSE: {ensemble_results['rmse']:.2f} hours\")\n",
    "print(f\"  R²: {ensemble_results['r2']:.4f}\")\n",
    "print(f\"  Spearman Correlation: {ensemble_results['spearman']:.4f}\")\n",
    "\n",
    "# Create ensemble plots\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    ensemble_results['original_values'], \n",
    "    ensemble_results['original_predictions'], \n",
    "    alpha=0.3\n",
    ")\n",
    "plt.plot([0, ensemble_results['original_values'].max()], \n",
    "         [0, ensemble_results['original_values'].max()], 'r--')\n",
    "plt.title('Ensemble: Actual vs. Predicted')\n",
    "plt.xlabel('Actual Resolution Hours')\n",
    "plt.ylabel('Predicted Resolution Hours')\n",
    "plt.savefig(f'{results_dir}/ensemble_predictions.png')\n",
    "plt.close()\n",
    "\n",
    "# Add ensemble to results\n",
    "model_results['Ensemble'] = ensemble_results\n",
    "\n",
    "# Save original results\n",
    "with open(f'{results_dir}/original_results.pkl', 'wb') as f:\n",
    "    pickle.dump(model_results, f)\n",
    "\n",
    "# Save original hyperparameters\n",
    "original_hyperparams = {\n",
    "    'Random Forest': {**base_models['Random Forest'].get_params()},\n",
    "    'Gradient Boosting': {**base_models['Gradient Boosting'].get_params()},\n",
    "    'XGBoost': {**base_models['XGBoost'].get_params()}\n",
    "}\n",
    "\n",
    "with open(f'{results_dir}/original_hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(original_hyperparams, f)\n",
    "\n",
    "# Compare model performance\n",
    "performance_comparison = pd.DataFrame([\n",
    "    (name, results['mae'], results['rmse'], results['r2'], results['spearman'])\n",
    "    for name, results in model_results.items()\n",
    "], columns=['Model', 'MAE', 'RMSE', 'R²', 'Spearman'])\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(performance_comparison.sort_values('Spearman', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "print(\"\\nAnalyzing feature importance...\")\n",
    "\n",
    "# Create function to visualize feature importance\n",
    "def plot_feature_importance(model, model_name, top_n=20):\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(f\"{model_name} does not provide feature importances\")\n",
    "        return None\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Get top N features\n",
    "    top_features = feature_importance.head(top_n)\n",
    "    \n",
    "    # Plot importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])\n",
    "    plt.title(f'{model_name}: Top {top_n} Feature Importance')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dir}/{model_name.replace(\" \", \"_\").lower()}_feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Analyze each model's feature importance\n",
    "feature_importance_results = {}\n",
    "for name, results in model_results.items():\n",
    "    if name != 'Ensemble' and hasattr(results['model'], 'feature_importances_'):\n",
    "        print(f\"\\nAnalyzing feature importance for {name}...\")\n",
    "        importance = plot_feature_importance(results['model'], name)\n",
    "        \n",
    "        if importance is not None:\n",
    "            feature_importance_results[name] = importance\n",
    "            \n",
    "            # Print top 10 features\n",
    "            print(f\"Top 10 features for {name}:\")\n",
    "            print(importance.head(10))\n",
    "\n",
    "# Find most common important features across models\n",
    "if feature_importance_results:\n",
    "    print(\"\\nIdentifying most common important features across models...\")\n",
    "    \n",
    "    # Get top 10 features from each model\n",
    "    top_features_by_model = {}\n",
    "    all_top_features = set()\n",
    "    \n",
    "    for name, importance in feature_importance_results.items():\n",
    "        top10 = importance.head(10)['Feature'].tolist()\n",
    "        top_features_by_model[name] = top10\n",
    "        all_top_features.update(top10)\n",
    "    \n",
    "    # Count feature occurrences across models\n",
    "    feature_counts = {}\n",
    "    for feature in all_top_features:\n",
    "        count = sum(1 for model_features in top_features_by_model.values() if feature in model_features)\n",
    "        feature_counts[feature] = count\n",
    "    \n",
    "    # Sort by count\n",
    "    sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Features appearing in multiple models' top 10:\")\n",
    "    for feature, count in sorted_features:\n",
    "        if count > 1:\n",
    "            print(f\"  {feature}: {count} models\")\n",
    "    \n",
    "    # Create consolidated feature importance\n",
    "    best_model = performance_comparison.sort_values('R²', ascending=False).iloc[0]['Model']\n",
    "    best_importance = feature_importance_results[best_model]\n",
    "    \n",
    "    # Save best feature importance\n",
    "    best_importance.to_csv(f'{results_dir}/best_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Create a plot of the best model's feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_n = 20\n",
    "    top_features = best_importance.head(top_n)\n",
    "    plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1])\n",
    "    plt.title(f'Top {top_n} Most Important Features for Task Effort Estimation')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{results_dir}/best_feature_importance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter tuning for the models\nprint(\"\\nPerforming hyperparameter tuning...\")\n\n# Define hyperparameter search spaces\nparam_spaces = {\n    'Random Forest': {\n        'n_estimators': [100, 200, 300, 500],\n        'max_depth': [None, 10, 20, 30, 40],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'max_features': ['auto', 'sqrt']\n    },\n    'Gradient Boosting': {\n        'n_estimators': [100, 200, 300, 500],\n        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n        'max_depth': [3, 5, 7, 9],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'subsample': [0.8, 0.9, 1.0]\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200, 300, 500],\n        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n        'max_depth': [3, 5, 7, 9],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'gamma': [0, 0.1, 0.2]\n    }\n}\n\n# Define scoring metric for hyperparameter tuning\ndef spearman_scorer(estimator, X, y):\n    y_pred = estimator.predict(X)\n    # Convert from log space\n    y_orig = np.expm1(y)\n    y_pred_orig = np.expm1(y_pred)\n    corr, _ = spearmanr(y_orig, y_pred_orig)\n    return corr\n\n# Function to tune hyperparameters\ndef tune_model(model, params, model_name, X_train, y_train, X_val, y_val, n_iter=20):\n    print(f\"\\nTuning hyperparameters for {model_name}...\")\n    \n    # Initialize RandomizedSearchCV\n    random_search = RandomizedSearchCV(\n        model, param_distributions=params, n_iter=n_iter,\n        scoring=spearman_scorer, cv=5, random_state=42, n_jobs=-1\n    )\n    \n    # Fit the search\n    start_time = time.time()\n    random_search.fit(X_train, y_train)\n    tuning_time = time.time() - start_time\n    print(f\"Tuning completed in {tuning_time:.2f} seconds\")\n    \n    # Get best model and parameters\n    best_model = random_search.best_estimator_\n    best_params = random_search.best_params_\n    best_score = random_search.best_score_\n    \n    print(f\"Best CV score: {best_score:.4f}\")\n    print(f\"Best parameters: {best_params}\")\n    \n    # Evaluate the tuned model\n    tuned_results = evaluate_model(best_model, X_val, y_val, f\"Tuned {model_name}\")\n    tuned_results['best_params'] = best_params\n    tuned_results['cv_score'] = best_score\n    tuned_results['tuning_time'] = tuning_time\n    \n    # Save tuned model\n    with open(f'{results_dir}/tuned_{model_name.replace(\" \", \"_\")}_model.pkl', 'wb') as f:\n        pickle.dump(best_model, f)\n    \n    return tuned_results, random_search.cv_results_\n\n# Store tuned results\ntuned_model_results = {}\ncv_results_dict = {}\n\n# Tune each model\nfor name, model in base_models.items():\n    tuned_results, cv_results = tune_model(\n        model, param_spaces[name], name,\n        X_train, y_train, X_val, y_val\n    )\n    tuned_model_results[name] = tuned_results\n    cv_results_dict[name] = cv_results\n\n# Save tuning results\nwith open(f'{results_dir}/tuned_results.pkl', 'wb') as f:\n    pickle.dump(tuned_model_results, f)\n\nwith open(f'{results_dir}/tuned_cv_results.pkl', 'wb') as f:\n    pickle.dump(cv_results_dict, f)\n\n# Create tuned ensemble predictions\nprint(\"\\nCreating tuned ensemble prediction...\")\ny_pred_tuned_ensemble = np.zeros_like(y_val)\nfor name, results in tuned_model_results.items():\n    y_pred_tuned_ensemble += results['predictions']\ny_pred_tuned_ensemble /= len(tuned_model_results)\n\n# Evaluate tuned ensemble\ntuned_ensemble_results = {\n    'name': 'Tuned Ensemble',\n    'predictions': y_pred_tuned_ensemble,\n    'original_predictions': np.expm1(y_pred_tuned_ensemble),\n    'original_values': np.expm1(y_val)\n}\n\n# Calculate tuned ensemble metrics\ntuned_ensemble_results['mae'] = mean_absolute_error(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\ntuned_ensemble_results['rmse'] = np.sqrt(mean_squared_error(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n))\ntuned_ensemble_results['r2'] = r2_score(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\ntuned_ensemble_results['spearman'], _ = spearmanr(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions']\n)\n\n# Print tuned ensemble results\nprint(f\"\\nTuned Ensemble (Validation Set):\")\nprint(f\"  MAE: {tuned_ensemble_results['mae']:.2f} hours\")\nprint(f\"  RMSE: {tuned_ensemble_results['rmse']:.2f} hours\")\nprint(f\"  R²: {tuned_ensemble_results['r2']:.4f}\")\nprint(f\"  Spearman Correlation: {tuned_ensemble_results['spearman']:.4f}\")\n\n# Create tuned ensemble plots\nplt.figure(figsize=(8, 6))\nplt.scatter(\n    tuned_ensemble_results['original_values'], \n    tuned_ensemble_results['original_predictions'], \n    alpha=0.3\n)\nplt.plot([0, tuned_ensemble_results['original_values'].max()], \n         [0, tuned_ensemble_results['original_values'].max()], 'r--')\nplt.title('Tuned Ensemble: Actual vs. Predicted')\nplt.xlabel('Actual Resolution Hours')\nplt.ylabel('Predicted Resolution Hours')\nplt.savefig(f'{results_dir}/tuned_ensemble_predictions.png')\nplt.close()\n\n# Add tuned ensemble to results\ntuned_model_results['Ensemble'] = tuned_ensemble_results\n\n# Compare original vs. tuned model performance\nmodel_comparison = []\nfor name in base_models.keys():\n    original_results = model_results[name]\n    tuned_results = tuned_model_results[name]\n    \n    model_comparison.append({\n        'Model': name,\n        'Original MAE': original_results['mae'],\n        'Tuned MAE': tuned_results['mae'],\n        'MAE Improvement': original_results['mae'] - tuned_results['mae'],\n        'MAE Improvement %': (original_results['mae'] - tuned_results['mae']) / original_results['mae'] * 100,\n        'Original RMSE': original_results['rmse'],\n        'Tuned RMSE': tuned_results['rmse'],\n        'RMSE Improvement': original_results['rmse'] - tuned_results['rmse'],\n        'RMSE Improvement %': (original_results['rmse'] - tuned_results['rmse']) / original_results['rmse'] * 100,\n        'Original R²': original_results['r2'],\n        'Tuned R²': tuned_results['r2'],\n        'R² Improvement': tuned_results['r2'] - original_results['r2'],\n        'Original Spearman': original_results['spearman'],\n        'Tuned Spearman': tuned_results['spearman'],\n        'Spearman Improvement': tuned_results['spearman'] - original_results['spearman']\n    })\n\n# Add ensemble comparison\nmodel_comparison.append({\n    'Model': 'Ensemble',\n    'Original MAE': model_results['Ensemble']['mae'],\n    'Tuned MAE': tuned_ensemble_results['mae'],\n    'MAE Improvement': model_results['Ensemble']['mae'] - tuned_ensemble_results['mae'],\n    'MAE Improvement %': (model_results['Ensemble']['mae'] - tuned_ensemble_results['mae']) / model_results['Ensemble']['mae'] * 100,\n    'Original RMSE': model_results['Ensemble']['rmse'],\n    'Tuned RMSE': tuned_ensemble_results['rmse'],\n    'RMSE Improvement': model_results['Ensemble']['rmse'] - tuned_ensemble_results['rmse'],\n    'RMSE Improvement %': (model_results['Ensemble']['rmse'] - tuned_ensemble_results['rmse']) / model_results['Ensemble']['rmse'] * 100,\n    'Original R²': model_results['Ensemble']['r2'],\n    'Tuned R²': tuned_ensemble_results['r2'],\n    'R² Improvement': tuned_ensemble_results['r2'] - model_results['Ensemble']['r2'],\n    'Original Spearman': model_results['Ensemble']['spearman'],\n    'Tuned Spearman': tuned_ensemble_results['spearman'],\n    'Spearman Improvement': tuned_ensemble_results['spearman'] - model_results['Ensemble']['spearman']\n})\n\n# Create comparison DataFrame\ncomparison_df = pd.DataFrame(model_comparison)\nprint(\"\\nComparison of Original vs. Tuned Models:\")\nprint(comparison_df[['Model', 'Original MAE', 'Tuned MAE', 'MAE Improvement %', \n                     'Original R²', 'Tuned R²', 'R² Improvement',\n                     'Original Spearman', 'Tuned Spearman', 'Spearman Improvement']])\n\n# Save comparison\ncomparison_df.to_csv(f'{results_dir}/model_comparison.csv', index=False)\n\n# Create comparison directory\ncomparison_dir = f'{results_dir}/comparison'\nos.makedirs(comparison_dir, exist_ok=True)\n\n# Visualize improvements\n# MAE comparison\nplt.figure(figsize=(10, 6))\nbarWidth = 0.35\nr1 = np.arange(len(comparison_df))\nr2 = [x + barWidth for x in r1]\n\nplt.bar(r1, comparison_df['Original MAE'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned MAE'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('MAE Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Mean Absolute Error')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/MAE_comparison.png')\nplt.close()\n\n# RMSE comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original RMSE'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned RMSE'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('RMSE Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Root Mean Squared Error')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/RMSE_comparison.png')\nplt.close()\n\n# R² comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original R²'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned R²'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('R² Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('R²')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/R2_comparison.png')\nplt.close()\n\n# Spearman comparison\nplt.figure(figsize=(10, 6))\nplt.bar(r1, comparison_df['Original Spearman'], width=barWidth, label='Original', color='skyblue')\nplt.bar(r2, comparison_df['Tuned Spearman'], width=barWidth, label='Tuned', color='lightgreen')\nplt.title('Spearman Correlation Comparison: Original vs. Tuned Models')\nplt.xlabel('Model')\nplt.ylabel('Spearman Correlation')\nplt.xticks([r + barWidth/2 for r in range(len(comparison_df))], comparison_df['Model'])\nplt.legend()\nplt.savefig(f'{comparison_dir}/Spearman_comparison.png')\nplt.close()\n\n# Improvement heatmap\nimprovement_data = comparison_df[['Model', 'MAE Improvement %', 'RMSE Improvement %', 'R² Improvement', 'Spearman Improvement']]\nimprovement_data = improvement_data.set_index('Model')\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(improvement_data, annot=True, cmap='RdYlGn', center=0, fmt='.2f')\nplt.title('Model Improvement Heatmap')\nplt.tight_layout()\nplt.savefig(f'{comparison_dir}/improvement_heatmap.png')\nplt.close()\n\n# Best model comparison\nplt.figure(figsize=(12, 6))\n\n# Find best model based on Spearman correlation\nbest_model_idx = comparison_df['Tuned Spearman'].idxmax()\nbest_model = comparison_df.iloc[best_model_idx]['Model']\n\nmetrics = ['MAE', 'RMSE', 'R²', 'Spearman']\noriginal_values = [comparison_df.iloc[best_model_idx]['Original MAE'],\n                  comparison_df.iloc[best_model_idx]['Original RMSE'],\n                  comparison_df.iloc[best_model_idx]['Original R²'],\n                  comparison_df.iloc[best_model_idx]['Original Spearman']]\ntuned_values = [comparison_df.iloc[best_model_idx]['Tuned MAE'],\n                comparison_df.iloc[best_model_idx]['Tuned RMSE'],\n                comparison_df.iloc[best_model_idx]['Tuned R²'],\n                comparison_df.iloc[best_model_idx]['Tuned Spearman']]\n\n# Create barplot\nplt.subplot(1, 2, 1)\nbarWidth = 0.35\nr1 = np.arange(len(metrics))\nr2 = [x + barWidth for x in r1]\n\nplt.bar(r1[:2], original_values[:2], width=barWidth, label='Original', color='skyblue')  # MAE and RMSE\nplt.bar(r2[:2], tuned_values[:2], width=barWidth, label='Tuned', color='lightgreen')  # MAE and RMSE\nplt.title(f'Best Model: {best_model} (Error Metrics)')\nplt.ylabel('Error Value (lower is better)')\nplt.xticks([r + barWidth/2 for r in range(2)], metrics[:2])\nplt.legend()\n\n# Create barplot for R² and Spearman\nplt.subplot(1, 2, 2)\nplt.bar(r1[2:], original_values[2:], width=barWidth, label='Original', color='skyblue')  # R² and Spearman\nplt.bar(r2[2:], tuned_values[2:], width=barWidth, label='Tuned', color='lightgreen')  # R² and Spearman\nplt.title(f'Best Model: {best_model} (Correlation Metrics)')\nplt.ylabel('Correlation Value (higher is better)')\nplt.xticks([r + barWidth/2 for r in range(2)], metrics[2:])\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(f'{comparison_dir}/best_models_comparison.png')\nplt.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Evaluation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation on the test set\n",
    "print(\"\\nEvaluating best model on the test set...\")\n",
    "\n",
    "# Find the best model based on validation results\n",
    "model_names = list(tuned_model_results.keys())\n",
    "model_names.remove('Ensemble')  # Remove ensemble from consideration\n",
    "\n",
    "best_model_name = model_names[0]\n",
    "best_score = tuned_model_results[best_model_name]['spearman']\n",
    "\n",
    "for name in model_names[1:]:\n",
    "    if tuned_model_results[name]['spearman'] > best_score:\n",
    "        best_score = tuned_model_results[name]['spearman']\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"Best model is {best_model_name} with validation Spearman correlation of {best_score:.4f}\")\n",
    "\n",
    "# Load the best model\n",
    "with open(f'{results_dir}/tuned_{best_model_name.replace(\" \", \"_\")}_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate_model(best_model, X_test, y_test, f\"Best Model ({best_model_name})\", is_val=False)\n",
    "\n",
    "# Save as the best tuned model\n",
    "with open(f'{results_dir}/best_tuned_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Generate feature importance for the best model\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    best_importance = plot_feature_importance(best_model, f\"Best Model ({best_model_name})\")\n",
    "    best_importance.to_csv(f'{results_dir}/best_tuned_model_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Plot feature importance by category\n",
    "    if best_importance is not None:\n",
    "        # Add category to importance DataFrame\n",
    "        best_importance['Category'] = best_importance['Feature'].apply(format_column_group)\n",
    "        \n",
    "        # Aggregate by category\n",
    "        category_importance = best_importance.groupby('Category')['Importance'].sum().reset_index()\n",
    "        category_importance = category_importance.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot category importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.pie(category_importance['Importance'], labels=category_importance['Category'], \n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "        plt.title(f'Feature Importance by Category')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_dir}/best_tuned_model_feature_importance_by_category.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Also create a bar chart of category importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Category', data=category_importance)\n",
    "        plt.title('Feature Category Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{results_dir}/category_importance_bar.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save category importance\n",
    "        category_importance.to_csv(f'{results_dir}/category_importance.csv', index=False)\n",
    "\n",
    "# Create text summary of the best model\n",
    "with open(f'{results_dir}/model_summary.txt', 'w') as f:\n",
    "    f.write(f\"# Task Effort Estimation Model Summary\\n\\n\")\n",
    "    f.write(f\"## Best Model: {best_model_name}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"### Test Set Performance\\n\")\n",
    "    f.write(f\"- MAE: {test_results['mae']:.2f} hours\\n\")\n",
    "    f.write(f\"- RMSE: {test_results['rmse']:.2f} hours\\n\")\n",
    "    f.write(f\"- R²: {test_results['r2']:.4f}\\n\")\n",
    "    f.write(f\"- Spearman Correlation: {test_results['spearman']:.4f}\\n\\n\")\n",
    "    \n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        f.write(f\"### Top 10 Most Important Features\\n\")\n",
    "        for idx, row in best_importance.head(10).iterrows():\n",
    "            f.write(f\"- {row['Feature']}: {row['Importance']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Feature Importance by Category\\n\")\n",
    "        for idx, row in category_importance.iterrows():\n",
    "            f.write(f\"- {row['Category']}: {row['Importance']:.4f} ({row['Importance']*100:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\n### Model Hyperparameters\\n\")\n",
    "    for param, value in best_model.get_params().items():\n",
    "        f.write(f\"- {param}: {value}\\n\")\n",
    "\n",
    "print(\"\\nFinal model evaluation complete. Results saved to the 'model_summary.txt' file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task Effort Predictor Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function for new tasks\n",
    "def predict_task_effort(task_features, model_path=f'{results_dir}/best_tuned_model.pkl'):\n",
    "    \"\"\"\n",
    "    Predict resolution hours for a new task.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    task_features : dict\n",
    "        Dictionary of task features\n",
    "    model_path : str\n",
    "        Path to the trained model pickle file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the prediction and additional information\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Load the feature names used during training\n",
    "    with open(f'{results_dir}/data_splits.pkl', 'rb') as f:\n",
    "        splits = pickle.load(f)\n",
    "        feature_names = splits['feature_names']\n",
    "    \n",
    "    # Convert task features to DataFrame\n",
    "    task_df = pd.DataFrame([task_features])\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for feat in feature_names:\n",
    "        if feat not in task_df.columns:\n",
    "            task_df[feat] = 0  # Default value for missing features\n",
    "    \n",
    "    # Keep only the features used in training\n",
    "    task_df = task_df[feature_names]\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_log = model.predict(task_df)[0]\n",
    "    \n",
    "    # Convert from log space\n",
    "    pred_hours = np.expm1(pred_log)\n",
    "    \n",
    "    # Calculate prediction intervals (approximate)\n",
    "    lower_bound = pred_hours * 0.7  # 30% lower than prediction\n",
    "    upper_bound = pred_hours * 1.3  # 30% higher than prediction\n",
    "    \n",
    "    return {\n",
    "        'prediction_hours': pred_hours,\n",
    "        'prediction_days': pred_hours / 24,\n",
    "        'prediction_work_days': pred_hours / 8,  # Assuming 8-hour workdays\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample Task Effort Prediction:\")\n",
    "\n",
    "# Create example task features\n",
    "example_task = {\n",
    "    'is_type_bug': 1,\n",
    "    'is_priority_major': 1,\n",
    "    'inward_count': 2,\n",
    "    'outward_count': 1,\n",
    "    'age_days': 5,\n",
    "    'created_is_weekend': 0,\n",
    "    'created_hour': 14,\n",
    "    'is_completed': 0\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "prediction = predict_task_effort(example_task)\n",
    "print(f\"Predicted resolution time: {prediction['prediction_hours']:.2f} hours\")\n",
    "print(f\"Equivalent to: {prediction['prediction_days']:.2f} days or {prediction['prediction_work_days']:.2f} work days\")\n",
    "print(f\"Prediction interval: {prediction['lower_bound']:.2f} to {prediction['upper_bound']:.2f} hours\")\n",
    "\n",
    "# Save the predictor function as a separate Python module\n",
    "with open(f'{results_dir}/task_effort_predictor.py', 'w') as f:\n",
    "    f.write(\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def predict_task_effort(task_features, model_path='best_tuned_model.pkl'):\n",
    "    \"\"\"\n",
    "    Predict resolution hours for a new task.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    task_features : dict\n",
    "        Dictionary of task features\n",
    "    model_path : str\n",
    "        Path to the trained model pickle file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the prediction and additional information\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Load the feature names used during training\n",
    "    with open('data_splits.pkl', 'rb') as f:\n",
    "        splits = pickle.load(f)\n",
    "        feature_names = splits['feature_names']\n",
    "    \n",
    "    # Convert task features to DataFrame\n",
    "    task_df = pd.DataFrame([task_features])\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for feat in feature_names:\n",
    "        if feat not in task_df.columns:\n",
    "            task_df[feat] = 0  # Default value for missing features\n",
    "    \n",
    "    # Keep only the features used in training\n",
    "    task_df = task_df[feature_names]\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_log = model.predict(task_df)[0]\n",
    "    \n",
    "    # Convert from log space\n",
    "    pred_hours = np.expm1(pred_log)\n",
    "    \n",
    "    # Calculate prediction intervals (approximate)\n",
    "    lower_bound = pred_hours * 0.7  # 30% lower than prediction\n",
    "    upper_bound = pred_hours * 1.3  # 30% higher than prediction\n",
    "    \n",
    "    return {\n",
    "        'prediction_hours': pred_hours,\n",
    "        'prediction_days': pred_hours / 24,\n",
    "        'prediction_work_days': pred_hours / 8,  # Assuming 8-hour workdays\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound\n",
    "    }\n",
    "\n",
    "def get_task_effort_range(task_type, priority, complexity):\n",
    "    \"\"\"\n",
    "    Get predefined effort ranges based on task type, priority, and complexity.\n",
    "    This is a simplified example that could be expanded with more detailed logic.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    task_type : str\n",
    "        Type of task (e.g. 'bug', 'feature', 'improvement')\n",
    "    priority : str\n",
    "        Priority level (e.g. 'low', 'medium', 'high', 'critical')\n",
    "    complexity : str\n",
    "        Complexity level (e.g. 'low', 'medium', 'high')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with min, max, and typical effort in hours\n",
    "    \"\"\"\n",
    "    # Define base hours by task type\n",
    "    if task_type.lower() == 'bug':\n",
    "        base_hours = 8  # 1 day\n",
    "    elif task_type.lower() == 'feature':\n",
    "        base_hours = 24  # 3 days\n",
    "    elif task_type.lower() == 'improvement':\n",
    "        base_hours = 16  # 2 days\n",
    "    else:\n",
    "        base_hours = 12  # 1.5 days (default)\n",
    "    \n",
    "    # Apply priority multiplier\n",
    "    if priority.lower() == 'critical':\n",
    "        priority_mult = 0.8  # Critical items might be fixed faster\n",
    "    elif priority.lower() == 'high':\n",
    "        priority_mult = 0.9\n",
    "    elif priority.lower() == 'low':\n",
    "        priority_mult = 1.2\n",
    "    else:  # medium\n",
    "        priority_mult = 1.0\n",
    "    \n",
    "    # Apply complexity multiplier\n",
    "    if complexity.lower() == 'high':\n",
    "        complexity_mult = 2.0\n",
    "    elif complexity.lower() == 'low':\n",
    "        complexity_mult = 0.5\n",
    "    else:  # medium\n",
    "        complexity_mult = 1.0\n",
    "    \n",
    "    # Calculate typical hours\n",
    "    typical_hours = base_hours * priority_mult * complexity_mult\n",
    "    \n",
    "    # Define range\n",
    "    return {\n",
    "        'min_hours': typical_hours * 0.7,\n",
    "        'typical_hours': typical_hours,\n",
    "        'max_hours': typical_hours * 1.5,\n",
    "        'work_days': typical_hours / 8\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTask effort predictor module saved to 'task_effort_predictor.py'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key findings and insights\n",
    "print(\"\\nKey Findings and Insights:\")\n",
    "\n",
    "# Load the best model metrics\n",
    "with open(f'{results_dir}/model_summary.txt', 'r') as f:\n",
    "    model_summary = f.read()\n",
    "\n",
    "# Extract metrics from summary\n",
    "import re\n",
    "mae_match = re.search(r'MAE: ([0-9.]+)', model_summary)\n",
    "rmse_match = re.search(r'RMSE: ([0-9.]+)', model_summary)\n",
    "r2_match = re.search(r'R²: ([0-9.]+)', model_summary)\n",
    "spearman_match = re.search(r'Spearman Correlation: ([0-9.]+)', model_summary)\n",
    "\n",
    "mae = float(mae_match.group(1)) if mae_match else 0\n",
    "rmse = float(rmse_match.group(1)) if rmse_match else 0\n",
    "r2 = float(r2_match.group(1)) if r2_match else 0\n",
    "spearman = float(spearman_match.group(1)) if spearman_match else 0\n",
    "\n",
    "# Print model metrics\n",
    "print(f\"1. Model Performance:\")\n",
    "print(f\"   - Our best model achieves a Mean Absolute Error of {mae:.2f} hours\")\n",
    "print(f\"   - The Spearman correlation of {spearman:.4f} indicates a moderate to strong ability to rank tasks by effort\")\n",
    "print(f\"   - The model explains approximately {r2*100:.1f}% of the variance in task resolution time\")\n",
    "\n",
    "# Load category importance\n",
    "try:\n",
    "    category_importance = pd.read_csv(f'{results_dir}/category_importance.csv')\n",
    "    print(f\"\\n2. Most Important Feature Categories:\")\n",
    "    for idx, row in category_importance.head(3).iterrows():\n",
    "        print(f\"   - {row['Category']}: {row['Importance']*100:.1f}% importance\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Load feature importance\n",
    "try:\n",
    "    feature_importance = pd.read_csv(f'{results_dir}/best_tuned_model_feature_importance.csv')\n",
    "    print(f\"\\n3. Top 5 Individual Features:\")\n",
    "    for idx, row in feature_importance.head(5).iterrows():\n",
    "        print(f\"   - {row['Feature']}: {row['Importance']*100:.1f}% importance\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create final visualization summarizing actual vs. predicted\n",
    "print(\"\\nGenerating final visualization of model performance...\")\n",
    "\n",
    "# Get predictions from best model on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_test_orig = np.expm1(y_test)\n",
    "y_pred_orig = np.expm1(y_pred)\n",
    "\n",
    "# Create scatter plot with hexbin for density\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hexbin(y_test_orig, y_pred_orig, gridsize=50, cmap='viridis', bins='log')\n",
    "plt.plot([0, y_test_orig.max()], [0, y_test_orig.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Resolution Hours', fontsize=14)\n",
    "plt.ylabel('Predicted Resolution Hours', fontsize=14)\n",
    "plt.title('Task Effort Estimation: Actual vs. Predicted', fontsize=16)\n",
    "\n",
    "# Add annotation with model performance\n",
    "plt.annotate(\n",
    "    f\"MAE: {mae:.1f} hours\\nRMSE: {rmse:.1f} hours\\nR²: {r2:.3f}\\nSpearman: {spearman:.3f}\", \n",
    "    xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "    fontsize=12, verticalalignment='top'\n",
    ")\n",
    "\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Log Count (Density)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{results_dir}/actual_vs_predicted.png', dpi=300)\n",
    "plt.savefig('task_effort_prediction.png', dpi=300)  # Save a copy in the main directory\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nSummary visualization saved as 'task_effort_prediction.png'\")\n",
    "print(\"\\nTask effort estimation model complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}