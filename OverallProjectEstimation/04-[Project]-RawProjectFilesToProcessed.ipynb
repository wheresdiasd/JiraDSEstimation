{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./project_level_data/combined/combined_projects_raw.csv...\n",
      "Loaded 971 rows with 1506 columns\n",
      "Analyzing column patterns...\n",
      "Column analysis complete. Results saved to ./processed_data/column_inventory_analyzed.json\n",
      "Column analysis visualizations saved to ./processed_data/visualizations\n",
      "Standardizing column names...\n",
      "Found 22 duplicate column sets after standardization.\n",
      "  Combined duplicate column set: type_blog___new_blog_request_pct (2 variants)\n",
      "  Combined duplicate column set: type_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_new_tlp___common_tasks_pct (2 variants)\n",
      "  Combined duplicate column set: type_blogs___access_to_existing_blog_pct (2 variants)\n",
      "  Combined duplicate column set: type_backport_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_dev_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_simple_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_blog___new_blog_request_count (2 variants)\n",
      "  Combined duplicate column set: type_sub_requirement__count (2 variants)\n",
      "  Combined duplicate column set: type_blogs___access_to_existing_blog_count (2 variants)\n",
      "  Combined duplicate column set: type_blogs___new_blog_user_account_request_pct (2 variants)\n",
      "  Combined duplicate column set: type_simple_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_qe_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_dev_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_backport_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_qe_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_new_tlp___common_tasks_count (2 variants)\n",
      "  Combined duplicate column set: type_docs_sub_task_count (2 variants)\n",
      "  Combined duplicate column set: type_docs_sub_task_pct (2 variants)\n",
      "  Combined duplicate column set: type_blogs___new_blog_user_account_request_count (2 variants)\n",
      "  Combined duplicate column set: type_sub_requirement__pct (2 variants)\n",
      "Column standardization complete. Reduced from 1506 to 1462 columns.\n",
      "Standardized data saved to ./processed_data/standardized_data.csv\n",
      "Creating feature subsets...\n",
      "Core features dataset saved with 10 columns\n",
      "Temporal features dataset saved with 507 columns\n",
      "Priority features dataset saved with 80 columns\n",
      "Issue type features dataset saved with 190 columns\n",
      "Resolution and efficiency features dataset saved with 117 columns\n",
      "Common features dataset saved with 102 columns (present in â‰¥50% of repos)\n",
      "Filled dataset saved with all 1462 columns and no missing values\n",
      "Creating repository-specific datasets...\n",
      "Created dataset for MariaDB with 174 columns\n",
      "Created dataset for Hyperledger with 204 columns\n",
      "Created dataset for MongoDB with 355 columns\n",
      "Created dataset for Jira with 167 columns\n",
      "Created dataset for Mojang with 101 columns\n",
      "Created dataset for RedHat with 703 columns\n",
      "Created dataset for Apache with 674 columns\n",
      "Repository-specific datasets saved to ./processed_data/repositories\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class ProjectDataProcessor:\n",
    "    \"\"\"\n",
    "    A complete pipeline for processing and standardizing project data\n",
    "    from multiple repositories with different column naming patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_file, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the processor with input file and output directory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_file : str\n",
    "            Path to the combined raw CSV file\n",
    "        output_dir : str\n",
    "            Directory where processed outputs will be saved\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize data storage\n",
    "        self.raw_data = None\n",
    "        self.column_inventory = None\n",
    "        self.standardized_data = None\n",
    "        \n",
    "        # Column pattern categories\n",
    "        self.pattern_categories = {\n",
    "            'core': [],  # Core project metadata\n",
    "            'temporal': [],  # Time-related metrics\n",
    "            'priority': [],  # Priority-related columns\n",
    "            'issue_type': [],  # Issue type columns\n",
    "            'priority_type': [],  # Combined priority-type columns\n",
    "            'dependency': [],  # Dependency-related columns\n",
    "            'resolution': [],  # Resolution metrics\n",
    "            'efficiency': [],  # Efficiency metrics\n",
    "            'other': []  # Uncategorized columns\n",
    "        }\n",
    "        \n",
    "        # Column mappings from original to standardized names\n",
    "        self.column_mappings = {}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load the raw combined data\"\"\"\n",
    "        print(f\"Loading data from {self.input_file}...\")\n",
    "        self.raw_data = pd.read_csv(self.input_file)\n",
    "        print(f\"Loaded {len(self.raw_data)} rows with {len(self.raw_data.columns)} columns\")\n",
    "        return self\n",
    "    \n",
    "    def analyze_columns(self):\n",
    "        \"\"\"Analyze column patterns and distribution\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        \n",
    "        print(\"Analyzing column patterns...\")\n",
    "        \n",
    "        # Track column presence by repository\n",
    "        repo_columns = defaultdict(list)\n",
    "        column_repos = defaultdict(list)\n",
    "        \n",
    "        # Count missing values per column\n",
    "        missing_counts = self.raw_data.isna().sum()\n",
    "        missing_pct = (missing_counts / len(self.raw_data) * 100).round(1)\n",
    "        \n",
    "        # Get unique repositories\n",
    "        repositories = self.raw_data['repository'].unique()\n",
    "        \n",
    "        # Analyze which columns exist in which repositories\n",
    "        for repo in repositories:\n",
    "            repo_data = self.raw_data[self.raw_data['repository'] == repo]\n",
    "            \n",
    "            # Find columns that have at least one non-null value in this repo\n",
    "            non_null_cols = [col for col in self.raw_data.columns \n",
    "                            if not repo_data[col].isna().all()]\n",
    "            \n",
    "            repo_columns[repo] = non_null_cols\n",
    "            \n",
    "            # Update column_repos\n",
    "            for col in non_null_cols:\n",
    "                column_repos[col].append(repo)\n",
    "        \n",
    "        # Categorize columns by naming pattern\n",
    "        pattern_counts = defaultdict(int)\n",
    "        \n",
    "        for col in self.raw_data.columns:\n",
    "            # Identify core columns\n",
    "            if col in ['project_id', 'project_key', 'project_name', 'repository', 'source_file', \n",
    "                      'total_issues']:\n",
    "                self.pattern_categories['core'].append(col)\n",
    "            \n",
    "            # Identify temporal columns\n",
    "            elif any(term in col for term in ['date', 'duration', 'days', 'month', 'time', 'hours']):\n",
    "                self.pattern_categories['temporal'].append(col)\n",
    "                pattern_counts['temporal'] += 1\n",
    "            \n",
    "            # Identify priority columns\n",
    "            elif col.startswith('priority_') and not '_type_' in col:\n",
    "                self.pattern_categories['priority'].append(col)\n",
    "                pattern_counts['priority'] += 1\n",
    "            \n",
    "            # Identify issue type columns\n",
    "            elif col.startswith('type_') and not '_resolution_' in col:\n",
    "                self.pattern_categories['issue_type'].append(col)\n",
    "                pattern_counts['issue_type'] += 1\n",
    "            \n",
    "            # Identify combined priority-type columns\n",
    "            elif '_type_' in col and 'priority_' in col:\n",
    "                self.pattern_categories['priority_type'].append(col)\n",
    "                pattern_counts['priority_type'] += 1\n",
    "            \n",
    "            # Identify dependency columns\n",
    "            elif any(term in col for term in ['link', 'inward', 'outward', 'dependencies']):\n",
    "                self.pattern_categories['dependency'].append(col)\n",
    "                pattern_counts['dependency'] += 1\n",
    "            \n",
    "            # Identify resolution columns\n",
    "            elif any(term in col for term in ['resolution', 'resolved']):\n",
    "                self.pattern_categories['resolution'].append(col)\n",
    "                pattern_counts['resolution'] += 1\n",
    "            \n",
    "            # Identify efficiency columns\n",
    "            elif any(term in col for term in ['efficiency', 'ratio', 'balance', 'velocity']):\n",
    "                self.pattern_categories['efficiency'].append(col)\n",
    "                pattern_counts['efficiency'] += 1\n",
    "            \n",
    "            # Other columns\n",
    "            else:\n",
    "                self.pattern_categories['other'].append(col)\n",
    "                pattern_counts['other'] += 1\n",
    "        \n",
    "        # Identify pattern variants for standardization\n",
    "        self._identify_column_variants()\n",
    "        \n",
    "        # Create column inventory\n",
    "        self.column_inventory = {\n",
    "            'total_columns': len(self.raw_data.columns),\n",
    "            'repositories': {\n",
    "                'count': len(repositories),\n",
    "                'names': list(repositories)\n",
    "            },\n",
    "            'columns_by_repository': {repo: cols for repo, cols in repo_columns.items()},\n",
    "            'repositories_by_column': {col: repos for col, repos in column_repos.items()},\n",
    "            'missing_percentages': {col: pct for col, pct in missing_pct.items()},\n",
    "            'pattern_categories': {cat: len(cols) for cat, cols in self.pattern_categories.items()},\n",
    "            'column_variants': self.column_mappings\n",
    "        }\n",
    "        \n",
    "        # Save inventory to file\n",
    "        inventory_file = os.path.join(self.output_dir, 'column_inventory_analyzed.json')\n",
    "        with open(inventory_file, 'w') as f:\n",
    "            # Convert sets to lists for JSON serialization\n",
    "            inventory_json = {k: v if not isinstance(v, set) else list(v) \n",
    "                             for k, v in self.column_inventory.items()}\n",
    "            \n",
    "            # Handle nested dictionaries with sets\n",
    "            for k, v in inventory_json.items():\n",
    "                if isinstance(v, dict):\n",
    "                    inventory_json[k] = {k2: v2 if not isinstance(v2, set) else list(v2) \n",
    "                                        for k2, v2 in v.items()}\n",
    "            \n",
    "            json.dump(inventory_json, f, indent=2)\n",
    "        \n",
    "        print(f\"Column analysis complete. Results saved to {inventory_file}\")\n",
    "        \n",
    "        # Create visualizations for column analysis\n",
    "        self._create_column_analysis_visualizations()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _identify_column_variants(self):\n",
    "        \"\"\"Identify column naming variants to create standardization mappings\"\"\"\n",
    "        # Group similar columns\n",
    "        # 1. Priority columns\n",
    "        priority_pattern = r'priority_([\\w-]+)(?:_-_p\\d+|___p\\d+)?_(count|pct)'\n",
    "        priority_columns = [col for col in self.raw_data.columns if re.match(priority_pattern, col)]\n",
    "        \n",
    "        for col in priority_columns:\n",
    "            match = re.match(priority_pattern, col)\n",
    "            if match:\n",
    "                priority_level = match.group(1)\n",
    "                metric_type = match.group(2)\n",
    "                standard_name = f\"priority_{priority_level}_{metric_type}\"\n",
    "                self.column_mappings[col] = standard_name\n",
    "        \n",
    "        # 2. Issue type columns\n",
    "        type_pattern = r'type_([\\w-]+)_(count|pct)'\n",
    "        type_columns = [col for col in self.raw_data.columns if re.match(type_pattern, col)]\n",
    "        \n",
    "        for col in type_columns:\n",
    "            match = re.match(type_pattern, col)\n",
    "            if match:\n",
    "                issue_type = match.group(1).replace('-', '_')  # Standardize - vs _\n",
    "                metric_type = match.group(2)\n",
    "                standard_name = f\"type_{issue_type}_{metric_type}\"\n",
    "                self.column_mappings[col] = standard_name\n",
    "        \n",
    "        # 3. Priority-type combined columns\n",
    "        priority_type_pattern = r'priority_([\\w-]+)(?:_-_p\\d+|___p\\d+)?_type_([\\w-]+)_(count|avg_resolution_hours)'\n",
    "        priority_type_columns = [col for col in self.raw_data.columns if re.match(priority_type_pattern, col)]\n",
    "        \n",
    "        for col in priority_type_columns:\n",
    "            match = re.match(priority_type_pattern, col)\n",
    "            if match:\n",
    "                priority_level = match.group(1)\n",
    "                issue_type = match.group(2).replace('-', '_')\n",
    "                metric_type = match.group(3)\n",
    "                standard_name = f\"priority_{priority_level}_type_{issue_type}_{metric_type}\"\n",
    "                self.column_mappings[col] = standard_name\n",
    "        \n",
    "        # 4. Resolution rate columns\n",
    "        resolution_pattern = r'type_([\\w-]+)_resolution_rate'\n",
    "        resolution_columns = [col for col in self.raw_data.columns if re.match(resolution_pattern, col)]\n",
    "        \n",
    "        for col in resolution_columns:\n",
    "            match = re.match(resolution_pattern, col)\n",
    "            if match:\n",
    "                issue_type = match.group(1).replace('-', '_')\n",
    "                standard_name = f\"type_{issue_type}_resolution_rate\"\n",
    "                self.column_mappings[col] = standard_name\n",
    "    \n",
    "    def _create_column_analysis_visualizations(self):\n",
    "        \"\"\"Create visualizations of column analysis\"\"\"\n",
    "        # Create a visualizations directory\n",
    "        viz_dir = os.path.join(self.output_dir, 'visualizations')\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Column category distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        categories = list(self.pattern_categories.keys())\n",
    "        category_counts = [len(self.pattern_categories[cat]) for cat in categories]\n",
    "        \n",
    "        sns.barplot(x=categories, y=category_counts)\n",
    "        plt.title('Column Distribution by Category')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'column_categories.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Repository column counts\n",
    "        repo_column_counts = {repo: len(cols) for repo, cols in \n",
    "                             self.column_inventory['columns_by_repository'].items()}\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        repos = list(repo_column_counts.keys())\n",
    "        counts = list(repo_column_counts.values())\n",
    "        \n",
    "        # Sort by count\n",
    "        sorted_indices = np.argsort(counts)[::-1]\n",
    "        sorted_repos = [repos[i] for i in sorted_indices]\n",
    "        sorted_counts = [counts[i] for i in sorted_indices]\n",
    "        \n",
    "        # Take top 20 for readability\n",
    "        display_repos = sorted_repos[:20]\n",
    "        display_counts = sorted_counts[:20]\n",
    "        \n",
    "        sns.barplot(x=display_repos, y=display_counts)\n",
    "        plt.title('Number of Non-Null Columns by Repository (Top 20)')\n",
    "        plt.xlabel('Repository')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'repository_columns.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Missing value distribution\n",
    "        missing_pct = pd.Series(self.column_inventory['missing_percentages'])\n",
    "        missing_pct = missing_pct.sort_values(ascending=False)\n",
    "        \n",
    "        # Plot histogram of missing percentages\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(missing_pct.values, bins=20)\n",
    "        plt.title('Distribution of Missing Value Percentages')\n",
    "        plt.xlabel('Percentage of Missing Values')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'missing_values_histogram.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 4. Top missing columns\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        top_missing = missing_pct.head(20)\n",
    "        \n",
    "        sns.barplot(x=top_missing.index, y=top_missing.values)\n",
    "        plt.title('Top 20 Columns with Highest Missing Percentages')\n",
    "        plt.xlabel('Column')\n",
    "        plt.ylabel('Missing Percentage')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(viz_dir, 'top_missing_columns.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Column analysis visualizations saved to {viz_dir}\")\n",
    "    \n",
    "    def standardize_columns(self):\n",
    "        \"\"\"Standardize column names based on identified patterns\"\"\"\n",
    "        if self.raw_data is None or self.column_inventory is None:\n",
    "            raise ValueError(\"Data not loaded or columns not analyzed. Call load_data() and analyze_columns() first.\")\n",
    "        \n",
    "        print(\"Standardizing column names...\")\n",
    "        \n",
    "        # Create a copy of the raw data\n",
    "        standardized_df = self.raw_data.copy()\n",
    "        \n",
    "        # Apply column mappings\n",
    "        rename_map = {col: new_name for col, new_name in self.column_mappings.items() \n",
    "                     if col in standardized_df.columns}\n",
    "        \n",
    "        # Perform the renaming\n",
    "        standardized_df = standardized_df.rename(columns=rename_map)\n",
    "        \n",
    "        # After renaming, some columns may be duplicated - combine them\n",
    "        # Group by the new column names\n",
    "        new_columns = standardized_df.columns\n",
    "        duplicate_cols = [col for col in set(new_columns) if list(new_columns).count(col) > 1]\n",
    "        \n",
    "        if duplicate_cols:\n",
    "            print(f\"Found {len(duplicate_cols)} duplicate column sets after standardization.\")\n",
    "            \n",
    "            # For each duplicate column set, combine the values\n",
    "            for col in duplicate_cols:\n",
    "                # Get all original columns that map to this standardized name\n",
    "                original_cols = [c for c in standardized_df.columns if c == col]\n",
    "                \n",
    "                if len(original_cols) <= 1:\n",
    "                    continue\n",
    "                \n",
    "                # Keep the first one and drop the rest\n",
    "                keep_col = original_cols[0]\n",
    "                drop_cols = original_cols[1:]\n",
    "                \n",
    "                # For each row, if the keep_col is NaN, try to fill it from the other columns\n",
    "                for drop_col in drop_cols:\n",
    "                    # Fill NaN values in keep_col with values from drop_col\n",
    "                    standardized_df[keep_col] = standardized_df[keep_col].fillna(standardized_df[drop_col])\n",
    "                \n",
    "                # Drop the duplicate columns\n",
    "                standardized_df = standardized_df.drop(columns=drop_cols)\n",
    "                \n",
    "                print(f\"  Combined duplicate column set: {col} ({len(original_cols)} variants)\")\n",
    "        \n",
    "        # Store the standardized data\n",
    "        self.standardized_data = standardized_df\n",
    "        \n",
    "        # Save standardized data\n",
    "        standardized_file = os.path.join(self.output_dir, 'standardized_data.csv')\n",
    "        standardized_df.to_csv(standardized_file, index=False)\n",
    "        \n",
    "        print(f\"Column standardization complete. Reduced from {len(self.raw_data.columns)} to {len(standardized_df.columns)} columns.\")\n",
    "        print(f\"Standardized data saved to {standardized_file}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_feature_subsets(self):\n",
    "        \"\"\"Create various feature subsets for different analysis purposes\"\"\"\n",
    "        if self.standardized_data is None:\n",
    "            raise ValueError(\"Data not standardized. Call standardize_columns() first.\")\n",
    "        \n",
    "        print(\"Creating feature subsets...\")\n",
    "        \n",
    "        # Get standardized data\n",
    "        df = self.standardized_data\n",
    "        \n",
    "        # 1. Core features dataset\n",
    "        core_features = ['project_id', 'project_key', 'project_name', 'repository', 'total_issues',\n",
    "                         'project_start_date', 'project_duration_days', 'avg_resolution_hours', \n",
    "                         'num_resolved_issues', 'pct_resolved_issues']\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        core_features = [col for col in core_features if col in df.columns]\n",
    "        \n",
    "        core_df = df[core_features]\n",
    "        core_file = os.path.join(self.output_dir, 'core_features.csv')\n",
    "        core_df.to_csv(core_file, index=False)\n",
    "        print(f\"Core features dataset saved with {len(core_features)} columns\")\n",
    "        \n",
    "        # 2. Temporal features dataset\n",
    "        temporal_features = core_features + [col for col in self.pattern_categories['temporal'] \n",
    "                                            if col in df.columns]\n",
    "        \n",
    "        temporal_df = df[temporal_features]\n",
    "        temporal_file = os.path.join(self.output_dir, 'temporal_features.csv')\n",
    "        temporal_df.to_csv(temporal_file, index=False)\n",
    "        print(f\"Temporal features dataset saved with {len(temporal_features)} columns\")\n",
    "        \n",
    "        # 3. Priority features dataset\n",
    "        priority_features = core_features + [col for col in self.pattern_categories['priority'] \n",
    "                                           if col in df.columns]\n",
    "        \n",
    "        priority_df = df[priority_features]\n",
    "        priority_file = os.path.join(self.output_dir, 'priority_features.csv')\n",
    "        priority_df.to_csv(priority_file, index=False)\n",
    "        print(f\"Priority features dataset saved with {len(priority_features)} columns\")\n",
    "        \n",
    "        # 4. Issue type features dataset\n",
    "        type_features = core_features + [col for col in self.pattern_categories['issue_type'] \n",
    "                                       if col in df.columns]\n",
    "        \n",
    "        type_df = df[type_features]\n",
    "        type_file = os.path.join(self.output_dir, 'issue_type_features.csv')\n",
    "        type_df.to_csv(type_file, index=False)\n",
    "        print(f\"Issue type features dataset saved with {len(type_features)} columns\")\n",
    "        \n",
    "        # 5. Resolution and efficiency features dataset\n",
    "        resolution_features = core_features + [col for col in self.pattern_categories['resolution'] + \n",
    "                                             self.pattern_categories['efficiency'] if col in df.columns]\n",
    "        \n",
    "        resolution_df = df[resolution_features]\n",
    "        resolution_file = os.path.join(self.output_dir, 'resolution_efficiency_features.csv')\n",
    "        resolution_df.to_csv(resolution_file, index=False)\n",
    "        print(f\"Resolution and efficiency features dataset saved with {len(resolution_features)} columns\")\n",
    "        \n",
    "        # 6. Common features dataset (columns present in at least 50% of repositories)\n",
    "        repo_count = len(df['repository'].unique())\n",
    "        threshold = repo_count * 0.35\n",
    "        \n",
    "        common_cols = [col for col in df.columns \n",
    "                      if len(self.column_inventory['repositories_by_column'].get(col, [])) >= threshold]\n",
    "        \n",
    "        common_df = df[common_cols]\n",
    "        common_file = os.path.join(self.output_dir, 'common_features.csv')\n",
    "        common_df.to_csv(common_file, index=False)\n",
    "        print(f\"Common features dataset saved with {len(common_cols)} columns (present in â‰¥50% of repos)\")\n",
    "        \n",
    "        # 7. Complete dataset with missing values filled\n",
    "        filled_df = df.copy()\n",
    "        \n",
    "        # Fill numeric columns with 0\n",
    "        num_cols = filled_df.select_dtypes(include=['number']).columns\n",
    "        filled_df[num_cols] = filled_df[num_cols].fillna(0)\n",
    "        \n",
    "        # Fill string/object columns with 'Unknown'\n",
    "        obj_cols = filled_df.select_dtypes(include=['object']).columns\n",
    "        filled_df[obj_cols] = filled_df[obj_cols].fillna('Unknown')\n",
    "        \n",
    "        # Fill datetime columns with the earliest date in the dataset\n",
    "        date_cols = [col for col in filled_df.columns if 'date' in col.lower()]\n",
    "        for col in date_cols:\n",
    "            if col in filled_df.columns:\n",
    "                try:\n",
    "                    filled_df[col] = pd.to_datetime(filled_df[col])\n",
    "                    min_date = filled_df[col].min()\n",
    "                    filled_df[col] = filled_df[col].fillna(min_date)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        filled_file = os.path.join(self.output_dir, 'filled_data.csv')\n",
    "        filled_df.to_csv(filled_file, index=False)\n",
    "        print(f\"Filled dataset saved with all {len(filled_df.columns)} columns and no missing values\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def create_repository_specific_datasets(self):\n",
    "        \"\"\"Create separate datasets for each repository\"\"\"\n",
    "        if self.standardized_data is None:\n",
    "            raise ValueError(\"Data not standardized. Call standardize_columns() first.\")\n",
    "        \n",
    "        print(\"Creating repository-specific datasets...\")\n",
    "        \n",
    "        # Get standardized data\n",
    "        df = self.standardized_data\n",
    "        \n",
    "        # Create directory for repo-specific datasets\n",
    "        repos_dir = os.path.join(self.output_dir, 'repositories')\n",
    "        os.makedirs(repos_dir, exist_ok=True)\n",
    "        \n",
    "        # Get unique repositories\n",
    "        repositories = df['repository'].unique()\n",
    "        \n",
    "        # Create dataset for each repository\n",
    "        for repo in repositories:\n",
    "            # Get data for this repository\n",
    "            repo_data = df[df['repository'] == repo]\n",
    "            \n",
    "            # Drop columns that are all NaN\n",
    "            repo_data = repo_data.dropna(axis=1, how='all')\n",
    "            \n",
    "            # Create file name (replace invalid characters)\n",
    "            safe_repo_name = re.sub(r'[\\\\/*?:\"<>|]', \"_\", repo)\n",
    "            repo_file = os.path.join(repos_dir, f\"{safe_repo_name}.csv\")\n",
    "            \n",
    "            # Save to file\n",
    "            repo_data.to_csv(repo_file, index=False)\n",
    "            \n",
    "            print(f\"Created dataset for {repo} with {len(repo_data.columns)} columns\")\n",
    "        \n",
    "        print(f\"Repository-specific datasets saved to {repos_dir}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete data processing pipeline\"\"\"\n",
    "        return (self.load_data()\n",
    "                .analyze_columns()\n",
    "                .standardize_columns()\n",
    "                .create_feature_subsets()\n",
    "                .create_repository_specific_datasets())\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    INPUT_FILE = \"./project_level_data/combined/combined_projects_raw.csv\"\n",
    "    OUTPUT_DIR = \"./processed_data\"\n",
    "    \n",
    "    # Run the pipeline\n",
    "    processor = ProjectDataProcessor(INPUT_FILE, OUTPUT_DIR)\n",
    "    processor.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
