{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "results_dir = 'prepared_processed_data_2'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "\n",
    "# 2. Basic exploration\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "# 3. Define time-related keywords to filter out columns\n",
    "time_related_keywords = [\n",
    "    'time', 'hour', 'date', 'day', 'week', 'month', 'duration', \n",
    "    'resolved', 'skewness', 'kurtosis', 'p25', 'p75', 'p90', 'iqr',\n",
    "    'volatility', 'velocity', 'rate', 'balance', 'weekend', 'stability',\n",
    "    'resolution', 'activity', 'experience', 'predictability'\n",
    "]\n",
    "\n",
    "# Filter out time-related columns\n",
    "time_columns = [col for col in df.columns if any(keyword in col.lower() for keyword in time_related_keywords)]\n",
    "non_time_columns = [col for col in df.columns if not any(keyword in col.lower() for keyword in time_related_keywords)]\n",
    "\n",
    "print(f\"\\nRemoved {len(time_columns)} time-related columns\")\n",
    "print(f\"Remaining {len(non_time_columns)} non-time-related columns\")\n",
    "\n",
    "# Create a filtered dataframe without time-related columns\n",
    "df_filtered = df[non_time_columns]\n",
    "\n",
    "# 4. Check data types and identify non-numeric columns\n",
    "print(\"\\n=== DATA TYPE ANALYSIS ===\")\n",
    "print(df_filtered.dtypes.value_counts())\n",
    "\n",
    "non_numeric_columns = df_filtered.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nNon-numeric columns ({len(non_numeric_columns)}):\")\n",
    "for col in non_numeric_columns:\n",
    "    unique_values = df_filtered[col].nunique()\n",
    "    print(f\"  - {col}: {df_filtered[col].dtype}, {unique_values} unique values\")\n",
    "    if unique_values < 10:  # Show examples if not too many\n",
    "        print(f\"    Values: {df_filtered[col].unique()}\")\n",
    "\n",
    "# 5. Handle non-numeric columns\n",
    "print(\"\\n=== HANDLING NON-NUMERIC COLUMNS ===\")\n",
    "# Identify columns to drop (identifiers) and columns to encode (categorical)\n",
    "cols_to_drop = []\n",
    "cols_to_encode = []\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    # Check if it's an identifier column\n",
    "    if any(keyword in col.lower() for keyword in ['project', 'key', 'name', 'id', 'source', 'repository', 'file']):\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        # Must be a categorical column\n",
    "        cols_to_encode.append(col)\n",
    "\n",
    "# Drop identifier columns\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping identifier columns: {cols_to_drop}\")\n",
    "    df_filtered = df_filtered.drop(columns=cols_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "if cols_to_encode:\n",
    "    print(f\"Encoding categorical columns: {cols_to_encode}\")\n",
    "    # Use pandas get_dummies for one-hot encoding\n",
    "    df_filtered = pd.get_dummies(df_filtered, columns=cols_to_encode, drop_first=True)\n",
    "    print(f\"Expanded to {df_filtered.shape[1]} columns after encoding\")\n",
    "\n",
    "# 6. Function for interquartile-based imputation and outlier capping\n",
    "def impute_and_cap_using_iqr(df, columns=None, fill_missing=True, cap_outliers=True, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Impute missing values and cap outliers using interquartile range method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    columns : list or None\n",
    "        Columns to process. If None, will process all numeric columns.\n",
    "    fill_missing : bool\n",
    "        Whether to fill missing values with median\n",
    "    cap_outliers : bool\n",
    "        Whether to cap outliers based on IQR\n",
    "    iqr_multiplier : float\n",
    "        Multiplier for IQR to define outlier boundaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate quartiles and IQR\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define lower and upper bounds\n",
    "        lower_bound = q1 - iqr_multiplier * iqr\n",
    "        upper_bound = q3 + iqr_multiplier * iqr\n",
    "        \n",
    "        # Fill missing values with median if requested\n",
    "        if fill_missing and df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"Filled {df[col].isna().sum()} missing values in {col} with median: {median_val:.4f}\")\n",
    "        \n",
    "        # Cap outliers if requested\n",
    "        if cap_outliers:\n",
    "            # Count outliers before capping\n",
    "            n_lower_outliers = (df_clean[col] < lower_bound).sum()\n",
    "            n_upper_outliers = (df_clean[col] > upper_bound).sum()\n",
    "            \n",
    "            if n_lower_outliers > 0 or n_upper_outliers > 0:\n",
    "                # Apply capping\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"Capped {n_lower_outliers} lower and {n_upper_outliers} upper outliers in {col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 7. Categorize features based on their type\n",
    "print(\"\\n=== CATEGORIZING FEATURES ===\")\n",
    "\n",
    "# Percentage features\n",
    "pct_features = [col for col in df_filtered.columns if 'pct' in col.lower()]\n",
    "\n",
    "# Count features\n",
    "count_features = [col for col in df_filtered.columns if 'count' in col.lower()]\n",
    "\n",
    "# General feature categories\n",
    "general_features = [\n",
    "    col for col in df_filtered.columns \n",
    "    if col not in pct_features and col not in count_features and '_' in col\n",
    "]\n",
    "\n",
    "# Link-related features\n",
    "link_features = [col for col in df_filtered.columns if 'link' in col.lower()]\n",
    "\n",
    "# Priority features\n",
    "priority_features = [col for col in df_filtered.columns if 'priority' in col.lower() and 'count' not in col.lower() and 'pct' not in col.lower()]\n",
    "\n",
    "# Type features\n",
    "type_features = [col for col in df_filtered.columns if 'type' in col.lower() and 'count' not in col.lower() and 'pct' not in col.lower()]\n",
    "\n",
    "# Team features\n",
    "team_features = [col for col in df_filtered.columns if 'team' in col.lower() or 'creator' in col.lower() or 'developer' in col.lower()]\n",
    "\n",
    "# Combine all feature categories\n",
    "feature_categories = {\n",
    "    'pct_features': pct_features,\n",
    "    'count_features': count_features,\n",
    "    'general_features': general_features,\n",
    "    'link_features': link_features,\n",
    "    'priority_features': priority_features,\n",
    "    'type_features': type_features,\n",
    "    'team_features': team_features\n",
    "}\n",
    "\n",
    "# Print feature categories\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category}: {len(features)} features\")\n",
    "    if len(features) > 0:\n",
    "        print(f\"  Sample: {', '.join(features[:3])}{'...' if len(features) > 3 else ''}\")\n",
    "\n",
    "# 8. Apply interquartile-based imputation to all numeric features\n",
    "print(\"\\n=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\")\n",
    "numeric_columns = df_filtered.select_dtypes(include=np.number).columns.tolist()\n",
    "df_clean = impute_and_cap_using_iqr(df_filtered, columns=numeric_columns, fill_missing=True, cap_outliers=True)\n",
    "\n",
    "# 9. Create the column transformer for feature scaling\n",
    "print(\"\\n=== CREATING FEATURE PREPROCESSING PIPELINE ===\")\n",
    "\n",
    "# Get the actual feature lists based on what's available in the dataframe\n",
    "pct_feats = feature_categories['pct_features']\n",
    "count_feats = feature_categories['count_features']\n",
    "general_feats = feature_categories['general_features']\n",
    "link_feats = feature_categories['link_features']\n",
    "priority_feats = feature_categories['priority_features']\n",
    "type_feats = feature_categories['type_features']\n",
    "team_feats = feature_categories['team_features']\n",
    "\n",
    "# Check if any feature categories are empty\n",
    "for category, features in {'pct': pct_feats, 'count': count_feats, \n",
    "                          'general': general_feats, 'link': link_feats,\n",
    "                          'priority': priority_feats, 'type': type_feats,\n",
    "                          'team': team_feats}.items():\n",
    "    if not features:\n",
    "        print(f\"Warning: No {category} features available for scaling\")\n",
    "\n",
    "# Create transformers only for non-empty feature lists\n",
    "transformers = []\n",
    "if pct_feats:\n",
    "    transformers.append(('pct_minmax', MinMaxScaler(), pct_feats))\n",
    "if count_feats:\n",
    "    transformers.append(('count_std', StandardScaler(), count_feats))\n",
    "if general_feats:\n",
    "    transformers.append(('general_std', StandardScaler(), general_feats))\n",
    "if link_feats:\n",
    "    transformers.append(('link_std', StandardScaler(), link_feats))\n",
    "if priority_feats:\n",
    "    transformers.append(('priority_std', StandardScaler(), priority_feats))\n",
    "if type_feats:\n",
    "    transformers.append(('type_std', StandardScaler(), type_feats))\n",
    "if team_feats:\n",
    "    transformers.append(('team_robust', RobustScaler(), team_feats))\n",
    "\n",
    "feature_preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough'  # Any columns not specified will pass through\n",
    ")\n",
    "\n",
    "# 10. Apply the preprocessing to features\n",
    "print(\"Scaling features...\")\n",
    "# Check if df_clean has any data\n",
    "if df_clean.empty:\n",
    "    print(\"Error: No features available for scaling!\")\n",
    "else:\n",
    "    scaled_features = feature_preprocessor.fit_transform(df_clean)\n",
    "\n",
    "    # 11. Create DataFrame with scaled features\n",
    "    feature_names = feature_preprocessor.get_feature_names_out()\n",
    "    df_scaled_features = pd.DataFrame(scaled_features, columns=feature_names, index=df_clean.index)\n",
    "\n",
    "    # 12. Check if we've introduced any NaN values during scaling\n",
    "    nan_count = df_scaled_features.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: {nan_count} NaN values introduced during scaling\")\n",
    "        # Replace NaNs with 0\n",
    "        df_scaled_features = df_scaled_features.fillna(0)\n",
    "        print(\"NaN values have been replaced with 0\")\n",
    "\n",
    "    # 13. Save the results\n",
    "    print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "    # Save the clean but unscaled dataset (after IQR imputation and outlier capping)\n",
    "    df_clean.to_csv(f'{results_dir}/common_features_iqr_cleaned_no_time.csv', index=False)\n",
    "    print(\"1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned_no_time.csv\")\n",
    "\n",
    "    # Save the preprocessor for later use\n",
    "    with open(f'{results_dir}/jira_feature_preprocessor_no_time.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_preprocessor, f)\n",
    "    print(\"2. Saved feature preprocessor: jira_feature_preprocessor_no_time.pkl\")\n",
    "\n",
    "    # Save the dataset with scaled features\n",
    "    df_scaled_features.to_csv(f'{results_dir}/common_features_scaled_no_time.csv', index=False)\n",
    "    print(\"3. Saved scaled features: common_features_scaled_no_time.csv\")\n",
    "\n",
    "    # 14. Print some summary statistics\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Filtered dataset (no time metrics): {df_filtered.shape}\")\n",
    "    print(f\"Final processed dataset shape: {df_scaled_features.shape}\")\n",
    "    print(f\"Removed {len(time_columns)} time-related columns\")\n",
    "\n",
    "print(\"\\n=== PROCESSING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "results_dir = 'prepared_processed_data_2'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define target variables that should be preserved\n",
    "target_variables = [\n",
    "    'avg_resolution_hours', \n",
    "    'median_resolution_hours',\n",
    "    'total_resolution_hours'\n",
    "]\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./processed_data/common_features.csv')\n",
    "\n",
    "# 2. Basic exploration\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "# 3. Define time-related keywords to filter out columns\n",
    "time_related_keywords = [\n",
    "    'time', 'hour', 'date', 'day', 'week', 'month', 'duration', \n",
    "    'resolved', 'skewness', 'kurtosis', 'p25', 'p75', 'p90', 'iqr',\n",
    "    'volatility', 'velocity', 'rate', 'balance', 'weekend', 'stability',\n",
    "    'resolution', 'activity', 'experience', 'predictability'\n",
    "]\n",
    "\n",
    "# Filter out time-related columns but preserve target variables\n",
    "time_columns = [\n",
    "    col for col in df.columns \n",
    "    if any(keyword in col.lower() for keyword in time_related_keywords) \n",
    "    and col not in target_variables\n",
    "]\n",
    "\n",
    "non_time_columns = [col for col in df.columns if col not in time_columns]\n",
    "\n",
    "print(f\"\\nRemoved {len(time_columns)} time-related columns (preserving target variables)\")\n",
    "print(f\"Remaining {len(non_time_columns)} columns including target variables\")\n",
    "print(f\"Target variables preserved: {target_variables}\")\n",
    "\n",
    "# Create a filtered dataframe without time-related columns except target variables\n",
    "df_filtered = df[non_time_columns]\n",
    "\n",
    "# 4. Check data types and identify non-numeric columns\n",
    "print(\"\\n=== DATA TYPE ANALYSIS ===\")\n",
    "print(df_filtered.dtypes.value_counts())\n",
    "\n",
    "non_numeric_columns = df_filtered.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(f\"\\nNon-numeric columns ({len(non_numeric_columns)}):\")\n",
    "for col in non_numeric_columns:\n",
    "    unique_values = df_filtered[col].nunique()\n",
    "    print(f\"  - {col}: {df_filtered[col].dtype}, {unique_values} unique values\")\n",
    "    if unique_values < 10:  # Show examples if not too many\n",
    "        print(f\"    Values: {df_filtered[col].unique()}\")\n",
    "\n",
    "# 5. Handle non-numeric columns\n",
    "print(\"\\n=== HANDLING NON-NUMERIC COLUMNS ===\")\n",
    "# Identify columns to drop (identifiers) and columns to encode (categorical)\n",
    "cols_to_drop = []\n",
    "cols_to_encode = []\n",
    "\n",
    "for col in non_numeric_columns:\n",
    "    # Check if it's an identifier column\n",
    "    if any(keyword in col.lower() for keyword in ['project', 'key', 'name', 'id', 'source', 'repository', 'file']):\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        # Must be a categorical column\n",
    "        cols_to_encode.append(col)\n",
    "\n",
    "# Drop identifier columns\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping identifier columns: {cols_to_drop}\")\n",
    "    df_filtered = df_filtered.drop(columns=cols_to_drop)\n",
    "\n",
    "# Encode categorical variables\n",
    "if cols_to_encode:\n",
    "    print(f\"Encoding categorical columns: {cols_to_encode}\")\n",
    "    # Use pandas get_dummies for one-hot encoding\n",
    "    df_filtered = pd.get_dummies(df_filtered, columns=cols_to_encode, drop_first=True)\n",
    "    print(f\"Expanded to {df_filtered.shape[1]} columns after encoding\")\n",
    "\n",
    "# 6. Function for interquartile-based imputation and outlier capping\n",
    "def impute_and_cap_using_iqr(df, columns=None, fill_missing=True, cap_outliers=True, iqr_multiplier=1.5):\n",
    "    \"\"\"\n",
    "    Impute missing values and cap outliers using interquartile range method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe\n",
    "    columns : list or None\n",
    "        Columns to process. If None, will process all numeric columns.\n",
    "    fill_missing : bool\n",
    "        Whether to fill missing values with median\n",
    "    cap_outliers : bool\n",
    "        Whether to cap outliers based on IQR\n",
    "    iqr_multiplier : float\n",
    "        Multiplier for IQR to define outlier boundaries\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Processed dataframe\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include=np.number).columns\n",
    "    \n",
    "    for col in columns:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate quartiles and IQR\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define lower and upper bounds\n",
    "        lower_bound = q1 - iqr_multiplier * iqr\n",
    "        upper_bound = q3 + iqr_multiplier * iqr\n",
    "        \n",
    "        # Fill missing values with median if requested\n",
    "        if fill_missing and df[col].isna().sum() > 0:\n",
    "            median_val = df[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "            print(f\"Filled {df[col].isna().sum()} missing values in {col} with median: {median_val:.4f}\")\n",
    "        \n",
    "        # Cap outliers if requested\n",
    "        if cap_outliers:\n",
    "            # Count outliers before capping\n",
    "            n_lower_outliers = (df_clean[col] < lower_bound).sum()\n",
    "            n_upper_outliers = (df_clean[col] > upper_bound).sum()\n",
    "            \n",
    "            if n_lower_outliers > 0 or n_upper_outliers > 0:\n",
    "                # Apply capping\n",
    "                df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                print(f\"Capped {n_lower_outliers} lower and {n_upper_outliers} upper outliers in {col}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# 7. Categorize features based on their type\n",
    "print(\"\\n=== CATEGORIZING FEATURES ===\")\n",
    "\n",
    "# Separate target variables for special handling\n",
    "target_vars = [col for col in df_filtered.columns if col in target_variables]\n",
    "\n",
    "# Percentage features\n",
    "pct_features = [col for col in df_filtered.columns if 'pct' in col.lower() and col not in target_vars]\n",
    "\n",
    "# Count features\n",
    "count_features = [col for col in df_filtered.columns if 'count' in col.lower() and col not in target_vars]\n",
    "\n",
    "# General feature categories\n",
    "general_features = [\n",
    "    col for col in df_filtered.columns \n",
    "    if col not in pct_features and col not in count_features and '_' in col and col not in target_vars\n",
    "]\n",
    "\n",
    "# Link-related features\n",
    "link_features = [col for col in df_filtered.columns if 'link' in col.lower() and col not in target_vars]\n",
    "\n",
    "# Priority features\n",
    "priority_features = [col for col in df_filtered.columns if 'priority' in col.lower() and 'count' not in col.lower() and 'pct' not in col.lower() and col not in target_vars]\n",
    "\n",
    "# Type features\n",
    "type_features = [col for col in df_filtered.columns if 'type' in col.lower() and 'count' not in col.lower() and 'pct' not in col.lower() and col not in target_vars]\n",
    "\n",
    "# Team features\n",
    "team_features = [col for col in df_filtered.columns if ('team' in col.lower() or 'creator' in col.lower() or 'developer' in col.lower()) and col not in target_vars]\n",
    "\n",
    "# Combine all feature categories\n",
    "feature_categories = {\n",
    "    'target_variables': target_vars,\n",
    "    'pct_features': pct_features,\n",
    "    'count_features': count_features,\n",
    "    'general_features': general_features,\n",
    "    'link_features': link_features,\n",
    "    'priority_features': priority_features,\n",
    "    'type_features': type_features,\n",
    "    'team_features': team_features\n",
    "}\n",
    "\n",
    "# Print feature categories\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category}: {len(features)} features\")\n",
    "    if len(features) > 0:\n",
    "        print(f\"  Sample: {', '.join(features[:3])}{'...' if len(features) > 3 else ''}\")\n",
    "\n",
    "# 8. Apply interquartile-based imputation to all numeric features\n",
    "print(\"\\n=== APPLYING INTERQUARTILE-BASED IMPUTATION AND OUTLIER CAPPING ===\")\n",
    "numeric_columns = df_filtered.select_dtypes(include=np.number).columns.tolist()\n",
    "df_clean = impute_and_cap_using_iqr(df_filtered, columns=numeric_columns, fill_missing=True, cap_outliers=True)\n",
    "\n",
    "# 9. Create the column transformer for feature scaling\n",
    "print(\"\\n=== CREATING FEATURE PREPROCESSING PIPELINE ===\")\n",
    "\n",
    "# Get the actual feature lists based on what's available in the dataframe\n",
    "target_vars = feature_categories['target_variables']\n",
    "pct_feats = feature_categories['pct_features']\n",
    "count_feats = feature_categories['count_features']\n",
    "general_feats = feature_categories['general_features']\n",
    "link_feats = feature_categories['link_features']\n",
    "priority_feats = feature_categories['priority_features']\n",
    "type_feats = feature_categories['type_features']\n",
    "team_feats = feature_categories['team_features']\n",
    "\n",
    "# Check if any feature categories are empty\n",
    "for category, features in {'target': target_vars, 'pct': pct_feats, 'count': count_feats, \n",
    "                          'general': general_feats, 'link': link_feats,\n",
    "                          'priority': priority_feats, 'type': type_feats,\n",
    "                          'team': team_feats}.items():\n",
    "    if not features:\n",
    "        print(f\"Warning: No {category} features available for scaling\")\n",
    "\n",
    "# Create transformers only for non-empty feature lists\n",
    "transformers = []\n",
    "if target_vars:\n",
    "    transformers.append(('target_std', StandardScaler(), target_vars))\n",
    "if pct_feats:\n",
    "    transformers.append(('pct_minmax', MinMaxScaler(), pct_feats))\n",
    "if count_feats:\n",
    "    transformers.append(('count_std', StandardScaler(), count_feats))\n",
    "if general_feats:\n",
    "    transformers.append(('general_std', StandardScaler(), general_feats))\n",
    "if link_feats:\n",
    "    transformers.append(('link_std', StandardScaler(), link_feats))\n",
    "if priority_feats:\n",
    "    transformers.append(('priority_std', StandardScaler(), priority_feats))\n",
    "if type_feats:\n",
    "    transformers.append(('type_std', StandardScaler(), type_feats))\n",
    "if team_feats:\n",
    "    transformers.append(('team_robust', RobustScaler(), team_feats))\n",
    "\n",
    "feature_preprocessor = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough'  # Any columns not specified will pass through\n",
    ")\n",
    "\n",
    "# 10. Apply the preprocessing to features\n",
    "print(\"Scaling features...\")\n",
    "# Check if df_clean has any data\n",
    "if df_clean.empty:\n",
    "    print(\"Error: No features available for scaling!\")\n",
    "else:\n",
    "    scaled_features = feature_preprocessor.fit_transform(df_clean)\n",
    "\n",
    "    # 11. Create DataFrame with scaled features\n",
    "    feature_names = feature_preprocessor.get_feature_names_out()\n",
    "    df_scaled_features = pd.DataFrame(scaled_features, columns=feature_names, index=df_clean.index)\n",
    "\n",
    "    # 12. Check if we've introduced any NaN values during scaling\n",
    "    nan_count = df_scaled_features.isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Warning: {nan_count} NaN values introduced during scaling\")\n",
    "        # Replace NaNs with 0\n",
    "        df_scaled_features = df_scaled_features.fillna(0)\n",
    "        print(\"NaN values have been replaced with 0\")\n",
    "\n",
    "    # 13. Save the results\n",
    "    print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "    # Save the clean but unscaled dataset (after IQR imputation and outlier capping)\n",
    "    df_clean.to_csv(f'{results_dir}/common_features_iqr_cleaned_with_targets.csv', index=False)\n",
    "    print(\"1. Saved cleaned dataset (after IQR processing): common_features_iqr_cleaned_with_targets.csv\")\n",
    "\n",
    "    # Save the preprocessor for later use\n",
    "    with open(f'{results_dir}/jira_feature_preprocessor_with_targets.pkl', 'wb') as f:\n",
    "        pickle.dump(feature_preprocessor, f)\n",
    "    print(\"2. Saved feature preprocessor: jira_feature_preprocessor_with_targets.pkl\")\n",
    "\n",
    "    # Save the dataset with scaled features\n",
    "    df_scaled_features.to_csv(f'{results_dir}/common_features_scaled_with_targets.csv', index=False)\n",
    "    print(\"3. Saved scaled features: common_features_scaled_with_targets.csv\")\n",
    "\n",
    "    # Also save just the target variables separately for easy access\n",
    "    if target_vars:\n",
    "        df_targets = df_clean[target_vars]\n",
    "        df_targets.to_csv(f'{results_dir}/target_variables.csv', index=False)\n",
    "        print(\"4. Saved target variables separately: target_variables.csv\")\n",
    "\n",
    "    # 14. Print some summary statistics\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Filtered dataset (with target variables): {df_filtered.shape}\")\n",
    "    print(f\"Final processed dataset shape: {df_scaled_features.shape}\")\n",
    "    print(f\"Removed {len(time_columns)} time-related columns (preserved {len(target_vars)} target variables)\")\n",
    "    if target_vars:\n",
    "        print(f\"Target variables preserved: {', '.join(target_vars)}\")\n",
    "\n",
    "print(\"\\n=== PROCESSING COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./prepared_processed_data_2/common_features_iqr_cleaned_with_targets.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('./prepared_processed_data_2/common_features_iqr_cleaned_with_targets.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Identify the resolution hours features (target variables)\n",
    "resolution_hours_features = [\n",
    "    'avg_resolution_hours', \n",
    "    'median_resolution_hours', \n",
    "    'total_resolution_hours'\n",
    "]\n",
    "\n",
    "# 1. Analyze the distribution of total_resolution_hours\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df['total_resolution_hours'], bins=50)\n",
    "plt.title('Distribution of Total Resolution Hours')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(y=df['total_resolution_hours'])\n",
    "plt.title('Boxplot of Total Resolution Hours')\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Take log for better visualization\n",
    "sns.histplot(np.log1p(df['total_resolution_hours']), bins=50)\n",
    "plt.title('Log Distribution of Total Resolution Hours')\n",
    "plt.xlabel('Log(Hours+1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "# Calculate upper whisker for visualization\n",
    "q1 = np.percentile(df['total_resolution_hours'], 25)\n",
    "q3 = np.percentile(df['total_resolution_hours'], 75)\n",
    "iqr = q3 - q1\n",
    "upper_whisker = q3 + 1.5 * iqr\n",
    "print(f\"Upper whisker boundary: {upper_whisker:.2f} hours\")\n",
    "# Zoom in on the box plot for better visibility\n",
    "sns.boxplot(y=df['total_resolution_hours'].clip(upper=upper_whisker*2))\n",
    "plt.title('Zoomed Boxplot (Up to 2x Upper Whisker)')\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('total_resolution_hours_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Advanced outlier identification approaches\n",
    "\n",
    "# Option 1: Project Size Segmentation\n",
    "print(\"\\n--- Project Size Segmentation ---\")\n",
    "# Define project size categories based on total_issues\n",
    "size_bins = [0, 10, 50, 100, 500, float('inf')]\n",
    "size_labels = ['Very Small', 'Small', 'Medium', 'Large', 'Very Large']\n",
    "df['project_size'] = pd.cut(df['total_issues'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Calculate statistics by project size\n",
    "size_stats = df.groupby('project_size')[resolution_hours_features].agg(['mean', 'median', 'std', 'count'])\n",
    "print(\"Resolution hours statistics by project size:\")\n",
    "print(size_stats)\n",
    "\n",
    "# Visualize resolution hours by project size\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='project_size', y='total_resolution_hours', data=df)\n",
    "plt.title('Total Resolution Hours by Project Size')\n",
    "plt.ylabel('Hours')\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.savefig('resolution_hours_by_project_size.png')\n",
    "plt.close()\n",
    "\n",
    "# Option 2: Clustering-based Segmentation\n",
    "print(\"\\n--- Clustering-based Segmentation ---\")\n",
    "# Select features for clustering\n",
    "clustering_features = ['total_issues', 'total_resolution_hours']\n",
    "# Filter out NaN values\n",
    "df_cluster = df[clustering_features].dropna()\n",
    "# Apply log transformation to resolution hours for better clustering\n",
    "df_cluster_scaled = df_cluster.copy()\n",
    "df_cluster_scaled['total_resolution_hours'] = np.log1p(df_cluster['total_resolution_hours'])\n",
    "# Standardize features\n",
    "scaler = RobustScaler()\n",
    "df_cluster_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_cluster_scaled),\n",
    "    columns=df_cluster_scaled.columns\n",
    ")\n",
    "\n",
    "# Apply KMeans clustering\n",
    "n_clusters = 4  # Can be adjusted based on data visualization\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(df_cluster_scaled)\n",
    "df_cluster['cluster'] = clusters\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    df_cluster['total_issues'],\n",
    "    df_cluster['total_resolution_hours'],\n",
    "    c=df_cluster['cluster'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('Clusters of Projects by Issues and Resolution Hours')\n",
    "plt.xlabel('Total Issues')\n",
    "plt.ylabel('Total Resolution Hours')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('project_clusters.png')\n",
    "plt.close()\n",
    "\n",
    "# Calculate statistics by cluster\n",
    "cluster_stats = df_cluster.groupby('cluster')[clustering_features].agg(['mean', 'median', 'std', 'count'])\n",
    "print(\"Project statistics by cluster:\")\n",
    "print(cluster_stats)\n",
    "\n",
    "# 3. Recommended approach: Stratified modeling with specialized handling for each segment\n",
    "\n",
    "# Identify extreme outliers based on statistical measures\n",
    "def detect_extreme_outliers(df, column, method='iqr', threshold=3):\n",
    "    \"\"\"\n",
    "    Identify extreme outliers that should be handled specially\n",
    "    \"\"\"\n",
    "    values = df[column].values\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        q1 = np.percentile(values, 25)\n",
    "        q3 = np.percentile(values, 75)\n",
    "        iqr = q3 - q1\n",
    "        upper_bound = q3 + threshold * iqr\n",
    "        outlier_indices = np.where(values > upper_bound)[0]\n",
    "        \n",
    "    elif method == 'zscore':\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values)\n",
    "        z_scores = (values - mean) / std\n",
    "        outlier_indices = np.where(np.abs(z_scores) > threshold)[0]\n",
    "        \n",
    "    elif method == 'percentile':\n",
    "        percentile = 100 - (100 / threshold)  # e.g., threshold=10 means top 10%\n",
    "        upper_bound = np.percentile(values, percentile)\n",
    "        outlier_indices = np.where(values > upper_bound)[0]\n",
    "    \n",
    "    return outlier_indices, df.index[outlier_indices].tolist()\n",
    "\n",
    "# Apply multiple detection methods\n",
    "print(\"\\n--- Extreme Outlier Detection ---\")\n",
    "iqr_outliers, iqr_indices = detect_extreme_outliers(df, 'total_resolution_hours', method='iqr', threshold=3)\n",
    "zscore_outliers, zscore_indices = detect_extreme_outliers(df, 'total_resolution_hours', method='zscore', threshold=3)\n",
    "percentile_outliers, percentile_indices = detect_extreme_outliers(df, 'total_resolution_hours', method='percentile', threshold=20)  # Top 5%\n",
    "\n",
    "print(f\"IQR method: {len(iqr_outliers)} extreme outliers detected\")\n",
    "print(f\"Z-score method: {len(zscore_outliers)} extreme outliers detected\")\n",
    "print(f\"Percentile method: {len(percentile_outliers)} extreme outliers detected\")\n",
    "\n",
    "# Find the intersection of outliers detected by multiple methods\n",
    "common_outliers = set(iqr_indices) & set(zscore_indices) & set(percentile_indices)\n",
    "print(f\"Number of outliers detected by all three methods: {len(common_outliers)}\")\n",
    "\n",
    "# 4. Recommended approach for handling outliers in software effort estimation:\n",
    "\n",
    "print(\"\\n--- Recommended Approach ---\")\n",
    "print(\"1. Stratified modeling - Create separate models for different project size categories\")\n",
    "print(\"2. Specialized transformations for each stratum\")\n",
    "print(\"3. Consider removing or special handling for the most extreme outliers detected by multiple methods\")\n",
    "\n",
    "# Create a cleaned dataset with different levels of outlier removal\n",
    "df_no_extreme_outliers = df[~df.index.isin(list(common_outliers))].copy()\n",
    "print(f\"\\nDataset shape after removing extreme outliers: {df_no_extreme_outliers.shape}\")\n",
    "\n",
    "# Apply log transformation to resolution hours\n",
    "for feature in resolution_hours_features:\n",
    "    df_no_extreme_outliers[f'{feature}_log'] = np.log1p(df_no_extreme_outliers[feature])\n",
    "\n",
    "# Compare distributions before and after outlier removal\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, feature in enumerate(resolution_hours_features):\n",
    "    # Original distribution\n",
    "    plt.subplot(3, 3, i*3+1)\n",
    "    sns.histplot(df[feature], bins=30)\n",
    "    plt.title(f'Original {feature}')\n",
    "    plt.xlabel('Hours')\n",
    "    \n",
    "    # After extreme outlier removal\n",
    "    plt.subplot(3, 3, i*3+2)\n",
    "    sns.histplot(df_no_extreme_outliers[feature], bins=30)\n",
    "    plt.title(f'After Extreme Outlier Removal')\n",
    "    plt.xlabel('Hours')\n",
    "    \n",
    "    # Log transformation after outlier removal\n",
    "    plt.subplot(3, 3, i*3+3)\n",
    "    sns.histplot(df_no_extreme_outliers[f'{feature}_log'], bins=30)\n",
    "    plt.title(f'Log-transformed (No Extreme Outliers)')\n",
    "    plt.xlabel('Log(Hours+1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Save the prepared datasets for modeling\n",
    "df_no_extreme_outliers.to_csv('se_effort_no_extreme_outliers.csv', index=False)\n",
    "\n",
    "# Create stratified datasets based on project size\n",
    "for size in size_labels:\n",
    "    size_df = df_no_extreme_outliers[df_no_extreme_outliers['project_size'] == size].copy()\n",
    "    if len(size_df) > 0:\n",
    "        size_df.to_csv(f'se_effort_{size.lower().replace(\" \", \"_\")}_projects.csv', index=False)\n",
    "        print(f\"Created dataset for {size} projects: {size_df.shape[0]} records\")\n",
    "\n",
    "print(\"\\nAnalysis and dataset preparation complete!\")\n",
    "print(\"Generated files:\")\n",
    "print(\"- total_resolution_hours_distribution.png: Original data distribution\")\n",
    "print(\"- resolution_hours_by_project_size.png: Boxplots by project size\")\n",
    "print(\"- project_clusters.png: Clustering visualization\")\n",
    "print(\"- distribution_comparison.png: Effect of outlier removal\")\n",
    "print(\"- se_effort_no_extreme_outliers.csv: Dataset with extreme outliers removed\")\n",
    "print(\"- se_effort_[size]_projects.csv: Stratified datasets by project size\")\n",
    "\n",
    "print(\"\\nRecommendations for modeling:\")\n",
    "print(\"1. For general-purpose modeling: Use 'se_effort_no_extreme_outliers.csv' with log transformation\")\n",
    "print(\"2. For size-specific modeling: Use the stratified datasets\")\n",
    "print(\"3. Consider building ensemble models that specialize in different project sizes\")\n",
    "print(\"4. Use the extreme outliers list to flag potentially problematic projects during prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processed_data/se_effort_no_extreme_outliers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./processed_data/se_effort_no_extreme_outliers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Start a D-Tale session and open it in the browser\u001b[39;00m\n\u001b[1;32m     11\u001b[0m d \u001b[38;5;241m=\u001b[39m dtale\u001b[38;5;241m.\u001b[39mshow(df, ignore_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_cell_edits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './processed_data/se_effort_no_extreme_outliers.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./se_effort_no_extreme_outliers.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
