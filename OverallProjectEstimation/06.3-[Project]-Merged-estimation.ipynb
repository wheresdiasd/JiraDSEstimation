{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'task_estimation_resolution_hours'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'merged_task_data/merged_project_task_data.csv'\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Convert string columns that should be numeric\n",
    "for col in df.columns:\n",
    "    if col.startswith('fields.issuetype') or col.startswith('fields.priority'):\n",
    "        # Keep these as string/categorical\n",
    "        continue\n",
    "    try:\n",
    "        # Try converting to numeric, coerce errors to NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    except:\n",
    "        # If conversion fails completely, leave as is\n",
    "        pass\n",
    "\n",
    "# Use only x% of the data for faster processing\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "print(f\"Reduced dataset shape (X%): {df.shape}\")\n",
    "\n",
    "# Define target variable - to be used only as the target, never as a feature\n",
    "target_variable = 'resolution_hours'\n",
    "\n",
    "# Define fields that must be excluded\n",
    "fields_to_exclude = ['fields.status.name', 'fields.project.key', 'fields.priority.name', 'fields.project.name']\n",
    "\n",
    "# Check which of these fields actually exist in the dataframe\n",
    "existing_fields_to_exclude = [field for field in fields_to_exclude if field in df.columns]\n",
    "print(f\"\\nFields to exclude that exist in the dataframe: {existing_fields_to_exclude}\")\n",
    "\n",
    "# Remove the fields from the dataframe itself to avoid correlation errors\n",
    "if existing_fields_to_exclude:\n",
    "    print(f\"Dropping these fields from the dataframe: {existing_fields_to_exclude}\")\n",
    "    df = df.drop(columns=existing_fields_to_exclude)\n",
    "    print(f\"Dataframe shape after dropping: {df.shape}\")\n",
    "\n",
    "# Check for missing values in the target variable\n",
    "print(f\"Missing values in target: {df[target_variable].isna().sum()}\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df.dropna(subset=[target_variable])\n",
    "print(f\"Dataset shape after dropping missing targets: {df.shape}\")\n",
    "\n",
    "# Plot distribution of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[target_variable].clip(0, 500), bins=50)\n",
    "plt.title('Distribution of Total Resolution Hours (clipped at 500 for visibility)')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig(f'{output_dir}/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Identify numeric columns for correlation analysis\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# Exclude target from potential features\n",
    "numeric_features = [col for col in numeric_columns if col != target_variable]\n",
    "print(f\"\\nUsing {len(numeric_features)} numeric features for correlation calculation\")\n",
    "\n",
    "# Calculate correlations between features and target\n",
    "if target_variable in df.columns and numeric_features:\n",
    "    # Create a dataframe with just the features and target for correlation\n",
    "    corr_df = df[numeric_features + [target_variable]]\n",
    "    target_correlations = corr_df.corr()[target_variable].drop(target_variable).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 10 correlated features with target:\")\n",
    "    print(target_correlations.head(10))\n",
    "    print(\"\\nBottom 10 correlated features with target:\")\n",
    "    print(target_correlations.tail(10))\n",
    "    \n",
    "    # Print all features that have correlation > 0.8 with target\n",
    "    high_corr_features = target_correlations[target_correlations > 0.8].index.tolist()\n",
    "    print(f\"\\nFeatures with >0.8 correlation with target: {high_corr_features}\")\n",
    "else:\n",
    "    print(\"Cannot calculate target correlations - either target or features missing\")\n",
    "    high_corr_features = []\n",
    "    \n",
    "# Define suspicious features (features that may lead to data leakage)\n",
    "suspicious_features = [\n",
    "    # Resolution-related features that would leak the target information\n",
    "    'avg_resolution_hours', 'median_resolution_hours', \n",
    "    'resolution_hours', 'log_resolution_hours'\n",
    "]\n",
    "\n",
    "# Add highly correlated features to suspicious list\n",
    "suspicious_features.extend(high_corr_features)\n",
    "\n",
    "# # Keep count_std__total_issues despite high correlation (if it exists)\n",
    "# if 'count_std__total_issues' in suspicious_features:\n",
    "#     suspicious_features.remove('count_std__total_issues')\n",
    "#     print(\"Keeping 'count_std__total_issues' despite high correlation with target\")\n",
    "\n",
    "# Remove count_std__total_issues\n",
    "if 'count_std__total_issues' in suspicious_features:\n",
    "    # Do nothing, it's already excluded\n",
    "    print(\"Removing 'count_std__total_issues' from features\")\n",
    "else:\n",
    "    suspicious_features.append('count_std__total_issues')\n",
    "    print(\"'count_std__total_issues' marked for exclusion\")\n",
    "\n",
    "# Add excluded fields to suspicious list\n",
    "suspicious_features.extend(fields_to_exclude)\n",
    "\n",
    "# Make sure target variable is not in suspicious features list (it's our target, not a feature)\n",
    "if target_variable in suspicious_features:\n",
    "    suspicious_features.remove(target_variable)\n",
    "    \n",
    "print(f\"\\nRemoving suspicious features: {suspicious_features}\")\n",
    "\n",
    "# Original features list, excluding suspicious features\n",
    "features = [\n",
    "    'fields.issuetype.id', 'fields.priority.id', 'priority_id', 'issue_type_id',\n",
    "    'type_task', 'type_bug', 'inward_count', 'outward_count', 'type_sub_task',\n",
    "    'created_hour', 'created_month', 'created_year',\n",
    "    'is_type_bug', 'is_type_task', 'is_type_story', 'is_type_improvement',\n",
    "    'is_type_new_feature', 'is_type_epic', 'is_type_sub-task',\n",
    "    'is_priority_blocker', 'is_priority_critical', 'is_priority_major',\n",
    "    'is_priority_minor', 'is_priority_trivial',\n",
    "    'pct_minmax__type_bug_pct', 'pct_minmax__type_task_pct',\n",
    "    'pct_minmax__type_new_feature_pct', 'pct_minmax__type_epic_pct',\n",
    "    'pct_minmax__type_improvement_pct', 'pct_minmax__type_story_pct',\n",
    "    'pct_minmax__type_documentation_pct', 'pct_minmax__priority_critical_pct',\n",
    "    'pct_minmax__priority_blocker_pct', 'pct_minmax__priority_high_pct',\n",
    "    'pct_minmax__priority_low_pct',\n",
    "    'remainder__team_size_creators', 'remainder__team_size_assignees',\n",
    "    'remainder__team_size_combined',\n",
    "    'stat_robust__weighted_priority_score', 'stat_robust__issue_type_entropy',\n",
    "    'stat_robust__high_to_low_priority_ratio', 'stat_robust__bug_ratio'\n",
    "]\n",
    "\n",
    "# If count_std__total_issues exists in the dataframe, add it to our features list\n",
    "# if 'count_std__total_issues' in df.columns and 'count_std__total_issues' not in features:\n",
    "#     features.append('count_std__total_issues')\n",
    "#     print(\"Added 'count_std__total_issues' to features list\")\n",
    "\n",
    "# Ensure we exclude the additional fields by checking if they exist in the dataset\n",
    "for field in fields_to_exclude:\n",
    "    if field in df.columns and field not in suspicious_features:\n",
    "        suspicious_features.append(field)\n",
    "        print(f\"Added {field} to suspicious features list\")\n",
    "\n",
    "# Filter dataset to include only features available in our dataset and not in suspicious list\n",
    "available_features = [f for f in features if f in df.columns and f not in suspicious_features]\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "print(f\"Missing features: {len(missing_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing feature list: {missing_features}\")\n",
    "    \n",
    "# Make absolutely sure we're not using the target as a feature\n",
    "if target_variable in available_features:\n",
    "    available_features.remove(target_variable)\n",
    "    print(f\"WARNING: Removed target variable '{target_variable}' from features list\")\n",
    "    \n",
    "# Explicitly check all fields that need to be dropped\n",
    "print(\"\\nChecking for fields that need to be dropped:\")\n",
    "for field in fields_to_exclude:\n",
    "    if field in df.columns:\n",
    "        print(f\"Field '{field}' exists in the dataframe\")\n",
    "        if field in available_features:\n",
    "            available_features.remove(field)\n",
    "            print(f\"-> Removed '{field}' from available features\")\n",
    "    else:\n",
    "        print(f\"Field '{field}' does not exist in the dataframe\")\n",
    "\n",
    "# Double-check if any of these fields might be in the final feature list\n",
    "for field in fields_to_exclude:\n",
    "    if field in available_features:\n",
    "        available_features.remove(field)\n",
    "        print(f\"-> Explicitly removed '{field}' from available features\")\n",
    "        \n",
    "# Print final confirmation\n",
    "print(\"\\nConfirming excluded fields:\")\n",
    "for field in fields_to_exclude:\n",
    "    print(f\"'{field}' is {'NOT' if field not in available_features else 'STILL'} in the feature list\")\n",
    "\n",
    "# Check for multicollinearity\n",
    "# Use only numeric features for correlation calculation\n",
    "numeric_available_features = [f for f in available_features if f in numeric_features]\n",
    "print(f\"\\nUsing {len(numeric_available_features)} numeric features for multicollinearity check\")\n",
    "\n",
    "if len(numeric_available_features) > 0:\n",
    "    # Verify one more time we don't have the target as a feature\n",
    "    if target_variable in numeric_available_features:\n",
    "        numeric_available_features.remove(target_variable)\n",
    "        print(f\"WARNING: Removed target variable from multicollinearity calculation\")\n",
    "    \n",
    "    correlation_matrix = df[numeric_available_features].corr().abs()\n",
    "    upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool_))\n",
    "    high_corr_pairs = [(col1, col2) for col1 in upper_tri.columns for col2 in upper_tri.index if upper_tri.loc[col2, col1] > 0.95]\n",
    "\n",
    "    print(f\"\\nHigh correlation pairs (>0.95):\")\n",
    "    for col1, col2 in high_corr_pairs:\n",
    "        print(f\"{col1} - {col2}: {upper_tri.loc[col2, col1]:.4f}\")\n",
    "    \n",
    "    # Optionally remove one of each highly correlated pair\n",
    "    features_to_drop = []\n",
    "    for col1, col2 in high_corr_pairs:\n",
    "        # # Keep count_std__total_issues even if highly correlated with another feature\n",
    "        # if col2 == 'count_std__total_issues':\n",
    "        #     # Swap the pair to keep count_std__total_issues\n",
    "        #     features_to_drop.append(col1)\n",
    "        # elif col1 == 'count_std__total_issues':\n",
    "        #     # Already in the order we want\n",
    "        #     features_to_drop.append(col2)\n",
    "        # else:\n",
    "            # Default: Keep the first one, drop the second one\n",
    "            if col2 not in features_to_drop:\n",
    "                features_to_drop.append(col2)\n",
    "\n",
    "    print(f\"\\nDropping features due to multicollinearity: {features_to_drop}\")\n",
    "else:\n",
    "    print(\"No numeric features available for multicollinearity check\")\n",
    "    features_to_drop = []\n",
    "\n",
    "final_features = [f for f in available_features if f not in features_to_drop]\n",
    "print(f\"Final feature count: {len(final_features)}\")\n",
    "\n",
    "# Final check to ensure we've dropped the specified fields\n",
    "for field in fields_to_exclude:\n",
    "    if field in final_features:\n",
    "        final_features.remove(field)\n",
    "        print(f\"Final check: Removed '{field}' from final_features\")\n",
    "        \n",
    "# Print confirmation of fields not in final features\n",
    "print(\"\\nFinal confirmation - excluded fields:\")\n",
    "for field in fields_to_exclude:\n",
    "    present = field in final_features\n",
    "    print(f\"'{field}' is {'STILL in' if present else 'NOT in'} the final feature list\")\n",
    "\n",
    "# Check if count_std__total_issues made it to the final list\n",
    "if 'count_std__total_issues' in final_features:\n",
    "    print(\"\\nSuccessfully kept 'count_std__total_issues' in the final feature list\")\n",
    "else:\n",
    "    print(\"\\n'count_std__total_issues' is not in the final feature list\")\n",
    "    # If it's in the dataframe but not in final features, this is concerning\n",
    "    if 'count_std__total_issues' in df.columns:\n",
    "        print(\"CAUTION: 'count_std__total_issues' exists in the dataset but was excluded from final features\")\n",
    "\n",
    "# Prepare the data\n",
    "X = df[final_features].copy()\n",
    "y = df[target_variable]  # The target variable\n",
    "\n",
    "# Fill any remaining NaN values with median\n",
    "for col in X.columns:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Split the data with stratification on binned target to ensure similar distributions\n",
    "y_binned = pd.qcut(y, q=10, duplicates='drop')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "\n",
    "# Final verification: ensure target is not used as a feature\n",
    "print(\"\\nFinal verification before model training:\")\n",
    "print(f\"Target variable: {target_variable}\")\n",
    "print(f\"Number of features: {len(final_features)}\")\n",
    "if target_variable in final_features:\n",
    "    print(f\"ERROR: Target variable found in feature list!\")\n",
    "    final_features.remove(target_variable)\n",
    "    X = df[final_features].copy()  # Recreate X without the target\n",
    "    print(f\"Removed target from features. New feature count: {len(final_features)}\")\n",
    "    \n",
    "    # Re-split with corrected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y_binned)\n",
    "else:\n",
    "    print(\"Verified: Target is not being used as a feature\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save data splits for later reference\n",
    "with open(f'{output_dir}/data_splits.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'feature_names': final_features,\n",
    "        'scaler': scaler\n",
    "    }, f)\n",
    "\n",
    "# Define the original models with more conservative hyperparameters to prevent overfitting\n",
    "original_models = {\n",
    "    'Random_Forest': RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10,\n",
    "        min_samples_leaf=5,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Gradient_Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=5, \n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Copy for tuning later\n",
    "models = original_models.copy()\n",
    "\n",
    "# Save original model hyperparameters for reference\n",
    "original_params = {name: model.get_params() for name, model in original_models.items()}\n",
    "with open(f'{output_dir}/original_hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(original_params, f)\n",
    "\n",
    "# First, train and evaluate original models\n",
    "original_results = {}\n",
    "original_predictions = {}\n",
    "\n",
    "print(\"\\nTraining original models (before hyperparameter tuning)...\")\n",
    "for name, model in original_models.items():\n",
    "    print(f\"\\nTraining original {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    original_predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)  # Rank correlation, less affected by outliers\n",
    "    \n",
    "    original_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    print(f\"Original {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}, Spearman: {spearman_corr:.4f}\")\n",
    "    \n",
    "    # Save the original model\n",
    "    with open(f'{output_dir}/original_{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Create original ensemble prediction\n",
    "original_ensemble_pred = np.mean([original_predictions[name] for name in original_models.keys()], axis=0)\n",
    "\n",
    "# Calculate metrics for original ensemble\n",
    "original_ensemble_mae = mean_absolute_error(y_test, original_ensemble_pred)\n",
    "original_ensemble_rmse = np.sqrt(mean_squared_error(y_test, original_ensemble_pred))\n",
    "original_ensemble_r2 = r2_score(y_test, original_ensemble_pred)\n",
    "original_ensemble_spearman, _ = spearmanr(y_test, original_ensemble_pred)\n",
    "\n",
    "original_results['Ensemble'] = {\n",
    "    'MAE': original_ensemble_mae,\n",
    "    'RMSE': original_ensemble_rmse,\n",
    "    'R2': original_ensemble_r2,\n",
    "    'Spearman': original_ensemble_spearman\n",
    "}\n",
    "\n",
    "print(f\"Original Ensemble - MAE: {original_ensemble_mae:.2f}, RMSE: {original_ensemble_rmse:.2f}, R2: {original_ensemble_r2:.4f}, Spearman: {original_ensemble_spearman:.4f}\")\n",
    "\n",
    "# Save original results\n",
    "with open(f'{output_dir}/original_results.pkl', 'wb') as f:\n",
    "    pickle.dump(original_results, f)\n",
    "\n",
    "# Create prediction and residual plots for original models\n",
    "for name, y_pred in original_predictions.items():\n",
    "    print(f\"\\nCreating plots for original {name} model...\")\n",
    "    \n",
    "    # Actual vs Predicted Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Hours')\n",
    "    plt.ylabel('Predicted Hours')\n",
    "    plt.title(f'Original {name} - Actual vs Predicted')\n",
    "    plt.savefig(f'{output_dir}/original_{name}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Residual Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    residuals = y_test - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Hours')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.title(f'Original {name} - Residual Plot')\n",
    "    plt.savefig(f'{output_dir}/original_{name}_residuals.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create a plot for the original ensemble predictions\n",
    "# Actual vs Predicted Plot for Original Ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, original_ensemble_pred, alpha=0.3)\n",
    "plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Hours')\n",
    "plt.ylabel('Predicted Hours')\n",
    "plt.title('Original Ensemble - Actual vs Predicted')\n",
    "plt.savefig(f'{output_dir}/original_ensemble_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Residual Plot for Original Ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "original_ensemble_residuals = y_test - original_ensemble_pred\n",
    "plt.scatter(original_ensemble_pred, original_ensemble_residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Hours')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Original Ensemble - Residual Plot')\n",
    "plt.savefig(f'{output_dir}/original_ensemble_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nGenerated prediction and residual plots for original models and ensemble.\")\n",
    "\n",
    "#########################\n",
    "# Hyperparameter Tuning #\n",
    "#########################\n",
    "\n",
    "print(\"\\nPerforming hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_leaf': [1, 3, 5],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_samples_leaf': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning for each model\n",
    "tuned_models = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster execution\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grids[name],\n",
    "        n_iter=10,  # Try 10 random combinations\n",
    "        cv=3,        # 3-fold cross-validation for speed\n",
    "        scoring='neg_mean_absolute_error',  # Optimize for MAE\n",
    "        n_jobs=-1,   # Use all available processors\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(f\"Best parameters for {name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Best score: {-grid_search.best_score_:.4f} (MAE)\")\n",
    "    \n",
    "    # Store best parameters and scores\n",
    "    best_params[name] = grid_search.best_params_\n",
    "    best_scores[name] = -grid_search.best_score_\n",
    "    \n",
    "    # Update the model with the best estimator\n",
    "    tuned_models[name] = grid_search.best_estimator_\n",
    "\n",
    "# Save hyperparameter tuning results\n",
    "with open(f'{output_dir}/hyperparameter_tuning_results.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'best_params': best_params,\n",
    "        'best_scores': best_scores\n",
    "    }, f)\n",
    "\n",
    "# Replace original models with tuned models for future steps\n",
    "models = tuned_models\n",
    "\n",
    "# Perform cross-validation on tuned models\n",
    "print(\"\\nPerforming 5-fold cross-validation with tuned models...\")\n",
    "tuned_cv_results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    tuned_cv_results[name] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_scores.mean(),\n",
    "        'std': cv_scores.std()\n",
    "    }\n",
    "    print(f\"{name} CV R² scores: {cv_scores}\")\n",
    "    print(f\"{name} CV R² mean: {cv_scores.mean():.4f}, std: {cv_scores.std():.4f}\")\n",
    "\n",
    "# Save cross-validation results\n",
    "with open(f'{output_dir}/tuned_cv_results.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_cv_results, f)\n",
    "\n",
    "# Train and evaluate tuned models\n",
    "tuned_results = {}\n",
    "tuned_predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining tuned {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    tuned_predictions[name] = y_pred\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    spearman_corr, _ = spearmanr(y_test, y_pred)\n",
    "    \n",
    "    tuned_results[name] = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Spearman': spearman_corr\n",
    "    }\n",
    "    \n",
    "    print(f\"Tuned {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}, Spearman: {spearman_corr:.4f}\")\n",
    "    \n",
    "    # Save the tuned model\n",
    "    with open(f'{output_dir}/tuned_{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "    plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Hours')\n",
    "    plt.ylabel('Predicted Hours')\n",
    "    plt.title(f'Tuned {name} - Actual vs Predicted')\n",
    "    plt.savefig(f'{output_dir}/tuned_{name}_predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Add residual subplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    residuals = y_test - y_pred\n",
    "    plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Hours')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.title(f'Tuned {name} - Residual Plot')\n",
    "    plt.savefig(f'{output_dir}/tuned_{name}_residuals.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create a tuned ensemble prediction\n",
    "tuned_ensemble_pred = np.mean([tuned_predictions[name] for name in models.keys()], axis=0)\n",
    "\n",
    "# Calculate metrics for tuned ensemble\n",
    "tuned_ensemble_mae = mean_absolute_error(y_test, tuned_ensemble_pred)\n",
    "tuned_ensemble_rmse = np.sqrt(mean_squared_error(y_test, tuned_ensemble_pred))\n",
    "tuned_ensemble_r2 = r2_score(y_test, tuned_ensemble_pred)\n",
    "tuned_ensemble_spearman, _ = spearmanr(y_test, tuned_ensemble_pred)\n",
    "\n",
    "tuned_results['Ensemble'] = {\n",
    "    'MAE': tuned_ensemble_mae,\n",
    "    'RMSE': tuned_ensemble_rmse,\n",
    "    'R2': tuned_ensemble_r2,\n",
    "    'Spearman': tuned_ensemble_spearman\n",
    "}\n",
    "\n",
    "print(f\"Tuned Ensemble - MAE: {tuned_ensemble_mae:.2f}, RMSE: {tuned_ensemble_rmse:.2f}, R2: {tuned_ensemble_r2:.4f}, Spearman: {tuned_ensemble_spearman:.4f}\")\n",
    "\n",
    "# Save tuned results\n",
    "with open(f'{output_dir}/tuned_results.pkl', 'wb') as f:\n",
    "    pickle.dump(tuned_results, f)\n",
    "\n",
    "# Plot actual vs predicted for tuned ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, tuned_ensemble_pred, alpha=0.3)\n",
    "plt.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Hours')\n",
    "plt.ylabel('Predicted Hours')\n",
    "plt.title('Tuned Ensemble - Actual vs Predicted')\n",
    "plt.savefig(f'{output_dir}/tuned_ensemble_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot residuals for tuned ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "tuned_ensemble_residuals = y_test - tuned_ensemble_pred\n",
    "plt.scatter(tuned_ensemble_pred, tuned_ensemble_residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Hours')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Tuned Ensemble - Residual Plot')\n",
    "plt.savefig(f'{output_dir}/tuned_ensemble_residuals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Identify the best tuned model\n",
    "best_tuned_model_name = min(tuned_results, key=lambda x: tuned_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "best_tuned_model = models[best_tuned_model_name]\n",
    "print(f\"Best tuned model based on MAE: {best_tuned_model_name}\")\n",
    "\n",
    "# Save the best tuned model\n",
    "with open(f'{output_dir}/best_tuned_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_tuned_model, f)\n",
    "\n",
    "# Plot feature importance for the best tuned model\n",
    "if hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = best_tuned_model.feature_importances_\n",
    "    \n",
    "    # Sort features by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Plot the top 20 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f'Feature Importance - Best Tuned Model ({best_tuned_model_name})')\n",
    "    plt.bar(range(min(20, len(final_features))), \n",
    "            importances[indices[:20]], \n",
    "            align='center')\n",
    "    plt.xticks(range(min(20, len(final_features))), \n",
    "               [final_features[i] for i in indices[:20]], \n",
    "               rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/best_tuned_model_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a DataFrame for easier analysis\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance to CSV\n",
    "    feature_importance_df.to_csv(f'{output_dir}/feature_importance.csv', index=False)\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "\n",
    "    # Calculate and plot correlations between top features and the target\n",
    "    top_features = [f for f in feature_importance_df['Feature'].head(10).tolist() if f in numeric_features]\n",
    "    if top_features:\n",
    "        # Final check that we don't include the target in the correlation matrix\n",
    "        if target_variable in top_features:\n",
    "            top_features.remove(target_variable)\n",
    "            print(f\"WARNING: Removed target from top features correlation\")\n",
    "            \n",
    "        print(f\"Calculating correlations for top {len(top_features)} numeric features\")\n",
    "        if top_features:\n",
    "            corr_columns = top_features + [target_variable]\n",
    "            corr_data = df[corr_columns].copy()\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr_data.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "            plt.title('Correlation Between Top Features and Target')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_dir}/top_features_correlation.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(\"No valid top features remain for correlation analysis\")\n",
    "    else:\n",
    "        print(\"No numeric top features available for correlation analysis\")\n",
    "\n",
    "    # Group feature importance by categories\n",
    "    # Define categories based on feature name prefixes\n",
    "    categories = {\n",
    "        'Issue Type': [f for f in final_features if 'type_' in f or 'is_type_' in f],\n",
    "        'Priority': [f for f in final_features if 'priority' in f],\n",
    "        'Project Stats': [f for f in final_features if 'count_' in f or 'pct_' in f],\n",
    "        'Team Size': [f for f in final_features if 'team_size' in f],\n",
    "        'Created Time': [f for f in final_features if 'created_' in f],\n",
    "        'Robust Stats': [f for f in final_features if 'stat_robust' in f],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    # Assign remaining features to 'Other'\n",
    "    for feature in final_features:\n",
    "        if not any(feature in cat_features for cat_features in categories.values()):\n",
    "            categories['Other'].append(feature)\n",
    "    \n",
    "    # Calculate importance by category\n",
    "    category_importance = {}\n",
    "    for category, cat_features in categories.items():\n",
    "        if cat_features:  # Skip empty categories\n",
    "            total_importance = sum(feature_importance_df.loc[feature_importance_df['Feature'].isin(cat_features), 'Importance'])\n",
    "            category_importance[category] = total_importance\n",
    "    \n",
    "    # Plot category importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    categories_sorted = sorted(category_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    categories_names = [item[0] for item in categories_sorted]\n",
    "    categories_values = [item[1] for item in categories_sorted]\n",
    "    \n",
    "    plt.bar(categories_names, categories_values)\n",
    "    plt.xlabel('Feature Category')\n",
    "    plt.ylabel('Total Importance')\n",
    "    plt.title('Feature Importance by Category')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/best_tuned_model_feature_importance_by_category.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create visualizations to compare original vs tuned models\n",
    "comparison_dir = f'{output_dir}/comparison'\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nCreating model comparison visualizations...\")\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name in original_results.keys():\n",
    "    if model_name != 'Ensemble' or (model_name == 'Ensemble' and model_name in tuned_results):\n",
    "        for metric_name in ['MAE', 'RMSE', 'R2', 'Spearman']:\n",
    "            original_value = original_results[model_name].get(metric_name, None)\n",
    "            tuned_value = tuned_results.get(model_name, {}).get(metric_name, None)\n",
    "            \n",
    "            if original_value is not None and tuned_value is not None:\n",
    "                # Calculate improvement\n",
    "                if metric_name in ['MAE', 'RMSE']:  # Lower is better\n",
    "                    improvement = ((original_value - tuned_value) / original_value) * 100\n",
    "                else:  # Higher is better\n",
    "                    improvement = ((tuned_value - original_value) / abs(original_value)) * 100\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Metric': metric_name,\n",
    "                    'Original': original_value,\n",
    "                    'Tuned': tuned_value,\n",
    "                    'Improvement (%)': improvement\n",
    "                })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.to_csv(f'{comparison_dir}/model_comparison.csv', index=False)\n",
    "\n",
    "# Create a performance comparison bar chart for each metric\n",
    "for metric in ['MAE', 'RMSE', 'R2', 'Spearman']:\n",
    "    metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Adjust these values based on your metric\n",
    "    better_direction = 'lower' if metric in ['MAE', 'RMSE'] else 'higher'\n",
    "    \n",
    "    # Setup the bar chart\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(len(metric_data['Model'].unique()))\n",
    "    \n",
    "    # Plot bars\n",
    "    plt.bar(x - bar_width/2, metric_data['Original'], width=bar_width, label='Original', color='lightblue')\n",
    "    plt.bar(x + bar_width/2, metric_data['Tuned'], width=bar_width, label='Hyperparameter Tuned', color='darkblue')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, original in enumerate(metric_data['Original']):\n",
    "        plt.text(i - bar_width/2, original * 1.02, f'{original:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "    for i, tuned in enumerate(metric_data['Tuned']):\n",
    "        plt.text(i + bar_width/2, tuned * 1.02, f'{tuned:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'{metric} Comparison: Original vs Tuned Models ({better_direction} is better)')\n",
    "    plt.xticks(x, metric_data['Model'])\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{comparison_dir}/{metric}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create an improvement percentage chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Pivot the data for easier plotting\n",
    "pivot_df = pd.pivot_table(\n",
    "    comparison_df, \n",
    "    values='Improvement (%)', \n",
    "    index='Model', \n",
    "    columns='Metric'\n",
    ")\n",
    "\n",
    "# Plot the improvement heatmap\n",
    "sns.heatmap(\n",
    "    pivot_df, \n",
    "    annot=True, \n",
    "    cmap='RdYlGn', \n",
    "    fmt='.1f',\n",
    "    linewidths=.5, \n",
    "    center=0\n",
    ")\n",
    "plt.title('Percentage Improvement from Hyperparameter Tuning')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{comparison_dir}/improvement_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Create side-by-side comparison of best models\n",
    "# Identify best original and tuned models\n",
    "best_original_model_name = min(original_results, key=lambda x: original_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "best_tuned_model_name = min(tuned_results, key=lambda x: tuned_results[x]['MAE'] if x != 'Ensemble' else float('inf'))\n",
    "\n",
    "print(f\"Best original model: {best_original_model_name}\")\n",
    "print(f\"Best tuned model: {best_tuned_model_name}\")\n",
    "\n",
    "# Create side-by-side comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Original model plot\n",
    "ax1.scatter(y_test, original_predictions[best_original_model_name], alpha=0.3)\n",
    "ax1.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "ax1.set_xlabel('Actual Hours')\n",
    "ax1.set_ylabel('Predicted Hours')\n",
    "ax1.set_title(f'Best Original Model: {best_original_model_name}')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add metrics annotation\n",
    "orig_metrics = original_results[best_original_model_name]\n",
    "orig_metrics_text = f\"MAE: {orig_metrics['MAE']:.2f}\\nRMSE: {orig_metrics['RMSE']:.2f}\\nR²: {orig_metrics['R2']:.4f}\"\n",
    "ax1.annotate(\n",
    "    orig_metrics_text, \n",
    "    xy=(0.05, 0.95), \n",
    "    xycoords='axes fraction', \n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "# Tuned model plot\n",
    "ax2.scatter(y_test, tuned_predictions[best_tuned_model_name], alpha=0.3)\n",
    "ax2.plot([0, y_test.max()], [0, y_test.max()], 'r--')\n",
    "ax2.set_xlabel('Actual Hours')\n",
    "ax2.set_ylabel('Predicted Hours')\n",
    "ax2.set_title(f'Best Tuned Model: {best_tuned_model_name}')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add metrics annotation\n",
    "tuned_metrics = tuned_results[best_tuned_model_name]\n",
    "tuned_metrics_text = f\"MAE: {tuned_metrics['MAE']:.2f}\\nRMSE: {tuned_metrics['RMSE']:.2f}\\nR²: {tuned_metrics['R2']:.4f}\"\n",
    "ax2.annotate(\n",
    "    tuned_metrics_text, \n",
    "    xy=(0.05, 0.95), \n",
    "    xycoords='axes fraction', \n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "    va='top'\n",
    ")\n",
    "\n",
    "plt.suptitle('Best Models Comparison: Original vs Tuned', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.savefig(f'{comparison_dir}/best_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Generate summary report\n",
    "with open(f'{output_dir}/model_summary.txt', 'w') as f:\n",
    "    f.write(\"# Resolution Hours Ensemble Model with Hyperparameter Tuning\\n\\n\")\n",
    "    f.write(\"## Performance Comparison\\n\\n\")\n",
    "    \n",
    "    # Write performance table\n",
    "    f.write(\"| Model | Metric | Original | Tuned | Improvement (%) |\\n\")\n",
    "    f.write(\"|-------|--------|----------|-------|----------------|\\n\")\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        f.write(f\"| {row['Model']} | {row['Metric']} | {row['Original']:.4f} | {row['Tuned']:.4f} | {row['Improvement (%)']:.2f} |\\n\")\n",
    "    \n",
    "    # Write hyperparameter differences\n",
    "    f.write(\"\\n## Hyperparameter Changes\\n\\n\")\n",
    "    for model_name in best_params:\n",
    "        f.write(f\"### {model_name}\\n\\n\")\n",
    "        f.write(\"| Parameter | Original | Tuned |\\n\")\n",
    "        f.write(\"|-----------|----------|-------|\\n\")\n",
    "        \n",
    "        orig_params = original_params[model_name]\n",
    "        tuned_params = best_params[model_name]\n",
    "        \n",
    "        for param, tuned_value in tuned_params.items():\n",
    "            orig_value = orig_params.get(param, \"N/A\")\n",
    "            f.write(f\"| {param} | {orig_value} | {tuned_value} |\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    # Write conclusion\n",
    "    f.write(\"\\n## Conclusion\\n\\n\")\n",
    "    \n",
    "    # Calculate average improvement\n",
    "    mae_improvement = comparison_df[comparison_df['Metric'] == 'MAE']['Improvement (%)'].mean()\n",
    "    r2_improvement = comparison_df[comparison_df['Metric'] == 'R2']['Improvement (%)'].mean()\n",
    "    \n",
    "    f.write(f\"Hyperparameter tuning resulted in an average MAE improvement of {mae_improvement:.2f}% \")\n",
    "    f.write(f\"and an average R² improvement of {r2_improvement:.2f}%.\\n\\n\")\n",
    "    \n",
    "    if best_original_model_name == best_tuned_model_name:\n",
    "        f.write(f\"Both before and after tuning, {best_original_model_name} was the best performing model.\\n\")\n",
    "    else:\n",
    "        f.write(f\"Before tuning, {best_original_model_name} was the best performing model. \")\n",
    "        f.write(f\"After tuning, {best_tuned_model_name} became the best performing model.\\n\")\n",
    "\n",
    "print(f\"\\nModel training, tuning, and comparison complete. Results saved to {output_dir}/\")\n",
    "print(f\"Generated model comparison visualizations in {comparison_dir}/\")\n",
    "print(f\"See {output_dir}/model_summary.txt for a detailed report\")\n",
    "\n",
    "# If count_std__total_issues was kept, check its importance\n",
    "if 'count_std__total_issues' in final_features and hasattr(best_tuned_model, 'feature_importances_'):\n",
    "    count_feature_idx = final_features.index('count_std__total_issues')\n",
    "    count_feature_importance = importances[count_feature_idx]\n",
    "    count_feature_rank = list(indices).index(count_feature_idx) + 1\n",
    "    \n",
    "    print(f\"\\nImportance of 'count_std__total_issues':\")\n",
    "    print(f\"- Importance value: {count_feature_importance:.6f}\")\n",
    "    print(f\"- Rank among all features: {count_feature_rank} out of {len(final_features)}\")\n",
    "    print(f\"- Percentile: {100 - (count_feature_rank / len(final_features) * 100):.1f}%\")\n",
    "    \n",
    "    # Highlight this in the summary as well\n",
    "    with open(f'{output_dir}/model_summary.txt', 'a') as f:\n",
    "        f.write(\"\\n## Special Feature Analysis: count_std__total_issues\\n\\n\")\n",
    "        f.write(f\"Importance value: {count_feature_importance:.6f}\\n\")\n",
    "        f.write(f\"Rank among all features: {count_feature_rank} out of {len(final_features)}\\n\")\n",
    "        f.write(f\"Percentile: {100 - (count_feature_rank / len(final_features) * 100):.1f}%\\n\")\n",
    "        \n",
    "        if count_feature_rank <= 10:\n",
    "            f.write(\"\\nThis feature is among the top 10 most important features for the model.\\n\")\n",
    "        elif count_feature_rank <= len(final_features) // 4:\n",
    "            f.write(\"\\nThis feature is in the top 25% of all features by importance.\\n\")\n",
    "        else:\n",
    "            f.write(\"\\nThis feature has moderate importance in the model.\\n\")\n",
    "\n",
    "# Print final message about count_std__total_issues to guide further analysis\n",
    "if 'count_std__total_issues' in final_features:\n",
    "    print(\"\\nNOTE: The 'count_std__total_issues' feature was successfully included in the model.\")\n",
    "    print(\"Check the feature importance section in the summary report to see its impact.\")\n",
    "else:\n",
    "    print(\"\\nNOTE: The 'count_std__total_issues' feature was not found in the final feature list.\")\n",
    "    print(\"Possible reasons:\")\n",
    "    print(\"1. It doesn't exist in the original dataset\")\n",
    "    print(\"2. It was removed during preprocessing\")\n",
    "    print(\"3. It was highly correlated with another feature and removed during multicollinearity check\")\n",
    "    \n",
    "print(\"\\nAnalysis complete! The hypertuned model is ready for use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
