{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"../FeatureCleaning/jira_extracted_data/Hyperledger/10001_Sawtooth.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Configuration\n",
    "base_folder = \"../FeatureCleaning/jira_extracted_data\"  # Change this to your base folder containing all repositories\n",
    "output_file = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "\n",
    "# Collect all task data into a single dataframe\n",
    "all_tasks = []\n",
    "\n",
    "# Traverse through all repository folders\n",
    "for repo_folder in glob.glob(os.path.join(base_folder, \"*\")):\n",
    "    if os.path.isdir(repo_folder):\n",
    "        repo_name = os.path.basename(repo_folder)\n",
    "        \n",
    "        # Find all CSV files in this repository\n",
    "        csv_files = glob.glob(os.path.join(repo_folder, \"*.csv\"))\n",
    "        \n",
    "        print(f\"Processing repository: {repo_name} ({len(csv_files)} files)\")\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for csv_file in csv_files:\n",
    "            file_name = os.path.basename(csv_file)\n",
    "            print(f\"  Reading file: {file_name}\")\n",
    "            \n",
    "            # Read the CSV file\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Add repository and file information\n",
    "                df['repository'] = repo_name\n",
    "                df['source_file'] = file_name\n",
    "                \n",
    "                # Append to our collection\n",
    "                all_tasks.append(df)\n",
    "                print(f\"    Added {len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {file_name}: {e}\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if all_tasks:\n",
    "    combined_df = pd.concat(all_tasks, ignore_index=True)\n",
    "    \n",
    "    # Save the consolidated file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSuccessfully created consolidated task data file:\")\n",
    "    print(f\"- File: {output_file}\")\n",
    "    print(f\"- Total rows: {len(combined_df)}\")\n",
    "    print(f\"- Columns: {', '.join(combined_df.columns)}\")\n",
    "else:\n",
    "    print(\"No data found to consolidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_5584/3784868714.py:8: DtypeWarning:\n",
      "\n",
      "Columns (10,23,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "csv_file_path = \"./TaskLevel/consolidated_task_data.csv\"\n",
    "output_dir = \"./TaskLevel/analysis_output\"\n",
    "missing_threshold = 35  # Threshold for dropping columns (%)\n",
    "chunk_size = 50000  # Process in chunks to manage memory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def analyze_full_dataset(filepath, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Analyze the entire dataset in chunks to find missing values and data types\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        chunk_size: Size of chunks to process at once\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with missing value analysis and data type information\n",
    "    \"\"\"\n",
    "    print(f\"Starting analysis of {filepath}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    # First pass: get column names and count total rows\n",
    "    print(\"First pass: counting rows and getting column names...\")\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Get column names from the first row\n",
    "    columns = pd.read_csv(filepath, nrows=0).columns.tolist()\n",
    "    \n",
    "    # Count rows without loading entire file\n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Total columns: {len(columns)}\")\n",
    "    \n",
    "    # Initialize counters for missing values and data type analysis\n",
    "    missing_counts = {col: 0 for col in columns}\n",
    "    \n",
    "    # Data type detection structures\n",
    "    non_null_sample_values = defaultdict(list)\n",
    "    unique_values = defaultdict(set)\n",
    "    numeric_cols = set()\n",
    "    likely_date_cols = set()\n",
    "    likely_boolean_cols = set()\n",
    "    \n",
    "    # Second pass: analyze missing values and collect sample data for type detection\n",
    "    print(\"Second pass: analyzing missing values and data types...\")\n",
    "    chunk_count = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(filepath, chunksize=chunk_size):\n",
    "        chunk_count += 1\n",
    "        \n",
    "        # Update missing value counts\n",
    "        for col in columns:\n",
    "            if col in chunk.columns:\n",
    "                missing_counts[col] += chunk[col].isna().sum()\n",
    "        \n",
    "        # Collect data for type detection (from first few chunks only)\n",
    "        if chunk_count <= 5:  # Limit sample collection to first 5 chunks\n",
    "            for col in columns:\n",
    "                if col in chunk.columns:\n",
    "                    # Skip columns with all missing values in this chunk\n",
    "                    if chunk[col].isna().all():\n",
    "                        continue\n",
    "                    \n",
    "                    # Get non-null values for sampling\n",
    "                    non_null_vals = chunk[col].dropna()\n",
    "                    if len(non_null_vals) > 0:\n",
    "                        # Add a small sample to our collection\n",
    "                        sample = non_null_vals.sample(min(20, len(non_null_vals)))\n",
    "                        non_null_sample_values[col].extend(sample.tolist())\n",
    "                        \n",
    "                        # Track unique values (up to a limit)\n",
    "                        if len(unique_values[col]) < 1000:  # Limit unique value tracking\n",
    "                            unique_values[col].update(sample.tolist())\n",
    "                        \n",
    "                        # Check if column is numeric\n",
    "                        if pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "                            numeric_cols.add(col)\n",
    "                        \n",
    "                        # Check if column might be a date\n",
    "                        if col not in likely_date_cols and pd.api.types.is_object_dtype(chunk[col]):\n",
    "                            date_samples = pd.to_datetime(sample, errors='coerce')\n",
    "                            if date_samples.notna().sum() / len(date_samples) > 0.9:\n",
    "                                likely_date_cols.add(col)\n",
    "                        \n",
    "                        # Check if column might be boolean\n",
    "                        if col not in likely_boolean_cols:\n",
    "                            bool_values = [True, False, 0, 1, \"0\", \"1\", \"true\", \"false\", \"True\", \"False\"]\n",
    "                            if all(str(val).lower() in [str(bv).lower() for bv in bool_values] for val in sample):\n",
    "                                likely_boolean_cols.add(col)\n",
    "        \n",
    "        # Report progress\n",
    "        if chunk_count % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Processed {chunk_count} chunks ({chunk_count * chunk_size} rows) in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Calculate missing percentages\n",
    "    missing_percent = {col: (count / total_rows) * 100 for col, count in missing_counts.items()}\n",
    "    \n",
    "    # Determine data types\n",
    "    data_types = {}\n",
    "    for col in columns:\n",
    "        if col in likely_date_cols:\n",
    "            data_types[col] = 'datetime'\n",
    "        elif col in likely_boolean_cols:\n",
    "            data_types[col] = 'boolean'\n",
    "        elif col in numeric_cols:\n",
    "            data_types[col] = 'numeric'\n",
    "        elif col in unique_values and len(unique_values[col]) < 20:\n",
    "            data_types[col] = 'categorical'\n",
    "        elif col in unique_values and len(unique_values[col]) < 100:\n",
    "            data_types[col] = 'categorical_high_cardinality'\n",
    "        else:\n",
    "            data_types[col] = 'text'\n",
    "    \n",
    "    # Create DataFrame with missing value statistics\n",
    "    missing_df = pd.DataFrame({\n",
    "        'column': list(missing_counts.keys()),\n",
    "        'missing_count': list(missing_counts.values()),\n",
    "        'missing_percent': [missing_percent[col] for col in missing_counts.keys()],\n",
    "        'data_type': [data_types.get(col, 'unknown') for col in missing_counts.keys()]\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_df = missing_df.sort_values('missing_percent', ascending=False)\n",
    "    \n",
    "    # Add action based on threshold\n",
    "    missing_df['action'] = np.where(\n",
    "        missing_df['missing_percent'] > missing_threshold, \n",
    "        'drop', \n",
    "        'impute'\n",
    "    )\n",
    "    \n",
    "    # Add sample values\n",
    "    missing_df['sample_values'] = missing_df['column'].apply(\n",
    "        lambda col: str(non_null_sample_values.get(col, [])[:3])\n",
    "    )\n",
    "    \n",
    "    # Add unique value counts\n",
    "    missing_df['unique_count'] = missing_df['column'].apply(\n",
    "        lambda col: len(unique_values.get(col, set()))\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Analysis completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    return missing_df, total_rows\n",
    "\n",
    "def determine_imputation_strategies(missing_df):\n",
    "    \"\"\"\n",
    "    Determine appropriate imputation strategies based on data types and missing percentages\n",
    "    \n",
    "    Args:\n",
    "        missing_df: DataFrame with missing value analysis\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with recommended imputation strategies\n",
    "    \"\"\"\n",
    "    # Add imputation strategy column\n",
    "    missing_df['imputation_strategy'] = 'none'\n",
    "    \n",
    "    # Strategies based on data type\n",
    "    for idx, row in missing_df.iterrows():\n",
    "        col = row['column']\n",
    "        dtype = row['data_type']\n",
    "        missing_pct = row['missing_percent']\n",
    "        \n",
    "        if row['action'] == 'drop':\n",
    "            missing_df.loc[idx, 'imputation_strategy'] = 'drop_column'\n",
    "            continue\n",
    "        \n",
    "        # Low missing values (<5%)\n",
    "        if missing_pct < 5:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median'\n",
    "            elif dtype in ['categorical', 'categorical_high_cardinality', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "                \n",
    "        # Medium missing values (5-15%)\n",
    "        elif missing_pct < 15:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median_by_group'\n",
    "            elif dtype in ['categorical', 'categorical_high_cardinality', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode_by_group'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "                \n",
    "        # High missing values (15-35%)\n",
    "        else:\n",
    "            if dtype == 'numeric':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'median_by_group'\n",
    "            elif dtype in ['categorical', 'boolean']:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'new_category'\n",
    "            elif dtype == 'categorical_high_cardinality':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'mode_by_group'\n",
    "            elif dtype == 'datetime':\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "            else:\n",
    "                missing_df.loc[idx, 'imputation_strategy'] = 'drop_rows'\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "def generate_imputation_code(missing_df):\n",
    "    \"\"\"\n",
    "    Generate Python code for implementing the recommended imputation strategies\n",
    "    \n",
    "    Args:\n",
    "        missing_df: DataFrame with missing value analysis and strategies\n",
    "        \n",
    "    Returns:\n",
    "        String containing Python code for imputation\n",
    "    \"\"\"\n",
    "    # Group columns by imputation strategy\n",
    "    strategy_groups = {}\n",
    "    for _, row in missing_df.iterrows():\n",
    "        strategy = row['imputation_strategy']\n",
    "        if strategy not in strategy_groups:\n",
    "            strategy_groups[strategy] = []\n",
    "        \n",
    "        strategy_groups[strategy].append((row['column'], row['data_type']))\n",
    "    \n",
    "    # Generate code\n",
    "    code_lines = [\n",
    "        \"import pandas as pd\",\n",
    "        \"import numpy as np\",\n",
    "        \"from sklearn.impute import SimpleImputer\",\n",
    "        \"\",\n",
    "        \"# Function to impute missing values based on recommended strategies\",\n",
    "        \"def impute_missing_values(df, grouping_cols=None):\",\n",
    "        \"    \\\"\\\"\\\"\",\n",
    "        \"    Impute missing values using recommended strategies\",\n",
    "        \"    \",\n",
    "        \"    Args:\",\n",
    "        \"        df: DataFrame to process\",\n",
    "        \"        grouping_cols: Columns to group by for group-based imputation\",\n",
    "        \"    \",\n",
    "        \"    Returns:\",\n",
    "        \"        DataFrame with imputed values\",\n",
    "        \"    \\\"\\\"\\\"\",\n",
    "        \"    # Make a copy to avoid modifying the original\",\n",
    "        \"    imputed_df = df.copy()\",\n",
    "        \"\",\n",
    "        \"    # Use default grouping columns if none provided\",\n",
    "        \"    if grouping_cols is None:\",\n",
    "        \"        # Check if these columns exist in the dataframe\",\n",
    "        \"        possible_groups = ['fields.issuetype.name', 'fields.priority.name', 'fields.project.key']\",\n",
    "        \"        grouping_cols = [col for col in possible_groups if col in df.columns]\",\n",
    "        \"\",\n",
    "        \"    # If no grouping columns are available, use median/mode without grouping\",\n",
    "        \"    has_groups = len(grouping_cols) > 0\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    # Add drop column code if needed\n",
    "    if 'drop_column' in strategy_groups and strategy_groups['drop_column']:\n",
    "        cols_to_drop = [col for col, _ in strategy_groups['drop_column']]\n",
    "        code_lines.extend([\n",
    "            \"    # 1. Drop columns with too many missing values\",\n",
    "            f\"    cols_to_drop = {cols_to_drop}\",\n",
    "            \"    print(f\\\"Dropping {len(cols_to_drop)} columns with >35% missing values\\\")\",\n",
    "            \"    imputed_df = imputed_df.drop(columns=[col for col in cols_to_drop if col in imputed_df.columns])\",\n",
    "            \"\",\n",
    "        ])\n",
    "    \n",
    "    # Add median imputation code if needed\n",
    "    if 'median' in strategy_groups and strategy_groups['median']:\n",
    "        numeric_cols = [col for col, dtype in strategy_groups['median'] if dtype == 'numeric']\n",
    "        if numeric_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 2. Simple median imputation for numeric columns\",\n",
    "                f\"    median_cols = {numeric_cols}\",\n",
    "                \"    existing_median_cols = [col for col in median_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_median_cols:\",\n",
    "                \"        print(f\\\"Applying median imputation to {len(existing_median_cols)} columns\\\")\",\n",
    "                \"        imputer = SimpleImputer(strategy='median')\",\n",
    "                \"        imputed_df[existing_median_cols] = imputer.fit_transform(imputed_df[existing_median_cols])\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add mode imputation code if needed\n",
    "    if 'mode' in strategy_groups and strategy_groups['mode']:\n",
    "        categorical_cols = [col for col, dtype in strategy_groups['mode'] \n",
    "                          if dtype in ['categorical', 'categorical_high_cardinality', 'boolean']]\n",
    "        if categorical_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 3. Simple mode imputation for categorical columns\",\n",
    "                f\"    mode_cols = {categorical_cols}\",\n",
    "                \"    existing_mode_cols = [col for col in mode_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_mode_cols:\",\n",
    "                \"        print(f\\\"Applying mode imputation to {len(existing_mode_cols)} columns\\\")\",\n",
    "                \"        for col in existing_mode_cols:\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add grouped median imputation code if needed\n",
    "    if 'median_by_group' in strategy_groups and strategy_groups['median_by_group']:\n",
    "        grouped_numeric_cols = [col for col, dtype in strategy_groups['median_by_group'] if dtype == 'numeric']\n",
    "        if grouped_numeric_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 4. Grouped median imputation for numeric columns\",\n",
    "                f\"    grouped_median_cols = {grouped_numeric_cols}\",\n",
    "                \"    existing_grouped_median_cols = [col for col in grouped_median_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_grouped_median_cols and has_groups:\",\n",
    "                \"        print(f\\\"Applying grouped median imputation to {len(existing_grouped_median_cols)} columns\\\")\",\n",
    "                \"        for col in existing_grouped_median_cols:\",\n",
    "                \"            # Calculate medians by group\",\n",
    "                \"            group_medians = imputed_df.groupby(grouping_cols)[col].median()\",\n",
    "                \"            # For each combination of grouping values, fill with the group median\",\n",
    "                \"            for group_values, median_value in group_medians.items():\",\n",
    "                \"                if not isinstance(group_values, tuple):\",\n",
    "                \"                    group_values = (group_values,)\",\n",
    "                \"                if pd.notna(median_value):\",\n",
    "                \"                    # Create a mask for this group\",\n",
    "                \"                    mask = pd.Series(True, index=imputed_df.index)\",\n",
    "                \"                    for i, group_col in enumerate(grouping_cols):\",\n",
    "                \"                        mask = mask & (imputed_df[group_col] == group_values[i])\",\n",
    "                \"                    # Apply the group median to missing values in this group\",\n",
    "                \"                    mask = mask & imputed_df[col].isna()\",\n",
    "                \"                    imputed_df.loc[mask, col] = median_value\",\n",
    "                \"            # For any remaining NaNs, use overall median\",\n",
    "                \"            overall_median = imputed_df[col].median()\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(overall_median)\",\n",
    "                \"    elif existing_grouped_median_cols:\",\n",
    "                \"        # Fall back to simple median if no grouping columns\",\n",
    "                \"        imputer = SimpleImputer(strategy='median')\",\n",
    "                \"        imputed_df[existing_grouped_median_cols] = imputer.fit_transform(imputed_df[existing_grouped_median_cols])\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add grouped mode imputation code if needed\n",
    "    if 'mode_by_group' in strategy_groups and strategy_groups['mode_by_group']:\n",
    "        grouped_cat_cols = [col for col, dtype in strategy_groups['mode_by_group'] \n",
    "                           if dtype in ['categorical', 'categorical_high_cardinality', 'boolean']]\n",
    "        if grouped_cat_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 5. Grouped mode imputation for categorical columns\",\n",
    "                f\"    grouped_mode_cols = {grouped_cat_cols}\",\n",
    "                \"    existing_grouped_mode_cols = [col for col in grouped_mode_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_grouped_mode_cols and has_groups:\",\n",
    "                \"        print(f\\\"Applying grouped mode imputation to {len(existing_grouped_mode_cols)} columns\\\")\",\n",
    "                \"        for col in existing_grouped_mode_cols:\",\n",
    "                \"            # Calculate modes by group\",\n",
    "                \"            for group_values, group_df in imputed_df.groupby(grouping_cols):\",\n",
    "                \"                if not isinstance(group_values, tuple):\",\n",
    "                \"                    group_values = (group_values,)\",\n",
    "                \"                # Get mode for this group\",\n",
    "                \"                mode_series = group_df[col].mode()\",\n",
    "                \"                if not mode_series.empty:\",\n",
    "                \"                    mode_value = mode_series[0]\",\n",
    "                \"                    # Create a mask for this group\",\n",
    "                \"                    mask = pd.Series(True, index=imputed_df.index)\",\n",
    "                \"                    for i, group_col in enumerate(grouping_cols):\",\n",
    "                \"                        mask = mask & (imputed_df[group_col] == group_values[i])\",\n",
    "                \"                    # Apply the group mode to missing values in this group\",\n",
    "                \"                    mask = mask & imputed_df[col].isna()\",\n",
    "                \"                    imputed_df.loc[mask, col] = mode_value\",\n",
    "                \"            # For any remaining NaNs, use overall mode\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"    elif existing_grouped_mode_cols:\",\n",
    "                \"        # Fall back to simple mode if no grouping columns\",\n",
    "                \"        for col in existing_grouped_mode_cols:\",\n",
    "                \"            mode_val = imputed_df[col].mode()[0] if not imputed_df[col].mode().empty else None\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna(mode_val)\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add new category imputation code if needed\n",
    "    if 'new_category' in strategy_groups and strategy_groups['new_category']:\n",
    "        new_cat_cols = [col for col, dtype in strategy_groups['new_category'] \n",
    "                       if dtype in ['categorical', 'boolean']]\n",
    "        if new_cat_cols:\n",
    "            code_lines.extend([\n",
    "                \"    # 6. New category imputation for categorical columns\",\n",
    "                f\"    new_category_cols = {new_cat_cols}\",\n",
    "                \"    existing_new_cat_cols = [col for col in new_category_cols if col in imputed_df.columns]\",\n",
    "                \"    if existing_new_cat_cols:\",\n",
    "                \"        print(f\\\"Applying new category imputation to {len(existing_new_cat_cols)} columns\\\")\",\n",
    "                \"        for col in existing_new_cat_cols:\",\n",
    "                \"            # Fill missing with a new category 'Unknown'\",\n",
    "                \"            imputed_df[col] = imputed_df[col].fillna('Unknown')\",\n",
    "                \"\",\n",
    "            ])\n",
    "    \n",
    "    # Add code to drop rows with remaining NaNs in key columns\n",
    "    code_lines.extend([\n",
    "        \"    # 7. Finally, drop rows with remaining NaNs in essential columns\",\n",
    "        \"    essential_columns = ['fields.issuetype.name', 'fields.created', 'key']\",\n",
    "        \"    existing_essential = [col for col in essential_columns if col in imputed_df.columns]\",\n",
    "        \"    if existing_essential:\",\n",
    "        \"        before_rows = len(imputed_df)\",\n",
    "        \"        imputed_df = imputed_df.dropna(subset=existing_essential)\",\n",
    "        \"        dropped_rows = before_rows - len(imputed_df)\",\n",
    "        \"        print(f\\\"Dropped {dropped_rows} rows with missing values in essential columns\\\")\",\n",
    "        \"\",\n",
    "        \"    return imputed_df\",\n",
    "        \"\",\n",
    "        \"# Example usage:\",\n",
    "        \"# df = pd.read_csv('your_file.csv')\",\n",
    "        \"# imputed_df = impute_missing_values(df)\",\n",
    "        \"# imputed_df.to_csv('imputed_data.csv', index=False)\",\n",
    "    ])\n",
    "    \n",
    "    return \"\\n\".join(code_lines)\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"Error: File not found at {csv_file_path}\")\n",
    "    else:\n",
    "        # Analyze full dataset\n",
    "        missing_analysis, total_rows = analyze_full_dataset(csv_file_path, chunk_size)\n",
    "        \n",
    "        # Determine imputation strategies\n",
    "        missing_analysis = determine_imputation_strategies(missing_analysis)\n",
    "        \n",
    "        # Save missing value analysis\n",
    "        analysis_output = os.path.join(output_dir, 'missing_value_analysis.csv')\n",
    "        missing_analysis.to_csv(analysis_output, index=False)\n",
    "        print(f\"Missing value analysis saved to {analysis_output}\")\n",
    "        \n",
    "        # Generate imputation code\n",
    "        imputation_code = generate_imputation_code(missing_analysis)\n",
    "        code_output = os.path.join(output_dir, 'imputation_code.py')\n",
    "        with open(code_output, 'w') as f:\n",
    "            f.write(imputation_code)\n",
    "        print(f\"Imputation code generated and saved to {code_output}\")\n",
    "        \n",
    "        # Create summary visualizations\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create histogram of missing value percentages\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.histplot(missing_analysis['missing_percent'], bins=20)\n",
    "        plt.axvline(x=missing_threshold, color='red', linestyle='--', \n",
    "                    label=f'Drop threshold ({missing_threshold}%)')\n",
    "        plt.title('Distribution of Missing Value Percentages')\n",
    "        plt.xlabel('Missing Percentage')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Create bar chart of action counts\n",
    "        plt.subplot(2, 1, 2)\n",
    "        action_counts = missing_analysis['action'].value_counts()\n",
    "        sns.barplot(x=action_counts.index, y=action_counts.values)\n",
    "        plt.title('Recommended Actions for Columns')\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Number of Columns')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'missing_value_summary.png'))\n",
    "        \n",
    "        # Create pie chart of data types\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        type_counts = missing_analysis['data_type'].value_counts()\n",
    "        plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%')\n",
    "        plt.title('Column Data Types')\n",
    "        plt.savefig(os.path.join(output_dir, 'data_type_distribution.png'))\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\n=== MISSING VALUE ANALYSIS SUMMARY ===\")\n",
    "        print(f\"Total rows in dataset: {total_rows}\")\n",
    "        print(f\"Total columns: {len(missing_analysis)}\")\n",
    "        print(f\"Columns to drop (>{missing_threshold}% missing): {missing_analysis['action'].value_counts()['drop']}\")\n",
    "        print(f\"Columns to impute: {missing_analysis['action'].value_counts()['impute']}\")\n",
    "        \n",
    "        # Print data type distribution\n",
    "        print(\"\\nData type distribution:\")\n",
    "        for dtype, count in type_counts.items():\n",
    "            print(f\"  - {dtype}: {count} columns\")\n",
    "        \n",
    "        # Print top 10 columns with highest missing percentages\n",
    "        print(\"\\nTop 10 columns with highest missing percentages:\")\n",
    "        for _, row in missing_analysis.head(10).iterrows():\n",
    "            print(f\"  - {row['column']}: {row['missing_percent']:.2f}% missing ({row['data_type']})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data analysis from ./TaskLevel/data_analysis_report.json\n",
      "Reading all rows from ./TaskLevel/consolidated_task_data_filtered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_8203/2761679221.py:57: FutureWarning:\n",
      "\n",
      "use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2259837 rows\n",
      "Dropping columns with high missing rates or URL data\n",
      "Dropped 9 columns\n",
      "Converting date columns to datetime...\n",
      "Creating features for effort estimation...\n",
      "Creating temporal features...\n",
      "Calculating resolution hours...\n",
      "Creating issue type encodings...\n",
      "Creating priority encodings...\n",
      "Performing imputation for missing values...\n",
      "Imputing resolution hours for 319374 unresolved issues\n",
      "Saving processed dataset with 55 columns to ./TaskLevel/processed_task_data.csv\n",
      "\n",
      "Created 19 new features:\n",
      "- created_day_of_week: 2259548 non-null values (100.0%)\n",
      "- created_hour: 2259548 non-null values (100.0%)\n",
      "- created_is_weekend: 2259837 non-null values (100.0%)\n",
      "- created_month: 2259548 non-null values (100.0%)\n",
      "- created_year: 2259548 non-null values (100.0%)\n",
      "- is_priority_blocker: 2259837 non-null values (100.0%)\n",
      "- is_priority_critical: 2259837 non-null values (100.0%)\n",
      "- is_priority_major: 2259837 non-null values (100.0%)\n",
      "- is_priority_minor: 2259837 non-null values (100.0%)\n",
      "- is_priority_trivial: 2259837 non-null values (100.0%)\n",
      "- is_type_bug: 2259837 non-null values (100.0%)\n",
      "- is_type_epic: 2259837 non-null values (100.0%)\n",
      "- is_type_improvement: 2259837 non-null values (100.0%)\n",
      "- is_type_new_feature: 2259837 non-null values (100.0%)\n",
      "- is_type_story: 2259837 non-null values (100.0%)\n",
      "- is_type_sub-task: 2259837 non-null values (100.0%)\n",
      "- is_type_task: 2259837 non-null values (100.0%)\n",
      "- log_resolution_hours: 2259837 non-null values (100.0%)\n",
      "- resolution_hours: 2259548 non-null values (100.0%)\n",
      "\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "input_file = \"./TaskLevel/consolidated_task_data_filtered.csv\"\n",
    "output_file = \"./TaskLevel/processed_task_data.csv\"\n",
    "analysis_file = \"./TaskLevel/data_analysis_report.json\"  # Adjusted path\n",
    "sample_size = None  # Set this to None to process all rows\n",
    "\n",
    "# Load analysis results with error handling\n",
    "try:\n",
    "    with open(analysis_file, 'r') as f:\n",
    "        analysis = json.load(f)\n",
    "    print(f\"Loaded data analysis from {analysis_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Analysis file {analysis_file} not found. Using default data types.\")\n",
    "    analysis = {'columns': {}, 'missing_value_columns': []}\n",
    "\n",
    "# Define data types (with fallback if analysis file wasn't found)\n",
    "dtype_map = {}\n",
    "if analysis['columns']:\n",
    "    for col, info in analysis['columns'].items():\n",
    "        if info['dtype'] == 'categorical' or info['dtype'] == 'text':\n",
    "            dtype_map[col] = 'string'\n",
    "        elif info['dtype'] == 'float64' or info['dtype'] == 'int64':\n",
    "            dtype_map[col] = 'float64'  # Use float64 for all numeric to handle NaN\n",
    "else:\n",
    "    # Fallback type mapping for critical columns\n",
    "    critical_columns = {\n",
    "        'fields.issuetype.name': 'string',\n",
    "        'fields.priority.name': 'string',\n",
    "        'fields.project.key': 'string',\n",
    "        'fields.status.name': 'string',\n",
    "        'fields.creator.name': 'string',\n",
    "        'is_completed': 'float64',\n",
    "        'is_resolved': 'float64',\n",
    "        'type_task': 'float64',\n",
    "        'type_bug': 'float64',\n",
    "        'resolution_time_days': 'float64',\n",
    "        'age_days': 'float64',\n",
    "        'type_sub_task': 'float64'\n",
    "    }\n",
    "    dtype_map.update(critical_columns)\n",
    "\n",
    "# Define columns to drop\n",
    "columns_to_drop = []\n",
    "if 'missing_value_columns' in analysis:\n",
    "    columns_to_drop = [col['column'] for col in analysis['missing_value_columns'] \n",
    "                      if col['missing_percentage'] > 35]\n",
    "columns_to_drop.extend(['fields.creator.avatarUrls.48x48', 'fields.creator.avatarUrls.24x24', \n",
    "                        'fields.creator.avatarUrls.16x16', 'fields.creator.avatarUrls.32x32', \n",
    "                        'fields.creator.self'])\n",
    "\n",
    "# Set pandas options for safer processing\n",
    "pd.set_option('mode.use_inf_as_na', True)\n",
    "\n",
    "try:\n",
    "    # Read data with appropriate types and error handling\n",
    "    if sample_size:\n",
    "        print(f\"Reading {sample_size} rows from {input_file}\")\n",
    "        df = pd.read_csv(input_file, dtype=dtype_map, nrows=sample_size, low_memory=False)\n",
    "    else:\n",
    "        print(f\"Reading all rows from {input_file}\")\n",
    "        df = pd.read_csv(input_file, dtype=dtype_map, low_memory=False)\n",
    "\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "    # Drop high-missing columns\n",
    "    print(f\"Dropping columns with high missing rates or URL data\")\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    if columns_to_drop:\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "        print(f\"Dropped {len(columns_to_drop)} columns\")\n",
    "\n",
    "    # Explicitly convert date columns\n",
    "    date_columns = ['fields.created', 'fields.updated', 'fields.resolutiondate']\n",
    "    print(\"Converting date columns to datetime...\")\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    print(\"Creating features for effort estimation...\")\n",
    "\n",
    "    # Process each feature carefully with try/except blocks\n",
    "    try:\n",
    "        # 1. Process datetime fields for temporal features\n",
    "        if 'fields.created' in df.columns:\n",
    "            print(\"Creating temporal features...\")\n",
    "            df['created_day_of_week'] = df['fields.created'].dt.dayofweek\n",
    "            df['created_is_weekend'] = (df['fields.created'].dt.dayofweek >= 5).astype(float)\n",
    "            df['created_hour'] = df['fields.created'].dt.hour\n",
    "            df['created_month'] = df['fields.created'].dt.month\n",
    "            df['created_year'] = df['fields.created'].dt.year\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating temporal features: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        # 2. Calculate resolution hours\n",
    "        if all(col in df.columns for col in ['fields.created', 'fields.resolutiondate']):\n",
    "            print(\"Calculating resolution hours...\")\n",
    "            df['resolution_hours'] = np.nan\n",
    "            mask = df['fields.resolutiondate'].notna()\n",
    "            if mask.any():\n",
    "                df.loc[mask, 'resolution_hours'] = (\n",
    "                    (df.loc[mask, 'fields.resolutiondate'] - df.loc[mask, 'fields.created']).dt.total_seconds() / 3600\n",
    "                )\n",
    "            df['log_resolution_hours'] = np.log1p(df['resolution_hours'].fillna(0).clip(lower=0))\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating resolution hours: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        # 3. Create one-hot encodings\n",
    "        if 'fields.issuetype.name' in df.columns:\n",
    "            print(\"Creating issue type encodings...\")\n",
    "            for issue_type in ['Bug', 'Task', 'Story', 'Improvement', 'New Feature', 'Epic', 'Sub-task']:\n",
    "                col_name = f'is_type_{issue_type.lower().replace(\" \", \"_\")}'\n",
    "                df[col_name] = (df['fields.issuetype.name'].str.lower() == issue_type.lower()).astype(float)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating issue type encodings: {str(e)}\")\n",
    "\n",
    "    try:\n",
    "        if 'fields.priority.name' in df.columns:\n",
    "            print(\"Creating priority encodings...\")\n",
    "            for priority in ['Blocker', 'Critical', 'Major', 'Minor', 'Trivial']:\n",
    "                col_name = f'is_priority_{priority.lower()}'\n",
    "                df[col_name] = df['fields.priority.name'].str.lower().str.contains(priority.lower(), na=False).astype(float)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating priority encodings: {str(e)}\")\n",
    "\n",
    "    # Imputation section with more robust error handling\n",
    "    print(\"Performing imputation for missing values...\")\n",
    "\n",
    "    # Calculate age in days for all issues (needed for imputation)\n",
    "    try:\n",
    "        if 'fields.created' in df.columns and 'age_days' not in df.columns:\n",
    "            print(\"Calculating age_days...\")\n",
    "            current_time = pd.Timestamp.now()\n",
    "            if 'fields.created' in df.columns and len(df) > 0 and df['fields.created'].iloc[0] is not None:\n",
    "                if hasattr(df['fields.created'].iloc[0], 'tzinfo') and df['fields.created'].iloc[0].tzinfo is not None:\n",
    "                    current_time = current_time.tz_localize('UTC')\n",
    "            df['age_days'] = (current_time - df['fields.created']).dt.total_seconds() / (24 * 3600)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating age_days: {str(e)}\")\n",
    "        df['age_days'] = 30.0  # Fallback value\n",
    "\n",
    "    # For unresolved issues, impute resolution hours\n",
    "    try:\n",
    "        unresolved_mask = df['fields.resolutiondate'].isna()\n",
    "        if unresolved_mask.any():\n",
    "            print(f\"Imputing resolution hours for {unresolved_mask.sum()} unresolved issues\")\n",
    "            \n",
    "            # First get global_median as fallback\n",
    "            resolved_mask = ~unresolved_mask\n",
    "            if resolved_mask.any() and 'resolution_hours' in df.columns:\n",
    "                global_median = df.loc[resolved_mask, 'resolution_hours'].median()\n",
    "                if pd.isna(global_median):  # If still no valid median\n",
    "                    global_median = 24.0  # Default to 24 hours\n",
    "            else:\n",
    "                global_median = 24.0  # Default value\n",
    "                \n",
    "            # Try group-based imputation\n",
    "            try:\n",
    "                if all(col in df.columns for col in ['fields.issuetype.name', 'fields.priority.name']):\n",
    "                    # Group by issue type and priority\n",
    "                    for name, group in df[resolved_mask].groupby(['fields.issuetype.name', 'fields.priority.name']):\n",
    "                        if len(group) > 0 and 'resolution_hours' in group.columns:\n",
    "                            median_hours = group['resolution_hours'].median()\n",
    "                            if pd.isna(median_hours):\n",
    "                                median_hours = global_median\n",
    "                                \n",
    "                            # Create safe mask for this group\n",
    "                            if isinstance(name, tuple) and len(name) == 2:\n",
    "                                issue_type, priority = name\n",
    "                                type_mask = (df['fields.issuetype.name'] == issue_type)\n",
    "                                prio_mask = (df['fields.priority.name'] == priority)\n",
    "                                group_mask = type_mask & prio_mask & unresolved_mask\n",
    "                                \n",
    "                                if group_mask.any():\n",
    "                                    # Apply age-based adjustment\n",
    "                                    age_factor = 1.0 + 0.1 * (df.loc[group_mask, 'age_days'] / 30.0).clip(0, 10.0)\n",
    "                                    df.loc[group_mask, 'resolution_hours'] = median_hours * age_factor\n",
    "            except Exception as e:\n",
    "                print(f\"Error in group-based imputation: {str(e)}\")\n",
    "                \n",
    "            # For any remaining missing values, use global median\n",
    "            missing_hours = df['resolution_hours'].isna() & unresolved_mask\n",
    "            if missing_hours.any():\n",
    "                age_factor = 1.0 + 0.1 * (df.loc[missing_hours, 'age_days'] / 30.0).clip(0, 10.0)\n",
    "                df.loc[missing_hours, 'resolution_hours'] = global_median * age_factor\n",
    "                \n",
    "            # Update log transform\n",
    "            if 'log_resolution_hours' in df.columns:\n",
    "                imputed_mask = unresolved_mask & df['resolution_hours'].notna() & (df['resolution_hours'] > 0)\n",
    "                if imputed_mask.any():\n",
    "                    df.loc[imputed_mask, 'log_resolution_hours'] = np.log1p(df.loc[imputed_mask, 'resolution_hours'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error imputing unresolved issues: {str(e)}\")\n",
    "\n",
    "    # Save processed dataset\n",
    "    print(f\"Saving processed dataset with {len(df.columns)} columns to {output_file}\")\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print summary of new features\n",
    "    try:\n",
    "        original_columns = set(pd.read_csv(input_file, nrows=0).columns)\n",
    "        new_columns = [col for col in df.columns if col not in original_columns]\n",
    "        \n",
    "        print(f\"\\nCreated {len(new_columns)} new features:\")\n",
    "        for col in sorted(new_columns):\n",
    "            non_null = df[col].notna().sum()\n",
    "            print(f\"- {col}: {non_null} non-null values ({non_null/len(df)*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating feature summary: {str(e)}\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Critical error in processing: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_8203/1714735836.py:8: DtypeWarning:\n",
      "\n",
      "Columns (10,18,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./TaskLevel/processed_task_data.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lt/2k0m0j2d6xqg0wz_673wgqt00000gn/T/ipykernel_8203/2200856607.py:5: DtypeWarning:\n",
      "\n",
      "Columns (10,18,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported first 20,000 records to ./TaskLevel/processed_task_data_first_20k.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 14:37:07,941 - INFO     - Executing shutdown due to inactivity...\n",
      "2025-04-08 14:37:08,379 - INFO     - Executing shutdown...\n",
      "2025-04-08 14:37:08,382 - INFO     - Not running with the Werkzeug Server, exiting by searching gc for BaseWSGIServer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "csv_file_path = \"./TaskLevel/processed_task_data.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the first 20,000 records\n",
    "df_subset = df.head(10000)\n",
    "\n",
    "# Export to a new CSV file\n",
    "output_path = \"./TaskLevel/processed_task_data_first_20k.csv\"\n",
    "df_subset.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Successfully exported first 20,000 records to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4178295320.py, line 107)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 107\u001b[0;36m\u001b[0m\n\u001b[0;31m    df_cleaned.to_csv('cleaned_jira_dataset.csv', iimport pandas as pd\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def preprocess_jira_dataset(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print initial information\n",
    "    print(\"Initial Dataset Information:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Columns to drop based on your specifications\n",
    "    columns_to_drop = [\n",
    "        'fields.issuelinks',\n",
    "        'fields.resolutiondate', \n",
    "        'status', \n",
    "        'priority_name', \n",
    "        'issue_type', \n",
    "        'resolution_time_days', \n",
    "        'created_day_of_week', \n",
    "        'source_file',\n",
    "        '_id',  # Adding this as it's likely a string column\n",
    "        'key',  # Adding this as it's likely a string column\n",
    "        'fields.creator.name',\n",
    "        'fields.creator.key',\n",
    "        'fields.creator.displayName',\n",
    "        'fields.creator.timeZone',\n",
    "        'repository'\n",
    "    ]\n",
    "    \n",
    "    # Drop specified columns\n",
    "    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Convert boolean columns to numeric\n",
    "    bool_columns = df_cleaned.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].astype(int)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    # For categorical columns, fill NaNs with a placeholder\n",
    "    categorical_columns = [\n",
    "        'type_sub_task', \n",
    "        'is_type_bug', \n",
    "        'is_type_task', \n",
    "        'is_type_story', \n",
    "        'is_type_improvement', \n",
    "        'is_type_new_feature', \n",
    "        'is_type_epic', \n",
    "        'is_type_sub-task',\n",
    "        'is_priority_blocker',\n",
    "        'is_priority_critical', \n",
    "        'is_priority_major', \n",
    "        'is_priority_minor', \n",
    "        'is_priority_trivial'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(0)  # Fill with 0 for categorical columns\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Numerical columns imputation\n",
    "    numerical_columns_to_impute = [\n",
    "        col for col in numerical_columns if 'id' not in col.lower() and \n",
    "        col in ['resolution_hours', 'log_resolution_hours', 'age_days']\n",
    "    ]\n",
    "    \n",
    "    # Impute numerical columns with median\n",
    "    for col in numerical_columns_to_impute:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "    \n",
    "    # Detailed Analysis of Missing Values\n",
    "    print(\"\\nMissing Values Analysis:\")\n",
    "    missing_percentages = df_cleaned.isnull().mean() * 100\n",
    "    missing_columns = missing_percentages[missing_percentages > 0]\n",
    "    print(missing_columns)\n",
    "    \n",
    "    # Correlation Analysis with only numeric columns\n",
    "    numeric_df = df_cleaned.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Visualize Missing Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(numeric_df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_values_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Correlation Analysis\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f', square=True)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Basic Statistical Summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(numeric_df.describe())\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df_cleaned.to_csv('cleaned_jira_dataset.csv', index=False)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = './TaskLevel/processed_task_data.csv'\n",
    "    cleaned_df = preprocess_jira_dataset(file_path)\n",
    "    print(\"\\nCleaning Complete. Cleaned dataset saved as 'cleaned_jira_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def preprocess_jira_dataset(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print initial information\n",
    "    print(\"Initial Dataset Information:\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Columns to drop based on your specifications\n",
    "    columns_to_drop = [\n",
    "        'fields.issuelinks',\n",
    "        'fields.resolutiondate', \n",
    "        'status', \n",
    "        'priority_name', \n",
    "        'issue_type', \n",
    "        'resolution_time_days', \n",
    "        'created_day_of_week', \n",
    "        'source_file',\n",
    "        '_id',  # Adding this as it's likely a string column\n",
    "        'key',  # Adding this as it's likely a string column\n",
    "        'fields.creator.name',\n",
    "        'fields.creator.key',\n",
    "        'fields.creator.displayName',\n",
    "        'fields.creator.timeZone',\n",
    "        'repository'\n",
    "    ]\n",
    "    \n",
    "    # Drop specified columns\n",
    "    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Convert boolean columns to numeric\n",
    "    bool_columns = df_cleaned.select_dtypes(include=['bool']).columns\n",
    "    for col in bool_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].astype(int)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    # For categorical columns, fill NaNs with a placeholder\n",
    "    categorical_columns = [\n",
    "        'type_sub_task', \n",
    "        'is_type_bug', \n",
    "        'is_type_task', \n",
    "        'is_type_story', \n",
    "        'is_type_improvement', \n",
    "        'is_type_new_feature', \n",
    "        'is_type_epic', \n",
    "        'is_type_sub-task',\n",
    "        'is_priority_blocker',\n",
    "        'is_priority_critical', \n",
    "        'is_priority_major', \n",
    "        'is_priority_minor', \n",
    "        'is_priority_trivial'\n",
    "    ]\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(0)  # Fill with 0 for categorical columns\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_columns = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Numerical columns imputation\n",
    "    # Specific columns we want to impute\n",
    "    specific_impute_columns = ['resolution_hours', 'log_resolution_hours', 'age_days']\n",
    "    \n",
    "    # Numerical columns to impute (excluding ID columns)\n",
    "    numerical_columns_to_impute = [\n",
    "        col for col in numeric_columns \n",
    "        if col in specific_impute_columns and 'id' not in col.lower()\n",
    "    ]\n",
    "    \n",
    "    # Impute numerical columns with median\n",
    "    for col in numerical_columns_to_impute:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "    \n",
    "    # Detailed Analysis of Missing Values\n",
    "    print(\"\\nMissing Values Analysis:\")\n",
    "    missing_percentages = df_cleaned.isnull().mean() * 100\n",
    "    missing_columns = missing_percentages[missing_percentages > 0]\n",
    "    print(missing_columns)\n",
    "    \n",
    "    # Update numeric columns after imputation\n",
    "    numeric_df = df_cleaned.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Visualize Missing Values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(numeric_df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Values Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('missing_values_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Correlation Analysis\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    correlation_matrix = numeric_df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f', square=True)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Basic Statistical Summary\n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    print(numeric_df.describe())\n",
    "    \n",
    "    # Save the cleaned dataset\n",
    "    df_cleaned.to_csv('cleaned_jira_dataset.csv', index=False)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = './TaskLevel/processed_task_data.csv'\n",
    "    cleaned_df = preprocess_jira_dataset(file_path)\n",
    "    print(\"\\nCleaning Complete. Cleaned dataset saved as 'cleaned_jira_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cleaned_jira_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cleaned_jira_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Start a D-Tale session and open it in the browser\u001b[39;00m\n\u001b[1;32m     11\u001b[0m d \u001b[38;5;241m=\u001b[39m dtale\u001b[38;5;241m.\u001b[39mshow(df, ignore_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_cell_edits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './cleaned_jira_dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./cleaned_jira_dataset.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
