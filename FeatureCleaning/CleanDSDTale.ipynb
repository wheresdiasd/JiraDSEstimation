{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fad86-86c9-43bc-a9db-9d8124c3a830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0096d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Mojang ...\n",
      "Found 420819 total issues in 'Mojang'. Analyzing projects...\n",
      "Found 8 projects in repository.\n",
      "Project distribution: 1 small, 3 medium, 4 large\n",
      "Selected large project: Minecraft Launcher with 17630 issues\n",
      "Fetching 17630 issues for project ID 11101...\n",
      "  - Processed batch 1 (300 issues, 300/17630)\n",
      "  - Processed batch 2 (300 issues, 600/17630)\n",
      "  - Processed batch 3 (300 issues, 900/17630)\n",
      "  - Processed batch 4 (300 issues, 1200/17630)\n",
      "  - Processed batch 5 (300 issues, 1500/17630)\n",
      "  - Processed batch 6 (300 issues, 1800/17630)\n",
      "  - Processed batch 7 (300 issues, 2100/17630)\n",
      "  - Processed batch 8 (300 issues, 2400/17630)\n",
      "  - Processed batch 9 (300 issues, 2700/17630)\n",
      "  - Processed batch 10 (300 issues, 3000/17630)\n",
      "  - Processed batch 11 (300 issues, 3300/17630)\n",
      "  - Processed batch 12 (300 issues, 3600/17630)\n",
      "  - Processed batch 13 (300 issues, 3900/17630)\n",
      "  - Processed batch 14 (300 issues, 4200/17630)\n",
      "  - Processed batch 15 (300 issues, 4500/17630)\n",
      "  - Processed batch 16 (300 issues, 4800/17630)\n",
      "  - Processed batch 17 (300 issues, 5100/17630)\n",
      "  - Processed batch 18 (300 issues, 5400/17630)\n",
      "  - Processed batch 19 (300 issues, 5700/17630)\n",
      "  - Processed batch 20 (300 issues, 6000/17630)\n",
      "  - Processed batch 21 (300 issues, 6300/17630)\n",
      "  - Processed batch 22 (300 issues, 6600/17630)\n",
      "  - Processed batch 23 (300 issues, 6900/17630)\n",
      "  - Processed batch 24 (300 issues, 7200/17630)\n",
      "  - Processed batch 25 (300 issues, 7500/17630)\n",
      "  - Processed batch 26 (300 issues, 7800/17630)\n",
      "  - Processed batch 27 (300 issues, 8100/17630)\n",
      "  - Processed batch 28 (300 issues, 8400/17630)\n",
      "  - Processed batch 29 (300 issues, 8700/17630)\n",
      "  - Processed batch 30 (300 issues, 9000/17630)\n",
      "  - Processed batch 31 (300 issues, 9300/17630)\n",
      "  - Processed batch 32 (300 issues, 9600/17630)\n",
      "  - Processed batch 33 (300 issues, 9900/17630)\n",
      "  - Processed batch 34 (300 issues, 10200/17630)\n",
      "  - Processed batch 35 (300 issues, 10500/17630)\n",
      "  - Processed batch 36 (300 issues, 10800/17630)\n",
      "  - Processed batch 37 (300 issues, 11100/17630)\n",
      "  - Processed batch 38 (300 issues, 11400/17630)\n",
      "  - Processed batch 39 (300 issues, 11700/17630)\n",
      "  - Processed batch 40 (300 issues, 12000/17630)\n",
      "  - Processed batch 41 (300 issues, 12300/17630)\n",
      "  - Processed batch 42 (300 issues, 12600/17630)\n",
      "  - Processed batch 43 (300 issues, 12900/17630)\n",
      "  - Processed batch 44 (300 issues, 13200/17630)\n",
      "  - Processed batch 45 (300 issues, 13500/17630)\n",
      "  - Processed batch 46 (300 issues, 13800/17630)\n",
      "  - Processed batch 47 (300 issues, 14100/17630)\n",
      "  - Processed batch 48 (300 issues, 14400/17630)\n",
      "  - Processed batch 49 (300 issues, 14700/17630)\n",
      "  - Processed batch 50 (300 issues, 15000/17630)\n",
      "  - Processed batch 51 (300 issues, 15300/17630)\n",
      "  - Processed batch 52 (300 issues, 15600/17630)\n",
      "  - Processed batch 53 (300 issues, 15900/17630)\n",
      "  - Processed batch 54 (300 issues, 16200/17630)\n",
      "  - Processed batch 55 (300 issues, 16500/17630)\n",
      "  - Processed batch 56 (300 issues, 16800/17630)\n",
      "  - Processed batch 57 (300 issues, 17100/17630)\n",
      "  - Processed batch 58 (300 issues, 17400/17630)\n",
      "  - Processed batch 59 (230 issues, 17630/17630)\n",
      "Final sample for 'Mojang': 17630 issues from 1 projects\n",
      "Combining 59 processed batches...\n",
      "Combined DataFrame has 17630 rows and 28 columns.\n",
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2022-01-05 00:56:55', '2022-01-04 20:43:21', '2022-01-04 18:25:00',\n",
      " '2021-12-31 12:56:34', '2021-12-30 16:28:09', '2022-01-04 05:21:37',\n",
      " '2021-12-28 21:29:04', '2021-12-31 19:44:53', '2021-12-29 00:18:23',\n",
      " '2022-01-01 17:54:05',\n",
      " ...\n",
      " '2013-04-18 21:19:09', '2013-04-26 10:50:11', '2013-04-20 14:37:35',\n",
      " '2013-01-25 01:33:26', '2013-04-05 18:36:55', '2013-04-18 21:01:32',\n",
      " '2013-04-24 19:57:11', '2013-01-19 22:06:14', '2013-04-19 00:13:46',\n",
      " '2013-04-24 19:57:27']\n",
      "Length: 16965, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2022-01-05 00:56:52', '2022-01-04 20:43:14', '2022-01-04 14:57:37',\n",
      " '2021-12-31 03:29:15', '2021-12-29 18:46:14', '2021-12-29 15:55:40',\n",
      " '2021-12-28 21:28:57', '2021-12-28 20:08:36', '2021-12-28 10:42:39',\n",
      " '2021-12-27 19:59:22',\n",
      " ...\n",
      " '2013-04-18 21:05:02', '2013-04-18 18:48:02', '2013-04-18 22:31:26',\n",
      " '2013-01-25 01:32:19', '2013-04-05 18:34:31', '2012-10-24 16:11:18',\n",
      " '2012-11-22 16:53:17', '2012-10-26 09:38:02', '2013-04-18 22:29:29',\n",
      " '2012-10-25 12:34:55']\n",
      "Length: 16965, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2022-01-05 00:56:55', '2022-01-04 20:43:21', '2022-01-04 18:25:00',\n",
      " '2021-12-31 12:56:34', '2021-12-30 16:28:09', '2022-01-04 05:21:37',\n",
      " '2021-12-28 21:29:04', '2021-12-31 19:44:53', '2021-12-29 00:18:23',\n",
      " '2022-01-01 17:59:23',\n",
      " ...\n",
      " '2015-09-06 21:17:18', '2015-05-07 03:52:52', '2015-09-07 19:10:58',\n",
      " '2013-05-02 17:03:02', '2017-06-06 11:53:38', '2015-05-04 06:35:23',\n",
      " '2021-05-04 11:40:18', '2015-05-04 21:17:41', '2017-02-11 20:08:03',\n",
      " '2020-10-26 22:02:20']\n",
      "Length: 16965, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ D-Tale session launched successfully.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json5 as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)  # We want to see all data\n",
    "from statistics import mean, median\n",
    "from time import time\n",
    "import json\n",
    "import dtale\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "\n",
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuetype_information.json') as f:\n",
    "    jira_issuetype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Link Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuelinktype_information.json') as f:\n",
    "    jira_issuelinktype_information = json.load(f)\n",
    "\n",
    "\n",
    "ALL_JIRAS = [jira_name for jira_name in jira_data_sources.keys()]\n",
    "\n",
    "df_jiras = pd.DataFrame(\n",
    "    np.nan,\n",
    "    columns=['Born', 'Issues', 'DIT', 'UIT', 'Links', 'DLT', 'ULT', 'Changes', 'Ch/I', 'UP', 'Comments', 'Co/I'],\n",
    "    index=ALL_JIRAS + ['Sum', 'Median', 'Std Dev']\n",
    ")\n",
    "\n",
    "def parse_date_str(x):\n",
    "    \"\"\"\n",
    "    Parse a date string using dateparser. If the string is \"Missing\", empty, or cannot be parsed, return pd.NaT.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    if s.lower() == \"missing\" or s == \"\":\n",
    "        return pd.NaT\n",
    "    \n",
    "    return x\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using our custom parse_date_str.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_date_str)\n",
    "    return df\n",
    "\n",
    "def drop_invalid_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    Drop rows where any of the specified date columns are NaT.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame.\n",
    "      date_columns (list): List of date column names to check.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with rows dropped if any of the specified date columns are NaT.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=date_columns)\n",
    "\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    (Date-like strings remain as strings.)\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories:\n",
    "      - Flatten the histories.\n",
    "      - Apply type conversion.\n",
    "      - Add an 'issue_key' (using 'key' if available, else 'id') for merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue (keyed by 'issue_key').\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "def parse_issuelinks(issuelinks):\n",
    "    \"\"\"\n",
    "    Parse the issuelinks field which is always a string (or null).\n",
    "    Returns a list of link dictionaries if parsing is successful, or an empty list.\n",
    "    \"\"\"\n",
    "    if issuelinks is None:\n",
    "        return []\n",
    "    if isinstance(issuelinks, str):\n",
    "        issuelinks = issuelinks.strip()\n",
    "        if not issuelinks:\n",
    "            return []\n",
    "        try:\n",
    "            parsed = ast.literal_eval(issuelinks)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing issuelinks: {issuelinks}. Error: {e}\")\n",
    "    return []\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' field and return only the total number of links.\n",
    "    \"\"\"\n",
    "    links = parse_issuelinks(issuelinks)\n",
    "    return {\"issuelinks_total\": len(links)}\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \"\"\"\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(emb_array, index=descriptions.index,\n",
    "                          columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])])\n",
    "    return emb_df\n",
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=500):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using batch processing to prevent connection timeouts.\n",
    "#     Uses a fixed maximum of 500 records per repository and queries only necessary fields.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Original sample ratio parameter (kept for compatibility)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Processing in batches of {batch_size}...\")\n",
    "    \n",
    "#     # --- 1) Calculate sample size with fixed maximum ---\n",
    "#     MAX_RECORDS = 500  # Fixed maximum number of records\n",
    "#     desired_sample_size = min(MAX_RECORDS, total_issues)\n",
    "#     print(f\"Using fixed maximum of {MAX_RECORDS} records. Will retrieve {desired_sample_size} issues.\")\n",
    "    \n",
    "#     # --- 2) Determine if we need to sample during fetching or after ---\n",
    "#     if desired_sample_size < total_issues / 10:\n",
    "#         # If we want a small sample, use random skip to efficiently get samples\n",
    "#         # This avoids loading all documents when we only need a small fraction\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Fetch documents by their indices using skip/limit, but only retrieve needed fields\n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # For larger samples, process in batches\n",
    "#         sampled_issues = []\n",
    "#         cursor = db[jira_name].find({}, needed_fields)  # Only retrieve needed fields\n",
    "        \n",
    "#         # Process in batches to avoid loading everything at once\n",
    "#         batch_count = 0\n",
    "#         while True:\n",
    "#             batch = list(cursor.limit(batch_size).skip(batch_count * batch_size))\n",
    "#             if not batch:\n",
    "#                 break\n",
    "                \n",
    "#             batch_count += 1\n",
    "#             print(f\"  - Processed batch {batch_count} ({len(batch)} issues)\")\n",
    "#             sampled_issues.extend(batch)\n",
    "            \n",
    "#         # Apply sampling after all batches are collected\n",
    "#         if len(sampled_issues) > desired_sample_size:\n",
    "#             print(f\"  - Sampling {desired_sample_size} issues from {len(sampled_issues)} collected issues\")\n",
    "#             sampled_issues = random.sample(sampled_issues, desired_sample_size)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues (out of {total_issues} total).\")\n",
    "    \n",
    "#     # --- 3) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main\n",
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=500, target_cap=900):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using project-based sampling.\n",
    "#     Samples complete projects to reach approximately target_cap total records.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Used to influence project selection (higher means more projects)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "#         target_cap (int): Target number of total issues to sample across projects\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "#     # --- 1) Get project distribution ---\n",
    "#     # Get count of issues by project.id\n",
    "#     project_counts = list(db[jira_name].aggregate([\n",
    "#         {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "#                    \"count\": {\"$sum\": 1},\n",
    "#                    \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "#     ]))\n",
    "    \n",
    "#     # Convert to list of dictionaries with project details\n",
    "#     projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "#                for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "#     print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "#     # --- 2) Select projects based on size categories ---\n",
    "#     # Adjust target cap based on sample_ratio if needed\n",
    "#     adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "#     # Group projects by size\n",
    "#     small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "#     medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "#     large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "#     print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "#     # Initialize selection\n",
    "#     selected_projects = []\n",
    "#     total_selected_issues = 0\n",
    "    \n",
    "#     # Helper function to find best fit project\n",
    "#     def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "#         if not project_list:\n",
    "#             return None\n",
    "        \n",
    "#         # Sort by size (descending if prefer_larger, ascending otherwise)\n",
    "#         sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "#         # Find first project that fits\n",
    "#         for project in sorted_projects:\n",
    "#             # Allow slight overage (up to 20%)\n",
    "#             if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "#                 return project\n",
    "        \n",
    "#         # If nothing fits within constraints, take smallest\n",
    "#         if not prefer_larger:\n",
    "#             return sorted_projects[0]  # smallest\n",
    "#         return None\n",
    "    \n",
    "#     # Try to select a diverse mix of projects\n",
    "    \n",
    "#     # First try to get a large project if it fits reasonably\n",
    "#     if large_projects:\n",
    "#         best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "#         if best_large and best_large[\"count\"] <= adjusted_target * 1.3:  # Allow 30% overage for large projects\n",
    "#             selected_projects.append(best_large)\n",
    "#             total_selected_issues += best_large[\"count\"]\n",
    "#             large_projects.remove(best_large)\n",
    "#             print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "#     # Then add some medium projects\n",
    "#     while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "#         if best_medium:\n",
    "#             selected_projects.append(best_medium)\n",
    "#             total_selected_issues += best_medium[\"count\"]\n",
    "#             medium_projects.remove(best_medium)\n",
    "#             print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # Finally fill in with small projects\n",
    "#     while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "#         if best_small:\n",
    "#             selected_projects.append(best_small)\n",
    "#             total_selected_issues += best_small[\"count\"]\n",
    "#             small_projects.remove(best_small)\n",
    "#             print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # If we're still significantly under target, add one more project that best fits\n",
    "#     if total_selected_issues < adjusted_target * 0.8:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         remaining_projects = small_projects + medium_projects + large_projects\n",
    "#         best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "#         if best_remaining:\n",
    "#             selected_projects.append(best_remaining)\n",
    "#             total_selected_issues += best_remaining[\"count\"]\n",
    "#             print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "#     # --- 3) Fetch issues from selected projects ---\n",
    "#     if not selected_projects:\n",
    "#         print(\"No projects could be selected. Using default sampling method.\")\n",
    "#         # Fall back to the original sampling method\n",
    "#         desired_sample_size = min(500, total_issues)\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # Get all issues from selected projects\n",
    "#         project_ids = [p[\"id\"] for p in selected_projects]\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Process each project\n",
    "#         for project_id in project_ids:\n",
    "#             # Get count for this project\n",
    "#             project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "            \n",
    "#             # Fetch in batches\n",
    "#             print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "#             cursor = db[jira_name].find({\"fields.project.id\": project_id}, needed_fields)\n",
    "            \n",
    "#             batch_count = 0\n",
    "#             fetched_count = 0\n",
    "            \n",
    "#             while fetched_count < project_issue_count:\n",
    "#                 batch = list(cursor.limit(batch_size).skip(batch_count * batch_size))\n",
    "#                 if not batch:\n",
    "#                     break\n",
    "                \n",
    "#                 batch_count += 1\n",
    "#                 fetched_count += len(batch)\n",
    "#                 print(f\"  - Fetched batch {batch_count} ({len(batch)} issues, {fetched_count}/{project_issue_count})\")\n",
    "#                 sampled_issues.extend(batch)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues from {len(selected_projects)} projects\")\n",
    "    \n",
    "#     # --- 4) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main\n",
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=300, target_cap=90000):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using project-based sampling.\n",
    "#     Samples complete projects to reach approximately target_cap total records.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Used to influence project selection (higher means more projects)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "#         target_cap (int): Target number of total issues to sample across projects\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "#     # --- 1) Get project distribution ---\n",
    "#     # Get count of issues by project.id\n",
    "#     project_counts = list(db[jira_name].aggregate([\n",
    "#         {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "#                    \"count\": {\"$sum\": 1},\n",
    "#                    \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "#     ]))\n",
    "    \n",
    "#     # Convert to list of dictionaries with project details\n",
    "#     projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "#                for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "#     print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "#     # --- 2) Select projects based on size categories ---\n",
    "#     # Adjust target cap based on sample_ratio if needed\n",
    "#     adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "#     # Group projects by size\n",
    "#     small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "#     medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "#     large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "#     print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "#     # Initialize selection\n",
    "#     selected_projects = []\n",
    "#     total_selected_issues = 0\n",
    "    \n",
    "#     # Helper function to find best fit project\n",
    "#     def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "#         if not project_list:\n",
    "#             return None\n",
    "        \n",
    "#         # Sort by size (descending if prefer_larger, ascending otherwise)\n",
    "#         sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "#         # Find first project that fits\n",
    "#         for project in sorted_projects:\n",
    "#             # Allow slight overage (up to 20%)\n",
    "#             if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "#                 return project\n",
    "        \n",
    "#         # If nothing fits within constraints, take smallest\n",
    "#         if not prefer_larger:\n",
    "#             return sorted_projects[0]  # smallest\n",
    "#         return None\n",
    "    \n",
    "#     # Try to select a diverse mix of projects\n",
    "    \n",
    "#     # First try to get a large project if it fits reasonably\n",
    "#     if large_projects:\n",
    "#         best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "#         if best_large and best_large[\"count\"] <= adjusted_target * 1.3:  # Allow 30% overage for large projects\n",
    "#             selected_projects.append(best_large)\n",
    "#             total_selected_issues += best_large[\"count\"]\n",
    "#             large_projects.remove(best_large)\n",
    "#             print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "#     # Then add some medium projects\n",
    "#     while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "#         if best_medium:\n",
    "#             selected_projects.append(best_medium)\n",
    "#             total_selected_issues += best_medium[\"count\"]\n",
    "#             medium_projects.remove(best_medium)\n",
    "#             print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # Finally fill in with small projects\n",
    "#     while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "#         if best_small:\n",
    "#             selected_projects.append(best_small)\n",
    "#             total_selected_issues += best_small[\"count\"]\n",
    "#             small_projects.remove(best_small)\n",
    "#             print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # If we're still significantly under target, add one more project that best fits\n",
    "#     if total_selected_issues < adjusted_target * 0.8:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         remaining_projects = small_projects + medium_projects + large_projects\n",
    "#         best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "#         if best_remaining:\n",
    "#             selected_projects.append(best_remaining)\n",
    "#             total_selected_issues += best_remaining[\"count\"]\n",
    "#             print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "#     # --- 3) Fetch issues from selected projects ---\n",
    "#     if not selected_projects:\n",
    "#         print(\"No projects could be selected. Using default sampling method.\")\n",
    "#         # Fall back to the original sampling method\n",
    "#         desired_sample_size = min(500, total_issues)\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # Get all issues from selected projects\n",
    "#         project_ids = [p[\"id\"] for p in selected_projects]\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Process each project\n",
    "#         for project_id in project_ids:\n",
    "#             # Get count for this project\n",
    "#             project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "            \n",
    "#             # Fetch in batches\n",
    "#             print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "            \n",
    "#             # Fixed: Create a new cursor for each batch instead of reusing the same cursor\n",
    "#             batch_count = 0\n",
    "#             fetched_count = 0\n",
    "            \n",
    "#             while fetched_count < project_issue_count:\n",
    "#                 # Create a fresh cursor for each batch with the appropriate skip and limit\n",
    "#                 cursor = db[jira_name].find(\n",
    "#                     {\"fields.project.id\": project_id}, \n",
    "#                     needed_fields\n",
    "#                 ).skip(batch_count * batch_size).limit(batch_size)\n",
    "                \n",
    "#                 batch = list(cursor)\n",
    "#                 if not batch:\n",
    "#                     break\n",
    "                \n",
    "#                 batch_count += 1\n",
    "#                 fetched_count += len(batch)\n",
    "#                 print(f\"  - Fetched batch {batch_count} ({len(batch)} issues, {fetched_count}/{project_issue_count})\")\n",
    "#                 sampled_issues.extend(batch)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues from {len(selected_projects)} projects\")\n",
    "    \n",
    "#     # --- 4) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main\n",
    "\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio, batch_size=300, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository using streaming approach with CSV storage.\n",
    "    \n",
    "    Parameters:\n",
    "        jira_name (str): Name of the Jira repository\n",
    "        db: MongoDB database connection\n",
    "        sample_ratio (float): Used to influence project selection\n",
    "        batch_size (int): Size of batches for processing\n",
    "        target_cap (int): Target number of total issues to sample\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with sampled issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "    # Get total count first (fast operation)\n",
    "    total_issues = db[jira_name].count_documents({})\n",
    "    if total_issues == 0:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # Define only the fields we actually need\n",
    "    needed_fields = {\n",
    "        # Essential identification fields\n",
    "        \"id\": 1,\n",
    "        \"key\": 1,\n",
    "        \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "        # Issue metadata\n",
    "        \"fields.summary\": 1,\n",
    "        \"fields.description\": 1,\n",
    "        \"fields.created\": 1,\n",
    "        \"fields.updated\": 1,\n",
    "        \"fields.resolutiondate\": 1,\n",
    "        \n",
    "        # Classification fields\n",
    "        \"fields.issuetype.name\": 1,\n",
    "        \"fields.priority.name\": 1,\n",
    "        \"fields.status.name\": 1,\n",
    "        \n",
    "        # People fields\n",
    "        \"fields.assignee.key\": 1,\n",
    "        \"fields.assignee.name\": 1,\n",
    "        \"fields.reporter.key\": 1, \n",
    "        \"fields.reporter.name\": 1,\n",
    "        \"fields.creator.key\": 1,\n",
    "        \"fields.creator.name\": 1,\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\": 1,\n",
    "        \"fields.project.key\": 1, \n",
    "        \"fields.project.name\": 1,\n",
    "        \n",
    "        # Relationships\n",
    "        \"fields.issuelinks\": 1,\n",
    "        \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\": 1,\n",
    "        \"fields.labels\": 1,\n",
    "        \"fields.fixVersions\": 1,\n",
    "        \n",
    "        # Comments\n",
    "        \"fields.comments\": 1\n",
    "    }\n",
    "    \n",
    "    print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "    # --- 1) Get project distribution ---\n",
    "    project_counts = list(db[jira_name].aggregate([\n",
    "        {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "                   \"count\": {\"$sum\": 1},\n",
    "                   \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "    ]))\n",
    "    \n",
    "    projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "               for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "    print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "    # --- 2) Select projects based on size categories ---\n",
    "    adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "    small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "    medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "    large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "    print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "    # Initialize selection\n",
    "    selected_projects = []\n",
    "    total_selected_issues = 0\n",
    "    \n",
    "    # Helper function to find best fit project\n",
    "    def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "        if not project_list:\n",
    "            return None\n",
    "        \n",
    "        sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "        for project in sorted_projects:\n",
    "            if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "                return project\n",
    "        \n",
    "        if not prefer_larger:\n",
    "            return sorted_projects[0]  # smallest\n",
    "        return None\n",
    "    \n",
    "    # Try to select a diverse mix of projects\n",
    "    \n",
    "    # First try to get a large project\n",
    "    if large_projects:\n",
    "        best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "        if best_large and best_large[\"count\"] <= adjusted_target * 1.3:\n",
    "            selected_projects.append(best_large)\n",
    "            total_selected_issues += best_large[\"count\"]\n",
    "            large_projects.remove(best_large)\n",
    "            print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "    # Then add some medium projects\n",
    "    while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "        if best_medium:\n",
    "            selected_projects.append(best_medium)\n",
    "            total_selected_issues += best_medium[\"count\"]\n",
    "            medium_projects.remove(best_medium)\n",
    "            print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Finally fill in with small projects\n",
    "    while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "        if best_small:\n",
    "            selected_projects.append(best_small)\n",
    "            total_selected_issues += best_small[\"count\"]\n",
    "            small_projects.remove(best_small)\n",
    "            print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # If we're still significantly under target, add one more project that best fits\n",
    "    if total_selected_issues < adjusted_target * 0.8:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        remaining_projects = small_projects + medium_projects + large_projects\n",
    "        best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "        if best_remaining:\n",
    "            selected_projects.append(best_remaining)\n",
    "            total_selected_issues += best_remaining[\"count\"]\n",
    "            print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "    # --- 3) Create temporary directory for streaming storage ---\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    batch_files = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # --- 4) Process issues using streaming approach ---\n",
    "        if not selected_projects:\n",
    "            print(\"No projects could be selected. Using default sampling method.\")\n",
    "            # Fall back to random sampling\n",
    "            desired_sample_size = min(500, total_issues)\n",
    "            sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                # Explicitly exclude _id field\n",
    "                cursor = db[jira_name].find({}, {**needed_fields, \"_id\": 0}).skip(idx).limit(1)\n",
    "                batch = list(cursor)\n",
    "                \n",
    "                # Process batch immediately\n",
    "                if batch:\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_random_{idx}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    total_processed += len(batch)\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "        else:\n",
    "            # Process each selected project\n",
    "            project_ids = [p[\"id\"] for p in selected_projects]\n",
    "            \n",
    "            for project_id in project_ids:\n",
    "                # Get count for this project\n",
    "                project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "                \n",
    "                # Fetch in batches\n",
    "                print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "                \n",
    "                batch_count = 0\n",
    "                fetched_count = 0\n",
    "                \n",
    "                while fetched_count < project_issue_count:\n",
    "                    # Create a fresh cursor for each batch with the appropriate skip and limit\n",
    "                    # Explicitly exclude _id field\n",
    "                    cursor = db[jira_name].find(\n",
    "                        {\"fields.project.id\": project_id}, \n",
    "                        {**needed_fields, \"_id\": 0}  # Exclude _id field\n",
    "                    ).skip(batch_count * batch_size).limit(batch_size)\n",
    "                    \n",
    "                    batch = list(cursor)\n",
    "                    if not batch:\n",
    "                        break\n",
    "\n",
    "                    # Store the batch size for printing later\n",
    "                    batch_size_actual = len(batch)\n",
    "                    \n",
    "                    # Process batch immediately\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_{project_id}_{batch_count}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                    fetched_count += batch_size_actual\n",
    "                    total_processed += batch_size_actual\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"  - Processed batch {batch_count} ({batch_size_actual} issues, {fetched_count}/{project_issue_count})\")\n",
    "        \n",
    "        print(f\"Final sample for '{jira_name}': {total_processed} issues from {len(selected_projects)} projects\")\n",
    "        \n",
    "        # --- 5) Combine all saved batches ---\n",
    "        if not batch_files:\n",
    "            print(\"No data collected.\")\n",
    "            return None\n",
    "        \n",
    "        # Read all CSV files and combine\n",
    "        print(f\"Combining {len(batch_files)} processed batches...\")\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                chunk = pd.read_csv(file, low_memory=False)\n",
    "                dfs.append(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading batch file {file}: {e}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"No valid data read from batches.\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all data\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Combined DataFrame has {len(final_df)} rows and {len(final_df.columns)} columns.\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing temp file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(temp_dir):\n",
    "                os.rmdir(temp_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing temp directory {temp_dir}: {e}\")\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3,\n",
    "                                zero_threshold=0.8, open_dtale=True, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    and drop changelog summary columns dominated by zeros.\n",
    "    \n",
    "    If open_dtale is True, launch a D-Tale session for interactive visualization;\n",
    "    otherwise, simply return the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        selected_jiras (list): List of Jira repositories to process\n",
    "        sample_ratio (float): Ratio of data to sample (1.0 = 100%)\n",
    "        missing_threshold (float): Threshold for dropping columns with missing values\n",
    "        zero_threshold (float): Threshold for dropping columns dominated by zeros\n",
    "        open_dtale (bool): Whether to launch a D-Tale session\n",
    "        target_cap (int): Maximum number of issues to retrieve per repository\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "\n",
    "    # Load Jira data sources configuration\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    # Process each repository in parallel\n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio, 300, target_cap): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "\n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "\n",
    "\n",
    "    # # Process the 'fields.description' field to generate dense embeddings\n",
    "    # if \"fields.description\" in final_df.columns:\n",
    "    #     desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "    #     final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "\n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "\n",
    "    date_cols = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\"]\n",
    "\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_cols)\n",
    "    final_df = drop_invalid_dates(final_df, date_cols)\n",
    "    final_df['fields.created'] = pd.to_datetime(final_df['fields.created'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.updated'] = pd.to_datetime(final_df['fields.updated'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.resolutiondate'] = pd.to_datetime(final_df['fields.resolutiondate'], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if open_dtale:\n",
    "        print(\"Data processed. Launching D-Tale session...\")\n",
    "        d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "        d.open_browser()\n",
    "        print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def export_clean_df():\n",
    "    \"\"\"\n",
    "    Run the full OverviewAnalysis pipeline and return the final DataFrame with all engineered features.\n",
    "    This version uses sample_ratio=1.0 to get 100% of the dataset and an unlimited target_cap.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The final processed DataFrame with all data.\n",
    "    \"\"\"\n",
    "    final_df = explore_all_fields_in_dtale(\n",
    "        selected_jiras=[\"Mojang\"],\n",
    "        sample_ratio=1.0,  # Set to 1.0 for 100% of data\n",
    "        missing_threshold=0.3,\n",
    "        zero_threshold=0.8,\n",
    "        open_dtale=True,\n",
    "        target_cap=20000  # No upper limit on the number of issues\n",
    "    )\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# For testing purposes, you can run export_clean_df() if executing this module directly.\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_training = export_clean_df()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
