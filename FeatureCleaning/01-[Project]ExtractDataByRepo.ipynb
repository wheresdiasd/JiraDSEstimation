{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing repository: Jira\n",
      "==================================================\n",
      "Fetching project IDs for Jira...\n",
      "Found 30 projects in Jira\n",
      "\n",
      "Processing project: Jira_Server_and_Data_Center (ID: 10240)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (1200 issues, 15600 total)\n",
      "  - Processed batch 14 (1200 issues, 16800 total)\n",
      "  - Processed batch 15 (1200 issues, 18000 total)\n",
      "  - Processed batch 16 (1200 issues, 19200 total)\n",
      "  - Processed batch 17 (1200 issues, 20400 total)\n",
      "  - Processed batch 18 (1200 issues, 21600 total)\n",
      "  - Processed batch 19 (1200 issues, 22800 total)\n",
      "  - Processed batch 20 (1200 issues, 24000 total)\n",
      "  - Processed batch 21 (1200 issues, 25200 total)\n",
      "  - Processed batch 22 (1200 issues, 26400 total)\n",
      "  - Processed batch 23 (1200 issues, 27600 total)\n",
      "  - Processed batch 24 (1200 issues, 28800 total)\n",
      "  - Processed batch 25 (1200 issues, 30000 total)\n",
      "  - Processed batch 26 (1200 issues, 31200 total)\n",
      "  - Processed batch 27 (1200 issues, 32400 total)\n",
      "  - Processed batch 28 (1200 issues, 33600 total)\n",
      "  - Processed batch 29 (1200 issues, 34800 total)\n",
      "  - Processed batch 30 (1200 issues, 36000 total)\n",
      "  - Processed batch 31 (1200 issues, 37200 total)\n",
      "  - Processed batch 32 (1200 issues, 38400 total)\n",
      "  - Processed batch 33 (1200 issues, 39600 total)\n",
      "  - Processed batch 34 (1200 issues, 40800 total)\n",
      "  - Processed batch 35 (1200 issues, 42000 total)\n",
      "  - Processed batch 36 (1200 issues, 43200 total)\n",
      "  - Processed batch 37 (1200 issues, 44400 total)\n",
      "  - Processed batch 38 (1200 issues, 45600 total)\n",
      "  - Processed batch 39 (1200 issues, 46800 total)\n",
      "  - Processed batch 40 (425 issues, 47225 total)\n",
      "Combining 40 processed batches...\n",
      "Processing batch file 1/40\n",
      "Processing batch file 2/40\n",
      "Processing batch file 3/40\n",
      "Processing batch file 4/40\n",
      "Processing batch file 5/40\n",
      "Processing batch file 6/40\n",
      "Processing batch file 7/40\n",
      "Processing batch file 8/40\n",
      "Processing batch file 9/40\n",
      "Processing batch file 10/40\n",
      "Processing batch file 11/40\n",
      "Processing batch file 12/40\n",
      "Processing batch file 13/40\n",
      "Processing batch file 14/40\n",
      "Processing batch file 15/40\n",
      "Processing batch file 16/40\n",
      "Processing batch file 17/40\n",
      "Processing batch file 18/40\n",
      "Processing batch file 19/40\n",
      "Processing batch file 20/40\n",
      "Processing batch file 21/40\n",
      "Processing batch file 22/40\n",
      "Processing batch file 23/40\n",
      "Processing batch file 24/40\n",
      "Processing batch file 25/40\n",
      "Processing batch file 26/40\n",
      "Processing batch file 27/40\n",
      "Processing batch file 28/40\n",
      "Processing batch file 29/40\n",
      "Processing batch file 30/40\n",
      "Processing batch file 31/40\n",
      "Processing batch file 32/40\n",
      "Processing batch file 33/40\n",
      "Processing batch file 34/40\n",
      "Processing batch file 35/40\n",
      "Processing batch file 36/40\n",
      "Processing batch file 37/40\n",
      "Processing batch file 38/40\n",
      "Processing batch file 39/40\n",
      "Processing batch file 40/40\n",
      "Combined DataFrame has 47225 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/10240_Jira_Server_and_Data_Center.csv\n",
      "Extraction complete for project Jira_Server_and_Data_Center. Shape of data: (47225, 50)\n",
      "Data saved to: jira_extracted_data/Jira/10240_Jira_Server_and_Data_Center.csv\n",
      "\n",
      "Processing project: Confluence_Server_and_Data_Center (ID: 10470)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (1200 issues, 15600 total)\n",
      "  - Processed batch 14 (1200 issues, 16800 total)\n",
      "  - Processed batch 15 (1200 issues, 18000 total)\n",
      "  - Processed batch 16 (1200 issues, 19200 total)\n",
      "  - Processed batch 17 (1200 issues, 20400 total)\n",
      "  - Processed batch 18 (1200 issues, 21600 total)\n",
      "  - Processed batch 19 (1200 issues, 22800 total)\n",
      "  - Processed batch 20 (1200 issues, 24000 total)\n",
      "  - Processed batch 21 (1200 issues, 25200 total)\n",
      "  - Processed batch 22 (1200 issues, 26400 total)\n",
      "  - Processed batch 23 (1200 issues, 27600 total)\n",
      "  - Processed batch 24 (1200 issues, 28800 total)\n",
      "  - Processed batch 25 (1200 issues, 30000 total)\n",
      "  - Processed batch 26 (1200 issues, 31200 total)\n",
      "  - Processed batch 27 (1200 issues, 32400 total)\n",
      "  - Processed batch 28 (1200 issues, 33600 total)\n",
      "  - Processed batch 29 (1200 issues, 34800 total)\n",
      "  - Processed batch 30 (1200 issues, 36000 total)\n",
      "  - Processed batch 31 (1200 issues, 37200 total)\n",
      "  - Processed batch 32 (1200 issues, 38400 total)\n",
      "  - Processed batch 33 (1200 issues, 39600 total)\n",
      "  - Processed batch 34 (1200 issues, 40800 total)\n",
      "  - Processed batch 35 (1200 issues, 42000 total)\n",
      "  - Processed batch 36 (1200 issues, 43200 total)\n",
      "  - Processed batch 37 (710 issues, 43910 total)\n",
      "Combining 37 processed batches...\n",
      "Processing batch file 1/37\n",
      "Processing batch file 2/37\n",
      "Processing batch file 3/37\n",
      "Processing batch file 4/37\n",
      "Processing batch file 5/37\n",
      "Processing batch file 6/37\n",
      "Processing batch file 7/37\n",
      "Processing batch file 8/37\n",
      "Processing batch file 9/37\n",
      "Processing batch file 10/37\n",
      "Processing batch file 11/37\n",
      "Processing batch file 12/37\n",
      "Processing batch file 13/37\n",
      "Processing batch file 14/37\n",
      "Processing batch file 15/37\n",
      "Processing batch file 16/37\n",
      "Processing batch file 17/37\n",
      "Processing batch file 18/37\n",
      "Processing batch file 19/37\n",
      "Processing batch file 20/37\n",
      "Processing batch file 21/37\n",
      "Processing batch file 22/37\n",
      "Processing batch file 23/37\n",
      "Processing batch file 24/37\n",
      "Processing batch file 25/37\n",
      "Processing batch file 26/37\n",
      "Processing batch file 27/37\n",
      "Processing batch file 28/37\n",
      "Processing batch file 29/37\n",
      "Processing batch file 30/37\n",
      "Processing batch file 31/37\n",
      "Processing batch file 32/37\n",
      "Processing batch file 33/37\n",
      "Processing batch file 34/37\n",
      "Processing batch file 35/37\n",
      "Processing batch file 36/37\n",
      "Processing batch file 37/37\n",
      "Combined DataFrame has 43910 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/10470_Confluence_Server_and_Data_Center.csv\n",
      "Extraction complete for project Confluence_Server_and_Data_Center. Shape of data: (43910, 50)\n",
      "Data saved to: jira_extracted_data/Jira/10470_Confluence_Server_and_Data_Center.csv\n",
      "\n",
      "Processing project: atlassian-seraph (ID: 10480)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (177 issues, 177 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 177 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/10480_atlassian-seraph.csv\n",
      "Extraction complete for project atlassian-seraph. Shape of data: (177, 52)\n",
      "Data saved to: jira_extracted_data/Jira/10480_atlassian-seraph.csv\n",
      "\n",
      "Processing project: Bamboo (ID: 11011)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (225 issues, 14625 total)\n",
      "Combining 13 processed batches...\n",
      "Processing batch file 1/13\n",
      "Processing batch file 2/13\n",
      "Processing batch file 3/13\n",
      "Processing batch file 4/13\n",
      "Processing batch file 5/13\n",
      "Processing batch file 6/13\n",
      "Processing batch file 7/13\n",
      "Processing batch file 8/13\n",
      "Processing batch file 9/13\n",
      "Processing batch file 10/13\n",
      "Processing batch file 11/13\n",
      "Processing batch file 12/13\n",
      "Processing batch file 13/13\n",
      "Combined DataFrame has 14625 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/11011_Bamboo.csv\n",
      "Extraction complete for project Bamboo. Shape of data: (14625, 49)\n",
      "Data saved to: jira_extracted_data/Jira/11011_Bamboo.csv\n",
      "\n",
      "Processing project: Crowd (ID: 11291)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (807 issues, 4407 total)\n",
      "Combining 4 processed batches...\n",
      "Processing batch file 1/4\n",
      "Processing batch file 2/4\n",
      "Processing batch file 3/4\n",
      "Processing batch file 4/4\n",
      "Combined DataFrame has 4407 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/11291_Crowd.csv\n",
      "Extraction complete for project Crowd. Shape of data: (4407, 50)\n",
      "Data saved to: jira_extracted_data/Jira/11291_Crowd.csv\n",
      "\n",
      "Processing project: Atlassian_Internationalization (ID: 11460)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (837 issues, 837 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 837 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/11460_Atlassian_Internationalization.csv\n",
      "Extraction complete for project Atlassian_Internationalization. Shape of data: (837, 46)\n",
      "Data saved to: jira_extracted_data/Jira/11460_Atlassian_Internationalization.csv\n",
      "\n",
      "Processing project: Crucible (ID: 11771)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (765 issues, 6765 total)\n",
      "Combining 6 processed batches...\n",
      "Processing batch file 1/6\n",
      "Processing batch file 2/6\n",
      "Processing batch file 3/6\n",
      "Processing batch file 4/6\n",
      "Processing batch file 5/6\n",
      "Processing batch file 6/6\n",
      "Combined DataFrame has 6765 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/11771_Crucible.csv\n",
      "Extraction complete for project Crucible. Shape of data: (6765, 50)\n",
      "Data saved to: jira_extracted_data/Jira/11771_Crucible.csv\n",
      "\n",
      "Processing project: Clover (ID: 11772)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (301 issues, 1501 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 1501 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/11772_Clover.csv\n",
      "Extraction complete for project Clover. Shape of data: (1501, 49)\n",
      "Data saved to: jira_extracted_data/Jira/11772_Clover.csv\n",
      "\n",
      "Processing project: FishEye (ID: 11830)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (761 issues, 5561 total)\n",
      "Combining 5 processed batches...\n",
      "Processing batch file 1/5\n",
      "Processing batch file 2/5\n",
      "Processing batch file 3/5\n",
      "Processing batch file 4/5\n",
      "Processing batch file 5/5\n",
      "Combined DataFrame has 5561 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/11830_FishEye.csv\n",
      "Extraction complete for project FishEye. Shape of data: (5561, 50)\n",
      "Data saved to: jira_extracted_data/Jira/11830_FishEye.csv\n",
      "\n",
      "Processing project: Jira_Software_Server_and_Data_Center (ID: 12200)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (73 issues, 13273 total)\n",
      "Combining 12 processed batches...\n",
      "Processing batch file 1/12\n",
      "Processing batch file 2/12\n",
      "Processing batch file 3/12\n",
      "Processing batch file 4/12\n",
      "Processing batch file 5/12\n",
      "Processing batch file 6/12\n",
      "Processing batch file 7/12\n",
      "Processing batch file 8/12\n",
      "Processing batch file 9/12\n",
      "Processing batch file 10/12\n",
      "Processing batch file 11/12\n",
      "Processing batch file 12/12\n",
      "Combined DataFrame has 13273 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/12200_Jira_Software_Server_and_Data_Center.csv\n",
      "Extraction complete for project Jira_Software_Server_and_Data_Center. Shape of data: (13273, 48)\n",
      "Data saved to: jira_extracted_data/Jira/12200_Jira_Software_Server_and_Data_Center.csv\n",
      "\n",
      "Processing project: Sourcetree_For_Mac (ID: 12910)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (492 issues, 5292 total)\n",
      "Combining 5 processed batches...\n",
      "Processing batch file 1/5\n",
      "Processing batch file 2/5\n",
      "Processing batch file 3/5\n",
      "Processing batch file 4/5\n",
      "Processing batch file 5/5\n",
      "Combined DataFrame has 5292 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/12910_Sourcetree_For_Mac.csv\n",
      "Extraction complete for project Sourcetree_For_Mac. Shape of data: (5292, 48)\n",
      "Data saved to: jira_extracted_data/Jira/12910_Sourcetree_For_Mac.csv\n",
      "\n",
      "Processing project: Bitbucket_Server (ID: 13310)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (685 issues, 7885 total)\n",
      "Combining 7 processed batches...\n",
      "Processing batch file 1/7\n",
      "Processing batch file 2/7\n",
      "Processing batch file 3/7\n",
      "Processing batch file 4/7\n",
      "Processing batch file 5/7\n",
      "Processing batch file 6/7\n",
      "Processing batch file 7/7\n",
      "Combined DataFrame has 7885 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/13310_Bitbucket_Server.csv\n",
      "Extraction complete for project Bitbucket_Server. Shape of data: (7885, 49)\n",
      "Data saved to: jira_extracted_data/Jira/13310_Bitbucket_Server.csv\n",
      "\n",
      "Processing project: atlassian-http (ID: 14310)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (2 issues, 2 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 2 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/14310_atlassian-http.csv\n",
      "Extraction complete for project atlassian-http. Shape of data: (2, 45)\n",
      "Data saved to: jira_extracted_data/Jira/14310_atlassian-http.csv\n",
      "\n",
      "Processing project: Sourcetree_for_Windows (ID: 14510)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (46 issues, 9646 total)\n",
      "Combining 9 processed batches...\n",
      "Processing batch file 1/9\n",
      "Processing batch file 2/9\n",
      "Processing batch file 3/9\n",
      "Processing batch file 4/9\n",
      "Processing batch file 5/9\n",
      "Processing batch file 6/9\n",
      "Processing batch file 7/9\n",
      "Processing batch file 8/9\n",
      "Processing batch file 9/9\n",
      "Combined DataFrame has 9646 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/14510_Sourcetree_for_Windows.csv\n",
      "Extraction complete for project Sourcetree_for_Windows. Shape of data: (9646, 48)\n",
      "Data saved to: jira_extracted_data/Jira/14510_Sourcetree_for_Windows.csv\n",
      "\n",
      "Processing project: Atlassian_Cloud (ID: 14613)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (555 issues, 7755 total)\n",
      "Combining 7 processed batches...\n",
      "Processing batch file 1/7\n",
      "Processing batch file 2/7\n",
      "Processing batch file 3/7\n",
      "Processing batch file 4/7\n",
      "Processing batch file 5/7\n",
      "Processing batch file 6/7\n",
      "Processing batch file 7/7\n",
      "Combined DataFrame has 7755 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/14613_Atlassian_Cloud.csv\n",
      "Extraction complete for project Atlassian_Cloud. Shape of data: (7755, 49)\n",
      "Data saved to: jira_extracted_data/Jira/14613_Atlassian_Cloud.csv\n",
      "\n",
      "Processing project: Jira_Service_Management_Server_and_Data_Center (ID: 15611)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (895 issues, 6895 total)\n",
      "Combining 6 processed batches...\n",
      "Processing batch file 1/6\n",
      "Processing batch file 2/6\n",
      "Processing batch file 3/6\n",
      "Processing batch file 4/6\n",
      "Processing batch file 5/6\n",
      "Processing batch file 6/6\n",
      "Combined DataFrame has 6895 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/15611_Jira_Service_Management_Server_and_Data_Center.csv\n",
      "Extraction complete for project Jira_Service_Management_Server_and_Data_Center. Shape of data: (6895, 49)\n",
      "Data saved to: jira_extracted_data/Jira/15611_Jira_Service_Management_Server_and_Data_Center.csv\n",
      "\n",
      "Processing project: Advanced_Roadmaps (ID: 16510)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (980 issues, 2180 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 2180 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/16510_Advanced_Roadmaps.csv\n",
      "Extraction complete for project Advanced_Roadmaps. Shape of data: (2180, 49)\n",
      "Data saved to: jira_extracted_data/Jira/16510_Advanced_Roadmaps.csv\n",
      "\n",
      "Processing project: Identity (ID: 16810)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (110 issues, 1310 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 1310 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/16810_Identity.csv\n",
      "Extraction complete for project Identity. Shape of data: (1310, 48)\n",
      "Data saved to: jira_extracted_data/Jira/16810_Identity.csv\n",
      "\n",
      "Processing project: Jira_Software_Cloud (ID: 18511)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (733 issues, 12733 total)\n",
      "Combining 11 processed batches...\n",
      "Processing batch file 1/11\n",
      "Processing batch file 2/11\n",
      "Processing batch file 3/11\n",
      "Processing batch file 4/11\n",
      "Processing batch file 5/11\n",
      "Processing batch file 6/11\n",
      "Processing batch file 7/11\n",
      "Processing batch file 8/11\n",
      "Processing batch file 9/11\n",
      "Processing batch file 10/11\n",
      "Processing batch file 11/11\n",
      "Combined DataFrame has 12733 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/18511_Jira_Software_Cloud.csv\n",
      "Extraction complete for project Jira_Software_Cloud. Shape of data: (12733, 49)\n",
      "Data saved to: jira_extracted_data/Jira/18511_Jira_Software_Cloud.csv\n",
      "\n",
      "Processing project: Jira_Service_Management_Cloud (ID: 18512)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (892 issues, 6892 total)\n",
      "Combining 6 processed batches...\n",
      "Processing batch file 1/6\n",
      "Processing batch file 2/6\n",
      "Processing batch file 3/6\n",
      "Processing batch file 4/6\n",
      "Processing batch file 5/6\n",
      "Processing batch file 6/6\n",
      "Combined DataFrame has 6892 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/18512_Jira_Service_Management_Cloud.csv\n",
      "Extraction complete for project Jira_Service_Management_Cloud. Shape of data: (6892, 49)\n",
      "Data saved to: jira_extracted_data/Jira/18512_Jira_Service_Management_Cloud.csv\n",
      "\n",
      "Processing project: Confluence_Cloud (ID: 18513)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (1200 issues, 15600 total)\n",
      "  - Processed batch 14 (1200 issues, 16800 total)\n",
      "  - Processed batch 15 (1200 issues, 18000 total)\n",
      "  - Processed batch 16 (1200 issues, 19200 total)\n",
      "  - Processed batch 17 (1200 issues, 20400 total)\n",
      "  - Processed batch 18 (1200 issues, 21600 total)\n",
      "  - Processed batch 19 (1200 issues, 22800 total)\n",
      "  - Processed batch 20 (1200 issues, 24000 total)\n",
      "  - Processed batch 21 (1115 issues, 25115 total)\n",
      "Combining 21 processed batches...\n",
      "Processing batch file 1/21\n",
      "Processing batch file 2/21\n",
      "Processing batch file 3/21\n",
      "Processing batch file 4/21\n",
      "Processing batch file 5/21\n",
      "Processing batch file 6/21\n",
      "Processing batch file 7/21\n",
      "Processing batch file 8/21\n",
      "Processing batch file 9/21\n",
      "Processing batch file 10/21\n",
      "Processing batch file 11/21\n",
      "Processing batch file 12/21\n",
      "Processing batch file 13/21\n",
      "Processing batch file 14/21\n",
      "Processing batch file 15/21\n",
      "Processing batch file 16/21\n",
      "Processing batch file 17/21\n",
      "Processing batch file 18/21\n",
      "Processing batch file 19/21\n",
      "Processing batch file 20/21\n",
      "Processing batch file 21/21\n",
      "Combined DataFrame has 25115 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/18513_Confluence_Cloud.csv\n",
      "Extraction complete for project Confluence_Cloud. Shape of data: (25115, 49)\n",
      "Data saved to: jira_extracted_data/Jira/18513_Confluence_Cloud.csv\n",
      "\n",
      "Processing project: Jira_Cloud (ID: 18514)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (1200 issues, 15600 total)\n",
      "  - Processed batch 14 (1200 issues, 16800 total)\n",
      "  - Processed batch 15 (1200 issues, 18000 total)\n",
      "  - Processed batch 16 (1200 issues, 19200 total)\n",
      "  - Processed batch 17 (1200 issues, 20400 total)\n",
      "  - Processed batch 18 (1200 issues, 21600 total)\n",
      "  - Processed batch 19 (1200 issues, 22800 total)\n",
      "  - Processed batch 20 (1200 issues, 24000 total)\n",
      "  - Processed batch 21 (1200 issues, 25200 total)\n",
      "  - Processed batch 22 (1200 issues, 26400 total)\n",
      "  - Processed batch 23 (1200 issues, 27600 total)\n",
      "  - Processed batch 24 (316 issues, 27916 total)\n",
      "Combining 24 processed batches...\n",
      "Processing batch file 1/24\n",
      "Processing batch file 2/24\n",
      "Processing batch file 3/24\n",
      "Processing batch file 4/24\n",
      "Processing batch file 5/24\n",
      "Processing batch file 6/24\n",
      "Processing batch file 7/24\n",
      "Processing batch file 8/24\n",
      "Processing batch file 9/24\n",
      "Processing batch file 10/24\n",
      "Processing batch file 11/24\n",
      "Processing batch file 12/24\n",
      "Processing batch file 13/24\n",
      "Processing batch file 14/24\n",
      "Processing batch file 15/24\n",
      "Processing batch file 16/24\n",
      "Processing batch file 17/24\n",
      "Processing batch file 18/24\n",
      "Processing batch file 19/24\n",
      "Processing batch file 20/24\n",
      "Processing batch file 21/24\n",
      "Processing batch file 22/24\n",
      "Processing batch file 23/24\n",
      "Processing batch file 24/24\n",
      "Combined DataFrame has 27916 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/18514_Jira_Cloud.csv\n",
      "Extraction complete for project Jira_Cloud. Shape of data: (27916, 50)\n",
      "Data saved to: jira_extracted_data/Jira/18514_Jira_Cloud.csv\n",
      "\n",
      "Processing project: Atlassian_Access (ID: 18910)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (527 issues, 527 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 527 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/18910_Atlassian_Access.csv\n",
      "Extraction complete for project Atlassian_Access. Shape of data: (527, 47)\n",
      "Data saved to: jira_extracted_data/Jira/18910_Atlassian_Access.csv\n",
      "\n",
      "Processing project: Migration_Platform (ID: 19710)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (786 issues, 786 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 786 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Jira/19710_Migration_Platform.csv\n",
      "Extraction complete for project Migration_Platform. Shape of data: (786, 48)\n",
      "Data saved to: jira_extracted_data/Jira/19710_Migration_Platform.csv\n",
      "\n",
      "Processing project: Atlassian_Product_Integrations (ID: 20010)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (438 issues, 438 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 438 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/20010_Atlassian_Product_Integrations.csv\n",
      "Extraction complete for project Atlassian_Product_Integrations. Shape of data: (438, 46)\n",
      "Data saved to: jira_extracted_data/Jira/20010_Atlassian_Product_Integrations.csv\n",
      "\n",
      "Processing project: Bitbucket_Cloud (ID: 20310)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (1200 issues, 14400 total)\n",
      "  - Processed batch 13 (1200 issues, 15600 total)\n",
      "  - Processed batch 14 (1200 issues, 16800 total)\n",
      "  - Processed batch 15 (1200 issues, 18000 total)\n",
      "  - Processed batch 16 (1200 issues, 19200 total)\n",
      "  - Processed batch 17 (1152 issues, 20352 total)\n",
      "Combining 17 processed batches...\n",
      "Processing batch file 1/17\n",
      "Processing batch file 2/17\n",
      "Processing batch file 3/17\n",
      "Processing batch file 4/17\n",
      "Processing batch file 5/17\n",
      "Processing batch file 6/17\n",
      "Processing batch file 7/17\n",
      "Processing batch file 8/17\n",
      "Processing batch file 9/17\n",
      "Processing batch file 10/17\n",
      "Processing batch file 11/17\n",
      "Processing batch file 12/17\n",
      "Processing batch file 13/17\n",
      "Processing batch file 14/17\n",
      "Processing batch file 15/17\n",
      "Processing batch file 16/17\n",
      "Processing batch file 17/17\n",
      "Combined DataFrame has 20352 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/20310_Bitbucket_Cloud.csv\n",
      "Extraction complete for project Bitbucket_Cloud. Shape of data: (20352, 48)\n",
      "Data saved to: jira_extracted_data/Jira/20310_Bitbucket_Cloud.csv\n",
      "\n",
      "Processing project: Server_Deployments_and_Scale (ID: 20410)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (58 issues, 58 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 58 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/20410_Server_Deployments_and_Scale.csv\n",
      "Extraction complete for project Server_Deployments_and_Scale. Shape of data: (58, 46)\n",
      "Data saved to: jira_extracted_data/Jira/20410_Server_Deployments_and_Scale.csv\n",
      "\n",
      "Processing project: Automation_for_Jira (ID: 20910)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (342 issues, 342 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 342 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/20910_Automation_for_Jira.csv\n",
      "Extraction complete for project Automation_for_Jira. Shape of data: (342, 47)\n",
      "Data saved to: jira_extracted_data/Jira/20910_Automation_for_Jira.csv\n",
      "\n",
      "Processing project: Atlassian_OAuth_2.0 (ID: 21312)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (4 issues, 4 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 4 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/21312_Atlassian_OAuth_2.0.csv\n",
      "Extraction complete for project Atlassian_OAuth_2.0. Shape of data: (4, 44)\n",
      "Data saved to: jira_extracted_data/Jira/21312_Atlassian_OAuth_2.0.csv\n",
      "\n",
      "Processing project: Jira_Work_Management_Cloud (ID: 21810)\n",
      "\n",
      "Extracting data from: Jira ...\n",
      "  - Processed batch 1 (136 issues, 136 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 136 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Jira/21810_Jira_Work_Management_Cloud.csv\n",
      "Extraction complete for project Jira_Work_Management_Cloud. Shape of data: (136, 46)\n",
      "Data saved to: jira_extracted_data/Jira/21810_Jira_Work_Management_Cloud.csv\n",
      "\n",
      "==================================================\n",
      "Processing repository: Hyperledger\n",
      "==================================================\n",
      "Fetching project IDs for Hyperledger...\n",
      "Found 32 projects in Hyperledger\n",
      "\n",
      "Processing project: Sawtooth (ID: 10001)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (463 issues, 1663 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 1663 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10001_Sawtooth.csv\n",
      "Extraction complete for project Sawtooth. Shape of data: (1663, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10001_Sawtooth.csv\n",
      "\n",
      "Processing project: Fabric (ID: 10002)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1200 issues, 2400 total)\n",
      "  - Processed batch 3 (1200 issues, 3600 total)\n",
      "  - Processed batch 4 (1200 issues, 4800 total)\n",
      "  - Processed batch 5 (1200 issues, 6000 total)\n",
      "  - Processed batch 6 (1200 issues, 7200 total)\n",
      "  - Processed batch 7 (1200 issues, 8400 total)\n",
      "  - Processed batch 8 (1200 issues, 9600 total)\n",
      "  - Processed batch 9 (1200 issues, 10800 total)\n",
      "  - Processed batch 10 (1200 issues, 12000 total)\n",
      "  - Processed batch 11 (1200 issues, 13200 total)\n",
      "  - Processed batch 12 (726 issues, 13926 total)\n",
      "Combining 12 processed batches...\n",
      "Processing batch file 1/12\n",
      "Processing batch file 2/12\n",
      "Processing batch file 3/12\n",
      "Processing batch file 4/12\n",
      "Processing batch file 5/12\n",
      "Processing batch file 6/12\n",
      "Processing batch file 7/12\n",
      "Processing batch file 8/12\n",
      "Processing batch file 9/12\n",
      "Processing batch file 10/12\n",
      "Processing batch file 11/12\n",
      "Processing batch file 12/12\n",
      "Combined DataFrame has 13926 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10002_Fabric.csv\n",
      "Extraction complete for project Fabric. Shape of data: (13926, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10002_Fabric.csv\n",
      "\n",
      "Processing project: Blockchain_Explorer (ID: 10100)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (844 issues, 844 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 844 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10100_Blockchain_Explorer.csv\n",
      "Extraction complete for project Blockchain_Explorer. Shape of data: (844, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10100_Blockchain_Explorer.csv\n",
      "\n",
      "Processing project: Cello (ID: 10200)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (717 issues, 717 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 717 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10200_Cello.csv\n",
      "Extraction complete for project Cello. Shape of data: (717, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10200_Cello.csv\n",
      "\n",
      "Processing project: Indy_Node (ID: 10303)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (1123 issues, 2323 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 2323 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10303_Indy_Node.csv\n",
      "Extraction complete for project Indy_Node. Shape of data: (2323, 51)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10303_Indy_Node.csv\n",
      "\n",
      "Processing project: Iroha (ID: 10304)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1172 issues, 1172 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 1172 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10304_Iroha.csv\n",
      "Extraction complete for project Iroha. Shape of data: (1172, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10304_Iroha.csv\n",
      "\n",
      "Processing project: Indy_SDK (ID: 10401)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (336 issues, 1536 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 1536 rows and 30 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10401_Indy_SDK.csv\n",
      "Extraction complete for project Indy_SDK. Shape of data: (1536, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10401_Indy_SDK.csv\n",
      "\n",
      "Processing project: Composer (ID: 10600)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (10 issues, 10 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 10 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10600_Composer.csv\n",
      "Extraction complete for project Composer. Shape of data: (10, 45)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10600_Composer.csv\n",
      "\n",
      "Processing project: Indy_Agent (ID: 10602)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (54 issues, 54 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 54 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10602_Indy_Agent.csv\n",
      "Extraction complete for project Indy_Agent. Shape of data: (54, 49)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10602_Indy_Agent.csv\n",
      "\n",
      "Processing project: Fabric_SDK_Node (ID: 10604)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (1200 issues, 1200 total)\n",
      "  - Processed batch 2 (411 issues, 1611 total)\n",
      "Combining 2 processed batches...\n",
      "Processing batch file 1/2\n",
      "Processing batch file 2/2\n",
      "Combined DataFrame has 1611 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10604_Fabric_SDK_Node.csv\n",
      "Extraction complete for project Fabric_SDK_Node. Shape of data: (1611, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10604_Fabric_SDK_Node.csv\n",
      "\n",
      "Processing project: Fabric_SDK_Java (ID: 10605)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (484 issues, 484 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 484 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10605_Fabric_SDK_Java.csv\n",
      "Extraction complete for project Fabric_SDK_Java. Shape of data: (484, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10605_Fabric_SDK_Java.csv\n",
      "\n",
      "Processing project: Fabric_SDK_REST (ID: 10606)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (10 issues, 10 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 10 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10606_Fabric_SDK_REST.csv\n",
      "Extraction complete for project Fabric_SDK_REST. Shape of data: (10, 46)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10606_Fabric_SDK_REST.csv\n",
      "\n",
      "Processing project: Fabric_CA (ID: 10607)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (855 issues, 855 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 855 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10607_Fabric_CA.csv\n",
      "Extraction complete for project Fabric_CA. Shape of data: (855, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10607_Fabric_CA.csv\n",
      "\n",
      "Processing project: Fabric_Baseimage (ID: 10608)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (139 issues, 139 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 139 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10608_Fabric_Baseimage.csv\n",
      "Extraction complete for project Fabric_Baseimage. Shape of data: (139, 49)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10608_Fabric_Baseimage.csv\n",
      "\n",
      "Processing project: Fabric_Chaintool (ID: 10609)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (58 issues, 58 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 58 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10609_Fabric_Chaintool.csv\n",
      "Extraction complete for project Fabric_Chaintool. Shape of data: (58, 51)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10609_Fabric_Chaintool.csv\n",
      "\n",
      "Processing project: Fabric_SDK_Go (ID: 10610)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (980 issues, 980 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 980 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10610_Fabric_SDK_Go.csv\n",
      "Extraction complete for project Fabric_SDK_Go. Shape of data: (980, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10610_Fabric_SDK_Go.csv\n",
      "\n",
      "Processing project: Fabric_SDK_Python (ID: 10611)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (263 issues, 263 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 263 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10611_Fabric_SDK_Python.csv\n",
      "Extraction complete for project Fabric_SDK_Python. Shape of data: (263, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10611_Fabric_SDK_Python.csv\n",
      "\n",
      "Processing project: Fabric_Lib_Go (ID: 10900)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (10 issues, 10 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 10 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10900_Fabric_Lib_Go.csv\n",
      "Extraction complete for project Fabric_Lib_Go. Shape of data: (10, 48)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10900_Fabric_Lib_Go.csv\n",
      "\n",
      "Processing project: Grid (ID: 10902)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (103 issues, 103 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 103 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10902_Grid.csv\n",
      "Extraction complete for project Grid. Shape of data: (103, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10902_Grid.csv\n",
      "\n",
      "Processing project: Ursa (ID: 10904)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (9 issues, 9 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 9 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/10904_Ursa.csv\n",
      "Extraction complete for project Ursa. Shape of data: (9, 47)\n",
      "Data saved to: jira_extracted_data/Hyperledger/10904_Ursa.csv\n",
      "\n",
      "Processing project: Fabric_Gateway_Java (ID: 11000)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (110 issues, 110 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 110 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11000_Fabric_Gateway_Java.csv\n",
      "Extraction complete for project Fabric_Gateway_Java. Shape of data: (110, 51)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11000_Fabric_Gateway_Java.csv\n",
      "\n",
      "Processing project: Hyperledger_Aries (ID: 11002)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (7 issues, 7 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 7 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11002_Hyperledger_Aries.csv\n",
      "Extraction complete for project Hyperledger_Aries. Shape of data: (7, 46)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11002_Hyperledger_Aries.csv\n",
      "\n",
      "Processing project: Transact (ID: 11003)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (56 issues, 56 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 56 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11003_Transact.csv\n",
      "Extraction complete for project Transact. Shape of data: (56, 48)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11003_Transact.csv\n",
      "\n",
      "Processing project: Besu (ID: 11100)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (203 issues, 203 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 203 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11100_Besu.csv\n",
      "Extraction complete for project Besu. Shape of data: (203, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11100_Besu.csv\n",
      "\n",
      "Processing project: Avalon (ID: 11200)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (6 issues, 6 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 6 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11200_Avalon.csv\n",
      "Extraction complete for project Avalon. Shape of data: (6, 45)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11200_Avalon.csv\n",
      "\n",
      "Processing project: Fabric_Contract_API_Go (ID: 11206)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (34 issues, 34 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 34 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11206_Fabric_Contract_API_Go.csv\n",
      "Extraction complete for project Fabric_Contract_API_Go. Shape of data: (34, 51)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11206_Fabric_Contract_API_Go.csv\n",
      "\n",
      "Processing project: Fabric_Chaincode_Node (ID: 11207)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (438 issues, 438 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 438 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11207_Fabric_Chaincode_Node.csv\n",
      "Extraction complete for project Fabric_Chaincode_Node. Shape of data: (438, 53)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11207_Fabric_Chaincode_Node.csv\n",
      "\n",
      "Processing project: Fabric_Chaincode_Java (ID: 11208)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (300 issues, 300 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 300 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11208_Fabric_Chaincode_Java.csv\n",
      "Extraction complete for project Fabric_Chaincode_Java. Shape of data: (300, 51)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11208_Fabric_Chaincode_Java.csv\n",
      "\n",
      "Processing project: Fabric_Chaincode_Go (ID: 11209)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (20 issues, 20 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 20 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11209_Fabric_Chaincode_Go.csv\n",
      "Extraction complete for project Fabric_Chaincode_Go. Shape of data: (20, 49)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11209_Fabric_Chaincode_Go.csv\n",
      "\n",
      "Processing project: Fabric_Chaincode_EVM (ID: 11210)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (150 issues, 150 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 150 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11210_Fabric_Chaincode_EVM.csv\n",
      "Extraction complete for project Fabric_Chaincode_EVM. Shape of data: (150, 52)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11210_Fabric_Chaincode_EVM.csv\n",
      "\n",
      "Processing project: Fabric_Chaincode_Wasm (ID: 11500)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (31 issues, 31 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 31 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11500_Fabric_Chaincode_Wasm.csv\n",
      "Extraction complete for project Fabric_Chaincode_Wasm. Shape of data: (31, 46)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11500_Fabric_Chaincode_Wasm.csv\n",
      "\n",
      "Processing project: Fabric_Gateway (ID: 11600)\n",
      "\n",
      "Extracting data from: Hyperledger ...\n",
      "  - Processed batch 1 (24 issues, 24 total)\n",
      "Combining 1 processed batches...\n",
      "Processing batch file 1/1\n",
      "Combined DataFrame has 24 rows and 29 columns.\n",
      "Saving data to jira_extracted_data/Hyperledger/11600_Fabric_Gateway.csv\n",
      "Extraction complete for project Fabric_Gateway. Shape of data: (24, 48)\n",
      "Data saved to: jira_extracted_data/Hyperledger/11600_Fabric_Gateway.csv\n",
      "\n",
      "All repositories processed.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import gc\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def extract_jira_with_features(jira_name, project_name=None, batch_size=1200, output_file=None, max_batches=None):\n",
    "    \"\"\"\n",
    "    Extract Jira data with basic feature engineering.\n",
    "    \n",
    "    Parameters:\n",
    "        jira_name (str): Name of the Jira repository\n",
    "        project_name (list): Optional list of project IDs to filter\n",
    "        batch_size (int): Size of batches for processing\n",
    "        output_file (str): Optional path to save the output CSV\n",
    "        max_batches (int): Optional maximum number of batches to process\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with extracted features\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting data from: {jira_name} ...\")\n",
    "    \n",
    "    # Get total count\n",
    "    total_issues = db[jira_name].count_documents({})\n",
    "    if total_issues == 0:\n",
    "        print(f\" No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # Define fields we need\n",
    "    needed_fields = {\n",
    "        # Essential fields\n",
    "        \"id\": 1,\n",
    "        \"key\": 1,\n",
    "        \n",
    "        # Classification fields\n",
    "        \"fields.issuetype.name\": 1,\n",
    "        \"fields.issuetype.id\": 1,\n",
    "        \"fields.priority.name\": 1, \n",
    "        \"fields.priority.id\": 1,\n",
    "        \"fields.status.name\": 1,\n",
    "        \n",
    "        # Temporal data\n",
    "        \"fields.created\": 1,\n",
    "        \"fields.updated\": 1,\n",
    "        \"fields.resolutiondate\": 1,\n",
    "        \n",
    "        # Estimation fields\n",
    "        \"fields.timeoriginalestimate\": 1,\n",
    "        \"fields.timeestimate\": 1,\n",
    "        \"fields.timespent\": 1,\n",
    "        \n",
    "        # Issue links\n",
    "        \"fields.issuelinks\": 1,\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\": 1,\n",
    "        \"fields.project.key\": 1,\n",
    "        \"fields.project.name\": 1,\n",
    "\n",
    "        # Team context\n",
    "        \"fields.asignee\": 1,\n",
    "        \"fields.creator\": 1\n",
    "    }\n",
    "    \n",
    "    # Create query filter\n",
    "    query_filter = {}\n",
    "    if project_name:\n",
    "        if isinstance(project_name, list):\n",
    "            query_filter[\"fields.project.id\"] = {\"$in\": project_name}\n",
    "        else:\n",
    "            query_filter[\"fields.project.id\"] = project_name\n",
    "    \n",
    "    # Process in batches and store in CSV for memory efficiency\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    batch_files = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    try:\n",
    "        batch_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Check if we've reached the maximum number of batches\n",
    "            if max_batches is not None and batch_count >= max_batches:\n",
    "                print(f\"Reached maximum batch limit ({max_batches}). Stopping extraction.\")\n",
    "                break\n",
    "                \n",
    "            # Create a fresh cursor for each batch\n",
    "            cursor = db[jira_name].find(\n",
    "                query_filter, \n",
    "                needed_fields\n",
    "            ).skip(batch_count * batch_size).limit(batch_size)\n",
    "            \n",
    "            batch = list(cursor)\n",
    "            if not batch:\n",
    "                break\n",
    "            \n",
    "            # Process batch\n",
    "            batch_df = pd.json_normalize(batch, sep='.')\n",
    "            \n",
    "            # Fix data types as in your notebook\n",
    "            batch_df = fix_data_types(batch_df)\n",
    "            \n",
    "            # Add repository name\n",
    "            batch_df['repository'] = jira_name\n",
    "            \n",
    "            # Save to CSV\n",
    "            batch_file = os.path.join(temp_dir, f\"{jira_name}_batch_{batch_count}.csv\")\n",
    "            batch_df.to_csv(batch_file, index=False)\n",
    "            batch_files.append(batch_file)\n",
    "            \n",
    "            batch_count += 1\n",
    "            total_processed += len(batch)\n",
    "            print(f\"  - Processed batch {batch_count} ({len(batch)} issues, {total_processed} total)\")\n",
    "            \n",
    "            # Free memory\n",
    "            del batch\n",
    "            del batch_df\n",
    "            gc.collect()\n",
    "        \n",
    "        if not batch_files:\n",
    "            print(\"No data collected.\")\n",
    "            return None\n",
    "        \n",
    "        # Read all CSV files and combine with better memory management\n",
    "        print(f\"Combining {len(batch_files)} processed batches...\")\n",
    "        final_df = None\n",
    "        \n",
    "        # Process one batch at a time to avoid loading all data into memory at once\n",
    "        for i, file in enumerate(batch_files):\n",
    "            try:\n",
    "                print(f\"Processing batch file {i+1}/{len(batch_files)}\")\n",
    "                chunk = pd.read_csv(file, low_memory=False)\n",
    "                \n",
    "                if final_df is None:\n",
    "                    final_df = chunk\n",
    "                else:\n",
    "                    final_df = pd.concat([final_df, chunk], ignore_index=True)\n",
    "                    \n",
    "                # Explicitly delete the chunk to free memory\n",
    "                del chunk\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading batch file {file}: {e}\")\n",
    "        \n",
    "        if final_df is None or len(final_df) == 0:\n",
    "            print(\"No valid data after combining batches.\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Combined DataFrame has {len(final_df)} rows and {len(final_df.columns)} columns.\")\n",
    "        \n",
    "        # --- Feature Engineering ---\n",
    "        \n",
    "        # 1. Process date fields using functions from your notebook\n",
    "        def parse_date_str(x):\n",
    "            \"\"\"\n",
    "            Parse a date string. If the string is \"Missing\", empty, or cannot be parsed, return pd.NaT.\n",
    "            \"\"\"\n",
    "            if pd.isnull(x):\n",
    "                return pd.NaT\n",
    "            s = str(x).strip()\n",
    "            if s.lower() == \"missing\" or s == \"\":\n",
    "                return pd.NaT\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def convert_date_columns_dateparser(df, date_columns):\n",
    "            \"\"\"Convert date columns from string to datetime using custom parse_date_str\"\"\"\n",
    "            for col in date_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].apply(parse_date_str)\n",
    "                    df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "            return df\n",
    "        \n",
    "        def drop_invalid_dates(df, date_columns):\n",
    "            \"\"\"Drop rows where any of the specified date columns are NaT\"\"\"\n",
    "            return df.dropna(subset=date_columns)\n",
    "        \n",
    "        # Apply date processing functions\n",
    "        date_columns = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\"]\n",
    "        final_df = convert_date_columns_dateparser(final_df, date_columns)\n",
    "        \n",
    "        # We'll keep all rows for now, even those with missing dates\n",
    "        # Comment this in if you want to drop rows with invalid dates:\n",
    "        # final_df = drop_invalid_dates(final_df, date_columns)\n",
    "        \n",
    "        # 2. Extract priority information into separate columns\n",
    "        if 'fields.priority.name' in final_df.columns:\n",
    "            final_df['priority_name'] = final_df['fields.priority.name']\n",
    "        if 'fields.priority.id' in final_df.columns:\n",
    "            final_df['priority_id'] = final_df['fields.priority.id']\n",
    "        \n",
    "        # 3. Extract issue type and status information into separate columns\n",
    "        if 'fields.issuetype.name' in final_df.columns:\n",
    "            final_df['issue_type'] = final_df['fields.issuetype.name']\n",
    "        if 'fields.issuetype.id' in final_df.columns:\n",
    "            final_df['issue_type_id'] = final_df['fields.issuetype.id']\n",
    "        if 'fields.status.name' in final_df.columns:\n",
    "            final_df['status'] = final_df['fields.status.name']\n",
    "        \n",
    "        # Add is_completed field based on status\n",
    "        if 'status' in final_df.columns:\n",
    "            # Common completion status terms (may need adjustment based on your workflow)\n",
    "            completed_statuses = ['Done', 'Closed', 'Resolved', 'Complete', 'Completed', 'Fixed', \n",
    "                                 'Finished', 'Released', 'Delivered', 'Verified']\n",
    "            \n",
    "            # Create the is_completed flag (case-insensitive matching)\n",
    "            final_df['is_completed'] = final_df['status'].apply(\n",
    "                lambda x: 1 if any(s.lower() in str(x).lower() for s in completed_statuses) else 0\n",
    "            )\n",
    "        \n",
    "        # 4. Create binary features for each issue type (renamed to type_{type})\n",
    "        if 'issue_type' in final_df.columns:\n",
    "            issue_types = final_df['issue_type'].dropna().unique()\n",
    "            for issue_type in issue_types:\n",
    "                safe_name = str(issue_type).lower().replace(' ', '_').replace('-', '_')\n",
    "                final_df[f'type_{safe_name}'] = (final_df['issue_type'] == issue_type).astype(int)\n",
    "        \n",
    "        # 5. Create binary features for each priority level (renamed to priority_{priority})\n",
    "        if 'priority_name' in final_df.columns:\n",
    "            priorities = final_df['priority_name'].dropna().unique()\n",
    "            for priority in priorities:\n",
    "                if pd.notna(priority):\n",
    "                    safe_name = str(priority).lower().replace(' ', '_').replace('-', '_')\n",
    "                    final_df[f'priority_{safe_name}'] = (final_df['priority_name'] == priority).astype(int)\n",
    "        \n",
    "        # 6. Process issue links\n",
    "        if 'fields.issuelinks' in final_df.columns:\n",
    "            def parse_issuelinks(issuelinks):\n",
    "                \"\"\"Parse the issuelinks field and return counts by type\"\"\"\n",
    "                if pd.isna(issuelinks) or not issuelinks:\n",
    "                    return {'inward_count': 0, 'outward_count': 0}\n",
    "                \n",
    "                # Convert string representation to list if needed\n",
    "                if isinstance(issuelinks, str):\n",
    "                    try:\n",
    "                        issuelinks = ast.literal_eval(issuelinks)\n",
    "                    except:\n",
    "                        return {'inward_count': 0, 'outward_count': 0}\n",
    "                \n",
    "                # Count inward and outward links\n",
    "                inward_count = sum(1 for link in issuelinks if 'inwardIssue' in link)\n",
    "                outward_count = sum(1 for link in issuelinks if 'outwardIssue' in link)\n",
    "                \n",
    "                return {\n",
    "                    'inward_count': inward_count,\n",
    "                    'outward_count': outward_count\n",
    "                }\n",
    "            \n",
    "            # Apply function and expand results\n",
    "            link_counts = final_df['fields.issuelinks'].apply(parse_issuelinks)\n",
    "            link_df = pd.json_normalize(link_counts)\n",
    "            \n",
    "            # Add columns to main dataframe\n",
    "            for col in link_df.columns:\n",
    "                final_df[col] = link_df[col]\n",
    "        \n",
    "        # 7. Create temporal features focusing on task estimation\n",
    "        if all(col in final_df.columns for col in ['fields.created', 'fields.resolutiondate']):\n",
    "            # Ensure both dates are timezone-aware and in UTC before calculations\n",
    "            final_df['fields.created'] = pd.to_datetime(final_df['fields.created'], utc=True)\n",
    "            final_df['fields.resolutiondate'] = pd.to_datetime(final_df['fields.resolutiondate'], utc=True)\n",
    "            \n",
    "            # Calculate time to resolution in days (only for resolved issues)\n",
    "            # Note: resolution_time_days will be NaN for unresolved issues (expected behavior)\n",
    "            resolved_mask = ~final_df['fields.resolutiondate'].isna()\n",
    "            \n",
    "            # Add resolved status flag\n",
    "            final_df['is_resolved'] = resolved_mask.astype(int)\n",
    "            \n",
    "            # Calculate resolution time only for resolved issues\n",
    "            final_df['resolution_time_days'] = np.nan\n",
    "            final_df.loc[resolved_mask, 'resolution_time_days'] = (\n",
    "                final_df.loc[resolved_mask, 'fields.resolutiondate'] - \n",
    "                final_df.loc[resolved_mask, 'fields.created']\n",
    "            ).dt.total_seconds() / (24 * 3600)\n",
    "            \n",
    "            # Handle negative values (data errors)\n",
    "            final_df.loc[final_df['resolution_time_days'] < 0, 'resolution_time_days'] = np.nan\n",
    "            \n",
    "            # Calculate age in days for all issues (resolved and unresolved)\n",
    "            current_time = pd.Timestamp.now(tz='UTC')\n",
    "            final_df['age_days'] = (\n",
    "                current_time - final_df['fields.created']\n",
    "            ).dt.total_seconds() / (24 * 3600)\n",
    "        \n",
    "        # 8. Estimation accuracy features\n",
    "        if all(col in final_df.columns for col in ['fields.timeoriginalestimate', 'fields.timespent']):\n",
    "            # Calculate ratio of time spent to estimated time (both in seconds)\n",
    "            mask = (final_df['fields.timeoriginalestimate'] > 0) & (final_df['fields.timespent'] > 0)\n",
    "            final_df['estimation_ratio'] = np.nan\n",
    "            final_df.loc[mask, 'estimation_ratio'] = (\n",
    "                final_df.loc[mask, 'fields.timespent'] / \n",
    "                final_df.loc[mask, 'fields.timeoriginalestimate']\n",
    "            )\n",
    "        \n",
    "        # Save the output if requested\n",
    "        if output_file:\n",
    "            print(f\"Saving data to {output_file}\")\n",
    "            final_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing temp file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(temp_dir):\n",
    "                os.rmdir(temp_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing temp directory {temp_dir}: {e}\")\n",
    "\n",
    "\n",
    "def extract_all_repositories(repositories=None, output_dir=\"jira_extracted_data\", max_batches=None):\n",
    "    \"\"\"\n",
    "    Extract data from multiple repositories, creating separate files for each project.\n",
    "    \n",
    "    Parameters:\n",
    "        repositories (list): List of repository names to process\n",
    "        output_dir (str): Directory to save output files\n",
    "        max_batches (int): Maximum number of batches to process per project\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use all repositories if none specified\n",
    "    if repositories is None:\n",
    "        repositories = list(jira_data_sources.keys())\n",
    "    \n",
    "    for repository in repositories:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing repository: {repository}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # First, extract all project IDs for the repository\n",
    "        print(f\"Fetching project IDs for {repository}...\")\n",
    "        try:\n",
    "            project_ids = list(db[repository].distinct(\"fields.project.id\"))\n",
    "            print(f\"Found {len(project_ids)} projects in {repository}\")\n",
    "            \n",
    "            # Create a repo-specific directory\n",
    "            repo_dir = os.path.join(output_dir, repository)\n",
    "            os.makedirs(repo_dir, exist_ok=True)\n",
    "            \n",
    "            # For each project ID, extract data and save to a separate CSV\n",
    "            for project_id in project_ids:\n",
    "                # Get the project name (for a more descriptive filename)\n",
    "                project_name_doc = db[repository].find_one(\n",
    "                    {\"fields.project.id\": project_id}, \n",
    "                    {\"fields.project.name\": 1}\n",
    "                )\n",
    "                \n",
    "                project_name = \"unknown\"\n",
    "                if project_name_doc and \"fields\" in project_name_doc and \"project\" in project_name_doc[\"fields\"]:\n",
    "                    project_name = project_name_doc[\"fields\"][\"project\"].get(\"name\", \"unknown\")\n",
    "                    # Make the project name safe for a filename\n",
    "                    project_name = project_name.replace(' ', '_').replace('/', '_').replace('\\\\', '_')\n",
    "                \n",
    "                print(f\"\\nProcessing project: {project_name} (ID: {project_id})\")\n",
    "                \n",
    "                # Create a unique output filename\n",
    "                output_file = os.path.join(repo_dir, f\"{project_id}_{project_name}.csv\")\n",
    "                \n",
    "                # Extract data for this project\n",
    "                df = extract_jira_with_features(\n",
    "                    jira_name=repository,\n",
    "                    project_name=project_id,\n",
    "                    output_file=output_file,\n",
    "                    max_batches=max_batches\n",
    "                )\n",
    "                \n",
    "                if df is not None:\n",
    "                    print(f\"Extraction complete for project {project_name}. Shape of data: {df.shape}\")\n",
    "                    print(f\"Data saved to: {output_file}\")\n",
    "                else:\n",
    "                    print(f\"Extraction failed or no data found for project {project_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing repository {repository}: {e}\")\n",
    "    \n",
    "    print(\"\\nAll repositories processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Option to specify repositories to process\n",
    "    # \"MariaDB\", \"Mojang\"\n",
    "    repositories_to_process = [\"Apache\"]  # Change this list as needed\n",
    "    \n",
    "    # Use the helper function to process all repositories\n",
    "    extract_all_repositories(\n",
    "        repositories=repositories_to_process,\n",
    "        output_dir=\"jira_extracted_data\",\n",
    "        max_batches=None  # Set to a number to limit batches or None for all\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./jira_extracted_data/MongoDB/10000_Core_Server.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
