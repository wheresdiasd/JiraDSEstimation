{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fad86-86c9-43bc-a9db-9d8124c3a830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0096d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 09:12:11,550 - ERROR    - Exception occurred while processing request: field larger than field limit (131072)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\", line 805, in _next_iter_line\n",
      "    line = next(self.data)\n",
      "_csv.Error: field larger than field limit (131072)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3968, in upload\n",
      "    df = pd.read_csv(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 626, in _read\n",
      "    return parser.read(nrows)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1923, in read\n",
      "    ) = self._engine.read(  # type: ignore[attr-defined]\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\", line 252, in read\n",
      "    content = self._get_lines(rows)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\", line 1140, in _get_lines\n",
      "    next_row = self._next_iter_line(row_num=self.pos + rows + 1)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\", line 834, in _next_iter_line\n",
      "    self._alert_malformed(msg, row_num)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/pandas/io/parsers/python_parser.py\", line 781, in _alert_malformed\n",
      "    raise ParserError(msg)\n",
      "pandas.errors.ParserError: field larger than field limit (131072)\n",
      "2025-03-14 09:12:11,616 - ERROR    - Exception occurred while processing request: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3961, in upload\n",
      "    if not request.files:\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/utils.py\", line 107, in __get__\n",
      "    value = self.fget(obj)  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 494, in files\n",
      "    self._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/wrappers.py\", line 115, in _load_form_data\n",
      "    super()._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 268, in _load_form_data\n",
      "    data = parser.parse(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 242, in parse\n",
      "    return parse_func(stream, mimetype, content_length, options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 267, in _parse_multipart\n",
      "    form, files = parser.parse(stream, boundary, content_length)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 368, in parse\n",
      "    for data in _chunk_iter(stream.read, self.buffer_size):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 423, in _chunk_iter\n",
      "    data = read(size)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
      "    self.on_disconnect()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
      "    raise ClientDisconnected()\n",
      "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "2025-03-14 09:12:11,616 - ERROR    - Exception occurred while processing request: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3961, in upload\n",
      "    if not request.files:\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/utils.py\", line 107, in __get__\n",
      "    value = self.fget(obj)  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 494, in files\n",
      "    self._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/wrappers.py\", line 115, in _load_form_data\n",
      "    super()._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 268, in _load_form_data\n",
      "    data = parser.parse(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 242, in parse\n",
      "    return parse_func(stream, mimetype, content_length, options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 267, in _parse_multipart\n",
      "    form, files = parser.parse(stream, boundary, content_length)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 368, in parse\n",
      "    for data in _chunk_iter(stream.read, self.buffer_size):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 423, in _chunk_iter\n",
      "    data = read(size)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
      "    self.on_disconnect()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
      "    raise ClientDisconnected()\n",
      "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "2025-03-14 09:12:11,630 - ERROR    - Exception occurred while processing request: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3961, in upload\n",
      "    if not request.files:\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/utils.py\", line 107, in __get__\n",
      "    value = self.fget(obj)  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 494, in files\n",
      "    self._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/wrappers.py\", line 115, in _load_form_data\n",
      "    super()._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 268, in _load_form_data\n",
      "    data = parser.parse(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 242, in parse\n",
      "    return parse_func(stream, mimetype, content_length, options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 267, in _parse_multipart\n",
      "    form, files = parser.parse(stream, boundary, content_length)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 368, in parse\n",
      "    for data in _chunk_iter(stream.read, self.buffer_size):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 423, in _chunk_iter\n",
      "    data = read(size)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
      "    self.on_disconnect()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
      "    raise ClientDisconnected()\n",
      "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "2025-03-14 09:12:11,645 - ERROR    - Exception occurred while processing request: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3961, in upload\n",
      "    if not request.files:\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/utils.py\", line 107, in __get__\n",
      "    value = self.fget(obj)  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 494, in files\n",
      "    self._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/wrappers.py\", line 115, in _load_form_data\n",
      "    super()._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 268, in _load_form_data\n",
      "    data = parser.parse(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 242, in parse\n",
      "    return parse_func(stream, mimetype, content_length, options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 267, in _parse_multipart\n",
      "    form, files = parser.parse(stream, boundary, content_length)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 368, in parse\n",
      "    for data in _chunk_iter(stream.read, self.buffer_size):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 423, in _chunk_iter\n",
      "    data = read(size)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
      "    self.on_disconnect()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
      "    raise ClientDisconnected()\n",
      "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "2025-03-14 09:12:11,646 - ERROR    - Exception occurred while processing request: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 120, in _handle_exceptions\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/views.py\", line 3961, in upload\n",
      "    if not request.files:\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/utils.py\", line 107, in __get__\n",
      "    value = self.fget(obj)  # type: ignore\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 494, in files\n",
      "    self._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/flask/wrappers.py\", line 115, in _load_form_data\n",
      "    super()._load_form_data()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wrappers/request.py\", line 268, in _load_form_data\n",
      "    data = parser.parse(\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 242, in parse\n",
      "    return parse_func(stream, mimetype, content_length, options)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 267, in _parse_multipart\n",
      "    form, files = parser.parse(stream, boundary, content_length)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 368, in parse\n",
      "    for data in _chunk_iter(stream.read, self.buffer_size):\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/formparser.py\", line 423, in _chunk_iter\n",
      "    data = read(size)\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
      "    self.on_disconnect()\n",
      "  File \"/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
      "    raise ClientDisconnected()\n",
      "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Hyperledger ...\n",
      "\n",
      "Processing repository: IntelDAOS ...\n",
      "\n",
      "Processing repository: MariaDB ...\n",
      "\n",
      "Processing repository: Mojang ...\n",
      "\n",
      "Processing repository: MongoDB ...\n",
      "\n",
      "Processing repository: Qt ...\n",
      "\n",
      "Processing repository: RedHat ...\n",
      "\n",
      "Processing repository: Sakai ...\n",
      "\n",
      "Processing repository: Sonatype ...\n",
      "\n",
      "Processing repository: Spring ...\n",
      "Found 9474 total issues in 'IntelDAOS'. Analyzing projects...\n",
      "Found 2 projects in repository.Found 28146 total issues in 'Hyperledger'. Analyzing projects...\n",
      "\n",
      "Project distribution: 0 small, 1 medium, 1 large\n",
      "Selected medium project: CaRT with 941 issues\n",
      "Added additional project: DAOS with 8533 issues\n",
      "Fetching 941 issues for project ID 10001...\n",
      "Found 31229 total issues in 'MariaDB'. Analyzing projects...\n",
      "Found 32 projects in repository.\n",
      "Project distribution: 30 small, 1 medium, 1 large\n",
      "Selected medium project: Indy Node with 2323 issues\n",
      "Selected small project: Sawtooth with 1663 issues\n",
      "Selected small project: Fabric SDK Node with 1611 issues\n",
      "Selected small project: Indy SDK with 1536 issues\n",
      "Selected small project: Iroha with 1172 issues\n",
      "Fetching 2323 issues for project ID 10303...\n",
      "  - Processed batch 1 (941 issues, 941/941)\n",
      "Fetching 8533 issues for project ID 10010...\n",
      "Found 69156 total issues in 'Spring'. Analyzing projects...\n",
      "Found 50550 total issues in 'Sakai'. Analyzing projects...\n",
      "Found 11 projects in repository.\n",
      "Project distribution: 8 small, 2 medium, 1 large\n",
      "Selected medium project: MariaDB ColumnStore with 3325 issues\n",
      "Selected medium project: MariaDB MaxScale with 3333 issues\n",
      "Selected small project: MariaDB Connector/J with 813 issues\n",
      "Selected small project: MariaDB Connector/C with 465 issues\n",
      "Selected small project: MariaDB Connector/ODBC with 288 issues\n",
      "Selected small project: MariaDB Foundation Development with 206 issues\n",
      "Selected small project: MariaDB Connector/node.js with 156 issues\n",
      "Fetching 3325 issues for project ID 10902...\n",
      "  - Processed batch 1 (1200 issues, 1200/2323)Found 87284 total issues in 'Sonatype'. Analyzing projects...\n",
      "\n",
      "  - Processed batch 1 (1200 issues, 1200/8533)\n",
      "  - Processed batch 1 (1200 issues, 1200/3325)\n",
      "  - Processed batch 2 (1123 issues, 2323/2323)\n",
      "Fetching 1663 issues for project ID 10001...\n",
      "  - Processed batch 2 (1200 issues, 2400/8533)\n",
      "  - Processed batch 1 (1200 issues, 1200/1663)\n",
      "  - Processed batch 2 (1200 issues, 2400/3325)\n",
      "Found 53 projects in repository.\n",
      "Project distribution: 52 small, 0 medium, 1 large\n",
      "Selected small project: Contrib: Evaluation System with 1577 issues\n",
      "Selected small project: Contrib: Gradebook2 with 1433 issues\n",
      "Selected small project: Dashboard with 400 issues\n",
      "Selected small project: Sakai Production/Pilot Deployments with 372 issues\n",
      "Selected small project: Contrib: SMS with 296 issues\n",
      "Selected small project: Contrib: Qualtrics LTI with 260 issues\n",
      "Selected small project: Contrib: TurnItIn with 250 issues\n",
      "Selected small project: Contrib: Sakai Kaltura Extension with 250 issues\n",
      "Selected small project: Contrib: QNA with 226 issues\n",
      "Selected small project: Contrib: Textbook with 222 issues\n",
      "Selected small project: Contrib: Clog with 195 issues\n",
      "Selected small project: Contrib: SCORM Player with 178 issues\n",
      "Selected small project: Contrib: BlogWow with 167 issues\n",
      "Selected small project: Contrib: Big Blue Button with 159 issues\n",
      "Selected small project: Contrib: Goal Management with 145 issues\n",
      "Selected small project: Contrib: Image Gallery with 111 issues\n",
      "Selected small project: Contrib: Sakaibrary with 111 issues\n",
      "Selected small project: Contrib: Yaft with 77 issues\n",
      "Selected small project: Contrib: Timeline with 75 issues\n",
      "Selected small project: Contrib: News Feeds with 64 issues\n",
      "Selected small project: Contrib: Checklist with 48 issues\n",
      "Selected small project: Contrib: Docs (New Resources) with 39 issues\n",
      "Selected small project: FARM with 39 issues\n",
      "Selected small project: Trinity with 38 issues\n",
      "Selected small project: Contrib: Sakai Maps with 33 issues\n",
      "Selected small project: Sakai Maven Plugin with 28 issues\n",
      "Selected small project: Contrib: Resources Viewer with 27 issues\n",
      "Selected small project: Contrib: Elluminate with 26 issues\n",
      "Selected small project: Contrib: XSD Weaver with 25 issues\n",
      "Selected small project: Contrib: Roleplay with 24 issues\n",
      "Selected small project: Contrib: Skin Manager with 23 issues\n",
      "Selected small project: Contrib: iTunesU with 22 issues\n",
      "Selected small project: Learning Analytics Initiative with 21 issues\n",
      "Selected small project: Contrib: Configuration Viewer with 21 issues\n",
      "Selected small project: RSF with 18 issues\n",
      "Selected small project: Contrib: Certification with 18 issues\n",
      "Selected small project: Contrib: Markup with 17 issues\n",
      "Selected small project: Contrib: PageOrderHelper with 17 issues\n",
      "Selected small project: Contrib: Sakora with 16 issues\n",
      "Selected small project: Contrib: MailArchive Wow! with 15 issues\n",
      "Selected small project: Contrib: Homepage Tool with 15 issues\n",
      "Selected small project: Contrib: AdminX and AdminLite  with 14 issues\n",
      "Selected small project: Contrib: Generic DAO with 14 issues\n",
      "Selected small project: Contrib: OCW with 12 issues\n",
      "Selected small project: Contrib: Wicket with 12 issues\n",
      "Selected small project: Contrib: LAMS with 10 issues\n",
      "Selected small project: Contrib: Web Conferencing with 9 issues\n",
      "Selected small project: Contrib: evalport with 9 issues\n",
      "Selected small project: Sakai Maven Archetypes with 6 issues\n",
      "Selected small project: Contrib: ImageQuiz with 6 issues\n",
      "Selected small project: Contrib: Sousa with 5 issues\n",
      "Selected small project: A11Y Testing Site with 4 issues\n",
      "Added additional project: Sakai with 43351 issues\n",
      "Fetching 1577 issues for project ID 10013...\n",
      "Found 80 projects in repository.\n",
      "Project distribution: 77 small, 2 medium, 1 large\n",
      "Selected large project: Spring Framework with 17413 issues\n",
      "Selected small project: Spring Roo with 3987 issues\n",
      "Fetching 17413 issues for project ID 10000...\n",
      "  - Processed batch 2 (463 issues, 1663/1663)\n",
      "Fetching 1611 issues for project ID 10604...\n",
      "Found 148579 total issues in 'Qt'. Analyzing projects...\n",
      "  - Processed batch 3 (1200 issues, 3600/8533)\n",
      "  - Processed batch 3 (925 issues, 3325/3325)\n",
      "Fetching 3333 issues for project ID 10600...\n",
      "Found 5 projects in repository.\n",
      "Project distribution: 3 small, 1 medium, 1 large\n",
      "Selected medium project: Dev - Nexus Repo with 10607 issues\n",
      "Selected small project: Community Support - Maven Central with 1641 issues\n",
      "Selected small project: Developer Experience with 50 issues\n",
      "Selected small project: OSS Index Vulnerabilities with 26 issues\n",
      "Added additional project: Community Support - Open Source Project Repository Hosting with 74960 issues\n",
      "Fetching 10607 issues for project ID 10001...\n",
      "  - Processed batch 1 (1200 issues, 1200/1611)\n",
      "  - Processed batch 1 (1200 issues, 1200/17413)\n",
      "  - Processed batch 2 (411 issues, 1611/1611)\n",
      "Fetching 1536 issues for project ID 10401...\n",
      "  - Processed batch 1 (1200 issues, 1200/3333)\n",
      "  - Processed batch 1 (1200 issues, 1200/1577)\n",
      "Found 137172 total issues in 'MongoDB'. Analyzing projects...\n",
      "  - Processed batch 4 (1200 issues, 4800/8533)\n",
      "  - Processed batch 1 (1200 issues, 1200/1536)\n",
      "  - Processed batch 2 (1200 issues, 2400/3333)\n",
      "  - Processed batch 2 (1200 issues, 2400/17413)\n",
      "  - Processed batch 2 (377 issues, 1577/1577)\n",
      "Fetching 1433 issues for project ID 10016...\n",
      "  - Processed batch 1 (1200 issues, 1200/10607)\n",
      "  - Processed batch 2 (336 issues, 1536/1536)\n",
      "Fetching 1172 issues for project ID 10304...\n",
      "  - Processed batch 5 (1200 issues, 6000/8533)\n",
      "  - Processed batch 3 (1200 issues, 3600/17413)\n",
      "  - Processed batch 3 (933 issues, 3333/3333)\n",
      "Fetching 813 issues for project ID 10301...\n",
      "  - Processed batch 1 (1172 issues, 1172/1172)\n",
      "Final sample for 'Hyperledger': 8305 issues from 5 projects\n",
      "Combining 9 processed batches...\n",
      "Combined DataFrame has 8305 rows and 29 columns.\n",
      "Found 21 projects in repository.\n",
      "Project distribution: 19 small, 1 medium, 1 large\n",
      "Selected medium project: Qt Creator with 25249 issues\n",
      "Selected small project: Qt Design Studio with 4619 issues\n",
      "Selected small project: Qt Quality Assurance Infrastructure with 4169 issues\n",
      "Selected small project: Qt 3D Studio with 4092 issues\n",
      "Selected small project: Qt Installer Framework with 2178 issues\n",
      "Fetching 25249 issues for project ID 10512...\n",
      "Found 420819 total issues in 'Mojang'. Analyzing projects...\n",
      "  - Processed batch 1 (813 issues, 813/813)\n",
      "Fetching 465 issues for project ID 10300...\n",
      "  - Processed batch 6 (1200 issues, 7200/8533)\n",
      "  - Processed batch 1 (465 issues, 465/465)\n",
      "Fetching 288 issues for project ID 10400...\n",
      "  - Processed batch 1 (1200 issues, 1200/1433)\n",
      "  - Processed batch 4 (1200 issues, 4800/17413)\n",
      "  - Processed batch 1 (288 issues, 288/288)\n",
      "Fetching 206 issues for project ID 11901...\n",
      "Found 27 projects in repository.\n",
      "Project distribution: 23 small, 3 medium, 1 large\n",
      "Selected medium project: WiredTiger with 8325 issues\n",
      "Selected medium project: Evergreen with 12546 issues\n",
      "Selected medium project: Documentation with 13871 issues\n",
      "Selected small project: Mongoid with 5089 issues\n",
      "Fetching 8325 issues for project ID 12782...\n",
      "  - Processed batch 1 (1200 issues, 1200/25249)\n",
      "  - Processed batch 2 (1200 issues, 2400/10607)\n",
      "  - Processed batch 7 (1200 issues, 8400/8533)\n",
      "  - Processed batch 1 (206 issues, 206/206)\n",
      "Fetching 156 issues for project ID 11300...\n",
      "  - Processed batch 2 (233 issues, 1433/1433)\n",
      "Fetching 400 issues for project ID 10047...\n",
      "  - Processed batch 8 (133 issues, 8533/8533)\n",
      "Final sample for 'IntelDAOS': 9474 issues from 2 projects\n",
      "Combining 9 processed batches...\n",
      "  - Processed batch 1 (156 issues, 156/156)\n",
      "Final sample for 'MariaDB': 8586 issues from 7 projects\n",
      "Combining 11 processed batches...\n",
      "Found 353000 total issues in 'RedHat'. Analyzing projects...\n",
      "  - Processed batch 1 (400 issues, 400/400)\n",
      "Fetching 372 issues for project ID 10054...\n",
      "  - Processed batch 2 (1200 issues, 2400/25249)\n",
      "  - Processed batch 5 (1200 issues, 6000/17413)\n",
      "  - Processed batch 1 (1200 issues, 1200/8325)\n",
      "Combined DataFrame has 8586 rows and 28 columns.\n",
      "  - Processed batch 3 (1200 issues, 3600/10607)\n",
      "  - Processed batch 1 (372 issues, 372/372)\n",
      "Fetching 296 issues for project ID 10038...\n",
      "  - Processed batch 3 (1200 issues, 3600/25249)\n",
      "  - Processed batch 6 (1200 issues, 7200/17413)\n",
      "  - Processed batch 4 (1200 issues, 4800/25249)\n",
      "Combined DataFrame has 9474 rows and 23 columns.\n",
      "  - Processed batch 1 (296 issues, 296/296)\n",
      "Fetching 260 issues for project ID 10029...\n",
      "  - Processed batch 2 (1200 issues, 2400/8325)\n",
      "  - Processed batch 7 (1200 issues, 8400/17413)\n",
      "  - Processed batch 4 (1200 issues, 4800/10607)\n",
      "  - Processed batch 5 (1200 issues, 6000/25249)\n",
      "Found 8 projects in repository.\n",
      "Project distribution: 6 small, 0 medium, 2 large\n",
      "Selected large project: Minecraft (Bedrock codebase) with 144778 issues\n",
      "Fetching 144778 issues for project ID 10200...\n",
      "  - Processed batch 3 (1200 issues, 3600/8325)\n",
      "  - Processed batch 1 (1200 issues, 1200/144778)\n",
      "  - Processed batch 6 (1200 issues, 7200/25249)\n",
      "  - Processed batch 8 (1200 issues, 9600/17413)\n",
      "  - Processed batch 1 (260 issues, 260/260)\n",
      "Fetching 250 issues for project ID 10042...\n",
      "  - Processed batch 2 (1200 issues, 2400/144778)\n",
      "  - Processed batch 4 (1200 issues, 4800/8325)\n",
      "  - Processed batch 5 (1200 issues, 6000/10607)\n",
      "  - Processed batch 7 (1200 issues, 8400/25249)\n",
      "  - Processed batch 1 (250 issues, 250/250)\n",
      "Fetching 250 issues for project ID 10032...\n",
      "  - Processed batch 3 (1200 issues, 3600/144778)\n",
      "  - Processed batch 9 (1200 issues, 10800/17413)\n",
      "  - Processed batch 5 (1200 issues, 6000/8325)\n",
      "  - Processed batch 4 (1200 issues, 4800/144778)\n",
      "  - Processed batch 8 (1200 issues, 9600/25249)\n",
      "  - Processed batch 5 (1200 issues, 6000/144778)\n",
      "  - Processed batch 10 (1200 issues, 12000/17413)\n",
      "  - Processed batch 6 (1200 issues, 7200/8325)\n",
      "  - Processed batch 1 (250 issues, 250/250)\n",
      "Fetching 226 issues for project ID 10028...\n",
      "  - Processed batch 6 (1200 issues, 7200/144778)\n",
      "  - Processed batch 9 (1200 issues, 10800/25249)\n",
      "  - Processed batch 6 (1200 issues, 7200/10607)\n",
      "  - Processed batch 7 (1200 issues, 8400/144778)\n",
      "  - Processed batch 11 (1200 issues, 13200/17413)\n",
      "  - Processed batch 10 (1200 issues, 12000/25249)\n",
      "  - Processed batch 8 (1200 issues, 9600/144778)\n",
      "  - Processed batch 1 (226 issues, 226/226)\n",
      "Fetching 222 issues for project ID 10040...\n",
      "  - Processed batch 11 (1200 issues, 13200/25249)\n",
      "  - Processed batch 9 (1200 issues, 10800/144778)\n",
      "  - Processed batch 12 (1200 issues, 14400/17413)\n",
      "  - Processed batch 7 (1200 issues, 8400/10607)\n",
      "  - Processed batch 10 (1200 issues, 12000/144778)\n",
      "  - Processed batch 12 (1200 issues, 14400/25249)\n",
      "  - Processed batch 1 (222 issues, 222/222)\n",
      "Fetching 195 issues for project ID 10007...\n",
      "  - Processed batch 13 (1200 issues, 15600/17413)  - Processed batch 11 (1200 issues, 13200/144778)\n",
      "\n",
      "  - Processed batch 13 (1200 issues, 15600/25249)\n",
      "  - Processed batch 12 (1200 issues, 14400/144778)\n",
      "  - Processed batch 8 (1200 issues, 9600/10607)\n",
      "  - Processed batch 14 (1200 issues, 16800/17413)\n",
      "  - Processed batch 13 (1200 issues, 15600/144778)\n",
      "  - Processed batch 14 (1200 issues, 16800/25249)\n",
      "Found 241 projects in repository.\n",
      "Project distribution: 240 small, 1 medium, 0 large\n",
      "Selected medium project: Tools (JBoss Tools) with 27295 issues\n",
      "Selected small project: JBoss Enterprise Application Platform with 17102 issues\n",
      "Selected small project: Keycloak with 15564 issues\n",
      "Selected small project: Red Hat Fuse with 13529 issues\n",
      "Selected small project: WildFly with 12984 issues\n",
      "Selected small project: Infinispan with 12829 issues\n",
      "Fetching 27295 issues for project ID 10020...\n",
      "  - Processed batch 1 (195 issues, 195/195)\n",
      "Fetching 178 issues for project ID 10036...\n",
      "  - Processed batch 14 (1200 issues, 16800/144778)\n",
      "  - Processed batch 15 (1200 issues, 18000/25249)\n",
      "  - Processed batch 15 (1200 issues, 18000/144778)\n",
      "  - Processed batch 16 (1200 issues, 19200/144778)\n",
      "  - Processed batch 9 (1007 issues, 10607/10607)\n",
      "Fetching 1641 issues for project ID 10302...\n",
      "  - Processed batch 1 (178 issues, 178/178)\n",
      "Fetching 167 issues for project ID 10004...\n",
      "  - Processed batch 7 (1125 issues, 8325/8325)\n",
      "Fetching 12546 issues for project ID 12787...\n",
      "  - Processed batch 15 (613 issues, 17413/17413)\n",
      "Fetching 3987 issues for project ID 10340...\n",
      "  - Processed batch 16 (1200 issues, 19200/25249)\n",
      "  - Processed batch 17 (1200 issues, 20400/144778)\n",
      "  - Processed batch 18 (1200 issues, 21600/144778)\n",
      "  - Processed batch 17 (1200 issues, 20400/25249)\n",
      "  - Processed batch 1 (1200 issues, 1200/3987)\n",
      "  - Processed batch 1 (167 issues, 167/167)\n",
      "Fetching 159 issues for project ID 10003...\n",
      "  - Processed batch 1 (1200 issues, 1200/1641)\n",
      "  - Processed batch 19 (1200 issues, 22800/144778)\n",
      "  - Processed batch 18 (1200 issues, 21600/25249)\n",
      "  - Processed batch 2 (1200 issues, 2400/3987)\n",
      "  - Processed batch 20 (1200 issues, 24000/144778)\n",
      "  - Processed batch 1 (159 issues, 159/159)\n",
      "Fetching 145 issues for project ID 10014...\n",
      "  - Processed batch 19 (1200 issues, 22800/25249)\n",
      "  - Processed batch 2 (441 issues, 1641/1641)\n",
      "Fetching 50 issues for project ID 13611...\n",
      "  - Processed batch 21 (1200 issues, 25200/144778)\n",
      "  - Processed batch 3 (1200 issues, 3600/3987)\n",
      "  - Processed batch 20 (1200 issues, 24000/25249)\n",
      "  - Processed batch 22 (1200 issues, 26400/144778)\n",
      "  - Processed batch 1 (145 issues, 145/145)\n",
      "Fetching 111 issues for project ID 10019...\n",
      "  - Processed batch 1 (1200 issues, 1200/12546)\n",
      "  - Processed batch 1 (50 issues, 50/50)\n",
      "Fetching 26 issues for project ID 14210...\n",
      "  - Processed batch 1 (1200 issues, 1200/27295)\n",
      "  - Processed batch 23 (1200 issues, 27600/144778)\n",
      "  - Processed batch 4 (387 issues, 3987/3987)\n",
      "Final sample for 'Spring': 21400 issues from 2 projects\n",
      "Combining 19 processed batches...\n",
      "  - Processed batch 21 (1200 issues, 25200/25249)\n",
      "  - Processed batch 1 (111 issues, 111/111)\n",
      "Fetching 111 issues for project ID 10034...\n",
      "  - Processed batch 1 (26 issues, 26/26)\n",
      "Fetching 74960 issues for project ID 10134...\n",
      "  - Processed batch 24 (1200 issues, 28800/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/74960)\n",
      "Combined DataFrame has 21400 rows and 29 columns.\n",
      "  - Processed batch 2 (1200 issues, 2400/74960)\n",
      "  - Processed batch 1 (111 issues, 111/111)\n",
      "Fetching 77 issues for project ID 10046...\n",
      "  - Processed batch 25 (1200 issues, 30000/144778)\n",
      "  - Processed batch 3 (1200 issues, 3600/74960)\n",
      "  - Processed batch 2 (1200 issues, 2400/12546)\n",
      "  - Processed batch 4 (1200 issues, 4800/74960)\n",
      "  - Processed batch 5 (1200 issues, 6000/74960)\n",
      "  - Processed batch 26 (1200 issues, 31200/144778)\n",
      "  - Processed batch 6 (1200 issues, 7200/74960)\n",
      "  - Processed batch 1 (77 issues, 77/77)\n",
      "Fetching 75 issues for project ID 10041...\n",
      "  - Processed batch 7 (1200 issues, 8400/74960)\n",
      "  - Processed batch 27 (1200 issues, 32400/144778)\n",
      "  - Processed batch 22 (49 issues, 25249/25249)\n",
      "Fetching 4619 issues for project ID 11740...\n",
      "  - Processed batch 8 (1200 issues, 9600/74960)\n",
      "  - Processed batch 1 (75 issues, 75/75)\n",
      "Fetching 64 issues for project ID 10025...\n",
      "  - Processed batch 9 (1200 issues, 10800/74960)\n",
      "  - Processed batch 28 (1200 issues, 33600/144778)\n",
      "  - Processed batch 10 (1200 issues, 12000/74960)\n",
      "  - Processed batch 2 (1200 issues, 2400/27295)\n",
      "  - Processed batch 1 (64 issues, 64/64)\n",
      "Fetching 48 issues for project ID 10006...\n",
      "  - Processed batch 11 (1200 issues, 13200/74960)\n",
      "  - Processed batch 29 (1200 issues, 34800/144778)\n",
      "  - Processed batch 3 (1200 issues, 3600/12546)\n",
      "  - Processed batch 12 (1200 issues, 14400/74960)\n",
      "  - Processed batch 30 (1200 issues, 36000/144778)\n",
      "  - Processed batch 13 (1200 issues, 15600/74960)\n",
      "  - Processed batch 1 (48 issues, 48/48)\n",
      "Fetching 39 issues for project ID 10009...\n",
      "  - Processed batch 14 (1200 issues, 16800/74960)\n",
      "  - Processed batch 31 (1200 issues, 37200/144778)\n",
      "  - Processed batch 15 (1200 issues, 18000/74960)\n",
      "  - Processed batch 1 (39 issues, 39/39)\n",
      "Fetching 39 issues for project ID 10048...\n",
      "  - Processed batch 32 (1200 issues, 38400/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/4619)\n",
      "  - Processed batch 16 (1200 issues, 19200/74960)\n",
      "  - Processed batch 1 (39 issues, 39/39)\n",
      "Fetching 38 issues for project ID 10055...\n",
      "  - Processed batch 17 (1200 issues, 20400/74960)\n",
      "  - Processed batch 33 (1200 issues, 39600/144778)\n",
      "  - Processed batch 4 (1200 issues, 4800/12546)\n",
      "  - Processed batch 18 (1200 issues, 21600/74960)\n",
      "  - Processed batch 34 (1200 issues, 40800/144778)\n",
      "  - Processed batch 1 (38 issues, 38/38)\n",
      "Fetching 33 issues for project ID 10033...\n",
      "  - Processed batch 3 (1200 issues, 3600/27295)\n",
      "  - Processed batch 19 (1200 issues, 22800/74960)\n",
      "  - Processed batch 2 (1200 issues, 2400/4619)\n",
      "  - Processed batch 1 (33 issues, 33/33)\n",
      "Fetching 28 issues for project ID 10053...\n",
      "  - Processed batch 35 (1200 issues, 42000/144778)\n",
      "  - Processed batch 20 (1200 issues, 24000/74960)\n",
      "  - Processed batch 21 (1200 issues, 25200/74960)\n",
      "  - Processed batch 36 (1200 issues, 43200/144778)\n",
      "  - Processed batch 5 (1200 issues, 6000/12546)\n",
      "  - Processed batch 1 (28 issues, 28/28)\n",
      "Fetching 27 issues for project ID 10030...\n",
      "  - Processed batch 22 (1200 issues, 26400/74960)\n",
      "  - Processed batch 37 (1200 issues, 44400/144778)\n",
      "  - Processed batch 23 (1200 issues, 27600/74960)\n",
      "  - Processed batch 1 (27 issues, 27/27)\n",
      "Fetching 26 issues for project ID 10010...\n",
      "  - Processed batch 3 (1200 issues, 3600/4619)\n",
      "  - Processed batch 38 (1200 issues, 45600/144778)\n",
      "  - Processed batch 24 (1200 issues, 28800/74960)\n",
      "  - Processed batch 39 (1200 issues, 46800/144778)\n",
      "  - Processed batch 1 (26 issues, 26/26)\n",
      "Fetching 25 issues for project ID 10045...\n",
      "  - Processed batch 25 (1200 issues, 30000/74960)\n",
      "  - Processed batch 40 (1200 issues, 48000/144778)\n",
      "  - Processed batch 1 (25 issues, 25/25)\n",
      "Fetching 24 issues for project ID 10031...\n",
      "  - Processed batch 26 (1200 issues, 31200/74960)\n",
      "  - Processed batch 6 (1200 issues, 7200/12546)\n",
      "  - Processed batch 4 (1200 issues, 4800/27295)\n",
      "  - Processed batch 4 (1019 issues, 4619/4619)\n",
      "Fetching 4169 issues for project ID 10600...\n",
      "  - Processed batch 41 (1200 issues, 49200/144778)\n",
      "  - Processed batch 27 (1200 issues, 32400/74960)\n",
      "  - Processed batch 1 (1200 issues, 1200/4169)\n",
      "  - Processed batch 1 (24 issues, 24/24)\n",
      "Fetching 23 issues for project ID 10037...\n",
      "  - Processed batch 2 (1200 issues, 2400/4169)\n",
      "  - Processed batch 28 (1200 issues, 33600/74960)\n",
      "  - Processed batch 42 (1200 issues, 50400/144778)\n",
      "  - Processed batch 1 (23 issues, 23/23)\n",
      "Fetching 22 issues for project ID 10021...\n",
      "  - Processed batch 3 (1200 issues, 3600/4169)\n",
      "  - Processed batch 29 (1200 issues, 34800/74960)\n",
      "  - Processed batch 43 (1200 issues, 51600/144778)\n",
      "  - Processed batch 1 (22 issues, 22/22)\n",
      "Fetching 21 issues for project ID 10050...\n",
      "  - Processed batch 30 (1200 issues, 36000/74960)\n",
      "  - Processed batch 44 (1200 issues, 52800/144778)\n",
      "  - Processed batch 7 (1200 issues, 8400/12546)\n",
      "  - Processed batch 31 (1200 issues, 37200/74960)\n",
      "  - Processed batch 1 (21 issues, 21/21)\n",
      "Fetching 21 issues for project ID 10008...\n",
      "  - Processed batch 45 (1200 issues, 54000/144778)\n",
      "  - Processed batch 32 (1200 issues, 38400/74960)\n",
      "  - Processed batch 1 (21 issues, 21/21)\n",
      "Fetching 18 issues for project ID 10051...\n",
      "  - Processed batch 46 (1200 issues, 55200/144778)\n",
      "  - Processed batch 5 (1200 issues, 6000/27295)\n",
      "  - Processed batch 33 (1200 issues, 39600/74960)\n",
      "  - Processed batch 4 (569 issues, 4169/4169)\n",
      "Fetching 4092 issues for project ID 11540...\n",
      "  - Processed batch 1 (18 issues, 18/18)\n",
      "Fetching 18 issues for project ID 10005...\n",
      "  - Processed batch 34 (1200 issues, 40800/74960)\n",
      "  - Processed batch 47 (1200 issues, 56400/144778)\n",
      "  - Processed batch 35 (1200 issues, 42000/74960)\n",
      "  - Processed batch 48 (1200 issues, 57600/144778)\n",
      "  - Processed batch 1 (18 issues, 18/18)\n",
      "Fetching 17 issues for project ID 10024...\n",
      "  - Processed batch 8 (1200 issues, 9600/12546)\n",
      "  - Processed batch 36 (1200 issues, 43200/74960)\n",
      "  - Processed batch 49 (1200 issues, 58800/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/4092)\n",
      "  - Processed batch 1 (17 issues, 17/17)\n",
      "Fetching 17 issues for project ID 10027...\n",
      "  - Processed batch 37 (1200 issues, 44400/74960)\n",
      "  - Processed batch 50 (1200 issues, 60000/144778)\n",
      "  - Processed batch 38 (1200 issues, 45600/74960)\n",
      "  - Processed batch 1 (17 issues, 17/17)\n",
      "Fetching 16 issues for project ID 10035...\n",
      "  - Processed batch 51 (1200 issues, 61200/144778)\n",
      "  - Processed batch 39 (1200 issues, 46800/74960)\n",
      "  - Processed batch 9 (1200 issues, 10800/12546)\n",
      "  - Processed batch 1 (16 issues, 16/16)\n",
      "Fetching 15 issues for project ID 10023...\n",
      "  - Processed batch 52 (1200 issues, 62400/144778)\n",
      "  - Processed batch 2 (1200 issues, 2400/4092)\n",
      "  - Processed batch 40 (1200 issues, 48000/74960)\n",
      "  - Processed batch 6 (1200 issues, 7200/27295)\n",
      "  - Processed batch 53 (1200 issues, 63600/144778)\n",
      "  - Processed batch 1 (15 issues, 15/15)\n",
      "Fetching 15 issues for project ID 10018...\n",
      "  - Processed batch 41 (1200 issues, 49200/74960)\n",
      "  - Processed batch 54 (1200 issues, 64800/144778)\n",
      "  - Processed batch 42 (1200 issues, 50400/74960)\n",
      "  - Processed batch 1 (15 issues, 15/15)\n",
      "Fetching 14 issues for project ID 10002...\n",
      "  - Processed batch 3 (1200 issues, 3600/4092)\n",
      "  - Processed batch 55 (1200 issues, 66000/144778)\n",
      "  - Processed batch 43 (1200 issues, 51600/74960)\n",
      "  - Processed batch 10 (1200 issues, 12000/12546)\n",
      "  - Processed batch 1 (14 issues, 14/14)\n",
      "Fetching 14 issues for project ID 10012...\n",
      "  - Processed batch 44 (1200 issues, 52800/74960)\n",
      "  - Processed batch 56 (1200 issues, 67200/144778)\n",
      "  - Processed batch 1 (14 issues, 14/14)\n",
      "Fetching 12 issues for project ID 10026...\n",
      "  - Processed batch 45 (1200 issues, 54000/74960)\n",
      "  - Processed batch 57 (1200 issues, 68400/144778)\n",
      "  - Processed batch 4 (492 issues, 4092/4092)\n",
      "Fetching 2178 issues for project ID 10630...\n",
      "  - Processed batch 46 (1200 issues, 55200/74960)\n",
      "  - Processed batch 1 (12 issues, 12/12)\n",
      "Fetching 12 issues for project ID 10044...\n",
      "  - Processed batch 1 (1200 issues, 1200/2178)\n",
      "  - Processed batch 7 (1200 issues, 8400/27295)\n",
      "  - Processed batch 58 (1200 issues, 69600/144778)\n",
      "  - Processed batch 47 (1200 issues, 56400/74960)\n",
      "  - Processed batch 1 (12 issues, 12/12)\n",
      "Fetching 10 issues for project ID 10022...\n",
      "  - Processed batch 59 (1200 issues, 70800/144778)\n",
      "  - Processed batch 48 (1200 issues, 57600/74960)\n",
      "  - Processed batch 11 (546 issues, 12546/12546)\n",
      "Fetching 13871 issues for project ID 10380...\n",
      "  - Processed batch 1 (10 issues, 10/10)\n",
      "Fetching 9 issues for project ID 10043...\n",
      "  - Processed batch 49 (1200 issues, 58800/74960)\n",
      "  - Processed batch 60 (1200 issues, 72000/144778)\n",
      "  - Processed batch 1 (9 issues, 9/9)\n",
      "Fetching 9 issues for project ID 10011...\n",
      "  - Processed batch 50 (1200 issues, 60000/74960)\n",
      "  - Processed batch 61 (1200 issues, 73200/144778)\n",
      "  - Processed batch 2 (978 issues, 2178/2178)\n",
      "Final sample for 'Qt': 40307 issues from 5 projects\n",
      "Combining 36 processed batches...\n",
      "  - Processed batch 51 (1200 issues, 61200/74960)\n",
      "  - Processed batch 1 (9 issues, 9/9)\n",
      "Fetching 6 issues for project ID 10052...\n",
      "  - Processed batch 62 (1200 issues, 74400/144778)\n",
      "Combined DataFrame has 40307 rows and 29 columns.\n",
      "  - Processed batch 1 (1200 issues, 1200/13871)\n",
      "  - Processed batch 52 (1200 issues, 62400/74960)\n",
      "  - Processed batch 63 (1200 issues, 75600/144778)\n",
      "  - Processed batch 1 (6 issues, 6/6)\n",
      "Fetching 6 issues for project ID 10020...\n",
      "  - Processed batch 8 (1200 issues, 9600/27295)\n",
      "  - Processed batch 53 (1200 issues, 63600/74960)\n",
      "  - Processed batch 1 (6 issues, 6/6)\n",
      "Fetching 5 issues for project ID 10039...\n",
      "  - Processed batch 64 (1200 issues, 76800/144778)\n",
      "  - Processed batch 1 (5 issues, 5/5)\n",
      "Fetching 4 issues for project ID 10000...\n",
      "  - Processed batch 54 (1200 issues, 64800/74960)\n",
      "  - Processed batch 65 (1200 issues, 78000/144778)\n",
      "  - Processed batch 1 (4 issues, 4/4)\n",
      "Fetching 43351 issues for project ID 10057...\n",
      "  - Processed batch 2 (1200 issues, 2400/13871)\n",
      "  - Processed batch 55 (1200 issues, 66000/74960)\n",
      "  - Processed batch 66 (1200 issues, 79200/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/43351)\n",
      "  - Processed batch 2 (1200 issues, 2400/43351)\n",
      "  - Processed batch 56 (1200 issues, 67200/74960)\n",
      "  - Processed batch 67 (1200 issues, 80400/144778)\n",
      "  - Processed batch 3 (1200 issues, 3600/43351)\n",
      "  - Processed batch 9 (1200 issues, 10800/27295)\n",
      "  - Processed batch 57 (1200 issues, 68400/74960)\n",
      "  - Processed batch 3 (1200 issues, 3600/13871)\n",
      "  - Processed batch 4 (1200 issues, 4800/43351)\n",
      "  - Processed batch 68 (1200 issues, 81600/144778)\n",
      "  - Processed batch 5 (1200 issues, 6000/43351)\n",
      "  - Processed batch 58 (1200 issues, 69600/74960)\n",
      "  - Processed batch 6 (1200 issues, 7200/43351)\n",
      "  - Processed batch 59 (1200 issues, 70800/74960)\n",
      "  - Processed batch 69 (1200 issues, 82800/144778)\n",
      "  - Processed batch 7 (1200 issues, 8400/43351)\n",
      "  - Processed batch 4 (1200 issues, 4800/13871)\n",
      "  - Processed batch 8 (1200 issues, 9600/43351)\n",
      "  - Processed batch 60 (1200 issues, 72000/74960)\n",
      "  - Processed batch 70 (1200 issues, 84000/144778)\n",
      "  - Processed batch 10 (1200 issues, 12000/27295)\n",
      "  - Processed batch 9 (1200 issues, 10800/43351)\n",
      "  - Processed batch 61 (1200 issues, 73200/74960)\n",
      "  - Processed batch 10 (1200 issues, 12000/43351)\n",
      "  - Processed batch 71 (1200 issues, 85200/144778)\n",
      "  - Processed batch 5 (1200 issues, 6000/13871)\n",
      "  - Processed batch 11 (1200 issues, 13200/43351)\n",
      "  - Processed batch 62 (1200 issues, 74400/74960)\n",
      "  - Processed batch 72 (1200 issues, 86400/144778)\n",
      "  - Processed batch 12 (1200 issues, 14400/43351)\n",
      "  - Processed batch 63 (560 issues, 74960/74960)\n",
      "Final sample for 'Sonatype': 87284 issues from 5 projects\n",
      "Combining 76 processed batches...\n",
      "  - Processed batch 13 (1200 issues, 15600/43351)\n",
      "  - Processed batch 11 (1200 issues, 13200/27295)\n",
      "  - Processed batch 73 (1200 issues, 87600/144778)\n",
      "  - Processed batch 6 (1200 issues, 7200/13871)\n",
      "  - Processed batch 14 (1200 issues, 16800/43351)\n",
      "  - Processed batch 15 (1200 issues, 18000/43351)\n",
      "  - Processed batch 74 (1200 issues, 88800/144778)\n",
      "Combined DataFrame has 87284 rows and 29 columns.\n",
      "  - Processed batch 16 (1200 issues, 19200/43351)\n",
      "  - Processed batch 75 (1200 issues, 90000/144778)\n",
      "  - Processed batch 7 (1200 issues, 8400/13871)\n",
      "  - Processed batch 17 (1200 issues, 20400/43351)\n",
      "  - Processed batch 76 (1200 issues, 91200/144778)\n",
      "  - Processed batch 12 (1200 issues, 14400/27295)\n",
      "  - Processed batch 18 (1200 issues, 21600/43351)\n",
      "  - Processed batch 77 (1200 issues, 92400/144778)\n",
      "  - Processed batch 19 (1200 issues, 22800/43351)\n",
      "  - Processed batch 8 (1200 issues, 9600/13871)\n",
      "  - Processed batch 78 (1200 issues, 93600/144778)\n",
      "  - Processed batch 20 (1200 issues, 24000/43351)\n",
      "  - Processed batch 79 (1200 issues, 94800/144778)\n",
      "  - Processed batch 13 (1200 issues, 15600/27295)\n",
      "  - Processed batch 9 (1200 issues, 10800/13871)\n",
      "  - Processed batch 21 (1200 issues, 25200/43351)\n",
      "  - Processed batch 80 (1200 issues, 96000/144778)\n",
      "  - Processed batch 22 (1200 issues, 26400/43351)\n",
      "  - Processed batch 81 (1200 issues, 97200/144778)\n",
      "  - Processed batch 10 (1200 issues, 12000/13871)\n",
      "  - Processed batch 23 (1200 issues, 27600/43351)\n",
      "  - Processed batch 82 (1200 issues, 98400/144778)\n",
      "  - Processed batch 14 (1200 issues, 16800/27295)\n",
      "  - Processed batch 24 (1200 issues, 28800/43351)\n",
      "  - Processed batch 83 (1200 issues, 99600/144778)\n",
      "  - Processed batch 11 (1200 issues, 13200/13871)\n",
      "  - Processed batch 25 (1200 issues, 30000/43351)\n",
      "  - Processed batch 84 (1200 issues, 100800/144778)\n",
      "  - Processed batch 26 (1200 issues, 31200/43351)\n",
      "  - Processed batch 85 (1200 issues, 102000/144778)\n",
      "  - Processed batch 15 (1200 issues, 18000/27295)\n",
      "  - Processed batch 12 (671 issues, 13871/13871)\n",
      "Fetching 5089 issues for project ID 12981...\n",
      "  - Processed batch 27 (1200 issues, 32400/43351)\n",
      "  - Processed batch 86 (1200 issues, 103200/144778)\n",
      "  - Processed batch 28 (1200 issues, 33600/43351)\n",
      "  - Processed batch 87 (1200 issues, 104400/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/5089)\n",
      "  - Processed batch 16 (1200 issues, 19200/27295)\n",
      "  - Processed batch 29 (1200 issues, 34800/43351)\n",
      "  - Processed batch 88 (1200 issues, 105600/144778)\n",
      "  - Processed batch 2 (1200 issues, 2400/5089)\n",
      "  - Processed batch 89 (1200 issues, 106800/144778)\n",
      "  - Processed batch 30 (1200 issues, 36000/43351)\n",
      "  - Processed batch 90 (1200 issues, 108000/144778)\n",
      "  - Processed batch 31 (1200 issues, 37200/43351)\n",
      "  - Processed batch 17 (1200 issues, 20400/27295)\n",
      "  - Processed batch 3 (1200 issues, 3600/5089)\n",
      "  - Processed batch 91 (1200 issues, 109200/144778)\n",
      "  - Processed batch 32 (1200 issues, 38400/43351)\n",
      "  - Processed batch 92 (1200 issues, 110400/144778)\n",
      "  - Processed batch 4 (1200 issues, 4800/5089)\n",
      "  - Processed batch 33 (1200 issues, 39600/43351)\n",
      "  - Processed batch 18 (1200 issues, 21600/27295)\n",
      "  - Processed batch 93 (1200 issues, 111600/144778)\n",
      "  - Processed batch 34 (1200 issues, 40800/43351)\n",
      "  - Processed batch 5 (289 issues, 5089/5089)\n",
      "Final sample for 'MongoDB': 39831 issues from 4 projects\n",
      "Combining 35 processed batches...\n",
      "  - Processed batch 94 (1200 issues, 112800/144778)\n",
      "  - Processed batch 35 (1200 issues, 42000/43351)\n",
      "  - Processed batch 95 (1200 issues, 114000/144778)\n",
      "Combined DataFrame has 39831 rows and 30 columns.\n",
      "  - Processed batch 19 (1200 issues, 22800/27295)\n",
      "  - Processed batch 36 (1200 issues, 43200/43351)\n",
      "  - Processed batch 96 (1200 issues, 115200/144778)\n",
      "  - Processed batch 37 (151 issues, 43351/43351)\n",
      "Final sample for 'Sakai': 50550 issues from 53 projects\n",
      "Combining 91 processed batches...\n",
      "  - Processed batch 97 (1200 issues, 116400/144778)\n",
      "  - Processed batch 20 (1200 issues, 24000/27295)\n",
      "  - Processed batch 98 (1200 issues, 117600/144778)\n",
      "Combined DataFrame has 50550 rows and 23 columns.\n",
      "  - Processed batch 99 (1200 issues, 118800/144778)\n",
      "  - Processed batch 100 (1200 issues, 120000/144778)\n",
      "  - Processed batch 21 (1200 issues, 25200/27295)\n",
      "  - Processed batch 101 (1200 issues, 121200/144778)\n",
      "  - Processed batch 102 (1200 issues, 122400/144778)\n",
      "  - Processed batch 103 (1200 issues, 123600/144778)\n",
      "  - Processed batch 22 (1200 issues, 26400/27295)\n",
      "  - Processed batch 104 (1200 issues, 124800/144778)\n",
      "  - Processed batch 105 (1200 issues, 126000/144778)\n",
      "  - Processed batch 106 (1200 issues, 127200/144778)\n",
      "  - Processed batch 107 (1200 issues, 128400/144778)\n",
      "  - Processed batch 23 (895 issues, 27295/27295)\n",
      "Fetching 17102 issues for project ID 12313422...\n",
      "  - Processed batch 108 (1200 issues, 129600/144778)\n",
      "  - Processed batch 109 (1200 issues, 130800/144778)\n",
      "  - Processed batch 1 (1200 issues, 1200/17102)\n",
      "  - Processed batch 110 (1200 issues, 132000/144778)\n",
      "  - Processed batch 111 (1200 issues, 133200/144778)\n",
      "  - Processed batch 112 (1200 issues, 134400/144778)\n",
      "  - Processed batch 2 (1200 issues, 2400/17102)\n",
      "  - Processed batch 113 (1200 issues, 135600/144778)\n",
      "  - Processed batch 114 (1200 issues, 136800/144778)\n",
      "  - Processed batch 3 (1200 issues, 3600/17102)\n",
      "  - Processed batch 115 (1200 issues, 138000/144778)\n",
      "  - Processed batch 116 (1200 issues, 139200/144778)\n",
      "  - Processed batch 4 (1200 issues, 4800/17102)\n",
      "  - Processed batch 117 (1200 issues, 140400/144778)\n",
      "  - Processed batch 118 (1200 issues, 141600/144778)\n",
      "  - Processed batch 119 (1200 issues, 142800/144778)\n",
      "  - Processed batch 5 (1200 issues, 6000/17102)\n",
      "  - Processed batch 120 (1200 issues, 144000/144778)\n",
      "  - Processed batch 6 (1200 issues, 7200/17102)\n",
      "  - Processed batch 121 (778 issues, 144778/144778)\n",
      "Final sample for 'Mojang': 144778 issues from 1 projects\n",
      "Combining 121 processed batches...\n",
      "  - Processed batch 7 (1200 issues, 8400/17102)\n",
      "Combined DataFrame has 144778 rows and 26 columns.\n",
      "  - Processed batch 8 (1200 issues, 9600/17102)\n",
      "  - Processed batch 9 (1200 issues, 10800/17102)\n",
      "  - Processed batch 10 (1200 issues, 12000/17102)\n",
      "  - Processed batch 11 (1200 issues, 13200/17102)\n",
      "  - Processed batch 12 (1200 issues, 14400/17102)\n",
      "  - Processed batch 13 (1200 issues, 15600/17102)\n",
      "  - Processed batch 14 (1200 issues, 16800/17102)\n",
      "  - Processed batch 15 (302 issues, 17102/17102)\n",
      "Fetching 15564 issues for project ID 12313920...\n",
      "  - Processed batch 1 (1200 issues, 1200/15564)\n",
      "  - Processed batch 2 (1200 issues, 2400/15564)\n",
      "  - Processed batch 3 (1200 issues, 3600/15564)\n",
      "  - Processed batch 4 (1200 issues, 4800/15564)\n",
      "  - Processed batch 5 (1200 issues, 6000/15564)\n",
      "  - Processed batch 6 (1200 issues, 7200/15564)\n",
      "  - Processed batch 7 (1200 issues, 8400/15564)\n",
      "  - Processed batch 8 (1200 issues, 9600/15564)\n",
      "  - Processed batch 9 (1200 issues, 10800/15564)\n",
      "  - Processed batch 10 (1200 issues, 12000/15564)\n",
      "  - Processed batch 11 (1200 issues, 13200/15564)\n",
      "  - Processed batch 12 (1200 issues, 14400/15564)\n",
      "  - Processed batch 13 (1164 issues, 15564/15564)\n",
      "Fetching 13529 issues for project ID 12314570...\n",
      "  - Processed batch 1 (1200 issues, 1200/13529)\n",
      "  - Processed batch 2 (1200 issues, 2400/13529)\n",
      "  - Processed batch 3 (1200 issues, 3600/13529)\n",
      "  - Processed batch 4 (1200 issues, 4800/13529)\n",
      "  - Processed batch 5 (1200 issues, 6000/13529)\n",
      "  - Processed batch 6 (1200 issues, 7200/13529)\n",
      "  - Processed batch 7 (1200 issues, 8400/13529)\n",
      "  - Processed batch 8 (1200 issues, 9600/13529)\n",
      "  - Processed batch 9 (1200 issues, 10800/13529)\n",
      "  - Processed batch 10 (1200 issues, 12000/13529)\n",
      "  - Processed batch 11 (1200 issues, 13200/13529)\n",
      "  - Processed batch 12 (329 issues, 13529/13529)\n",
      "Fetching 12984 issues for project ID 12313721...\n",
      "  - Processed batch 1 (1200 issues, 1200/12984)\n",
      "  - Processed batch 2 (1200 issues, 2400/12984)\n",
      "  - Processed batch 3 (1200 issues, 3600/12984)\n",
      "  - Processed batch 4 (1200 issues, 4800/12984)\n",
      "  - Processed batch 5 (1200 issues, 6000/12984)\n",
      "  - Processed batch 6 (1200 issues, 7200/12984)\n",
      "  - Processed batch 7 (1200 issues, 8400/12984)\n",
      "  - Processed batch 8 (1200 issues, 9600/12984)\n",
      "  - Processed batch 9 (1200 issues, 10800/12984)\n",
      "  - Processed batch 10 (1200 issues, 12000/12984)\n",
      "  - Processed batch 11 (984 issues, 12984/12984)\n",
      "Fetching 12829 issues for project ID 12310799...\n",
      "  - Processed batch 1 (1200 issues, 1200/12829)\n",
      "  - Processed batch 2 (1200 issues, 2400/12829)\n",
      "  - Processed batch 3 (1200 issues, 3600/12829)\n",
      "  - Processed batch 4 (1200 issues, 4800/12829)\n",
      "  - Processed batch 5 (1200 issues, 6000/12829)\n",
      "  - Processed batch 6 (1200 issues, 7200/12829)\n",
      "  - Processed batch 7 (1200 issues, 8400/12829)\n",
      "  - Processed batch 8 (1200 issues, 9600/12829)\n",
      "  - Processed batch 9 (1200 issues, 10800/12829)\n",
      "  - Processed batch 10 (1200 issues, 12000/12829)\n",
      "  - Processed batch 11 (829 issues, 12829/12829)\n",
      "Final sample for 'RedHat': 99303 issues from 6 projects\n",
      "Combining 85 processed batches...\n",
      "Combined DataFrame has 99303 rows and 29 columns.\n",
      "Data processed. Launching D-Tale session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2020-03-27 11:49:02', '2020-03-30 14:22:52', '2020-03-13 12:24:32',\n",
      " '2020-03-20 12:10:26', '2020-03-20 12:11:30', '2020-03-27 07:24:13',\n",
      " '2020-03-27 07:25:17', '2020-02-28 15:30:20', '2020-02-26 17:05:44',\n",
      " '2020-03-20 12:10:48',\n",
      " ...\n",
      " '2009-03-03 11:32:28', '2009-04-15 04:34:25', '2009-03-03 11:32:29',\n",
      " '2009-03-03 11:32:28', '2009-09-24 07:52:33', '2009-03-03 11:32:29',\n",
      " '2009-03-11 11:08:38', '2014-12-01 03:18:47', '2009-04-08 09:00:10',\n",
      " '2009-04-08 04:56:38']\n",
      "Length: 472471, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2020-03-13 12:28:08', '2020-03-13 12:26:27', '2020-02-28 15:28:29',\n",
      " '2020-02-28 02:06:01', '2020-02-27 22:54:44', '2020-02-26 22:56:06',\n",
      " '2020-02-26 22:44:35', '2020-02-17 07:52:39', '2020-02-17 07:50:17',\n",
      " '2020-01-29 16:00:00',\n",
      " ...\n",
      " '2009-03-03 10:56:12', '2009-03-18 10:16:42', '2009-03-03 11:27:23',\n",
      " '2009-03-03 10:59:15', '2009-03-12 04:49:25', '2009-03-03 11:04:41',\n",
      " '2009-03-03 11:23:30', '2009-03-11 09:28:26', '2009-04-03 20:28:45',\n",
      " '2009-04-03 18:42:43']\n",
      "Length: 472471, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/utils.py:911: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '<DatetimeArray>\n",
      "['2020-03-27 11:49:14', '2020-03-30 14:22:52', '2020-03-13 12:48:53',\n",
      " '2020-03-20 12:10:26', '2020-03-20 12:11:30', '2020-03-27 07:24:13',\n",
      " '2020-03-27 07:25:17', '2020-02-28 15:30:20', '2020-02-27 01:09:46',\n",
      " '2020-03-27 08:22:06',\n",
      " ...\n",
      " '2020-02-07 06:14:54', '2020-02-07 06:15:31', '2020-02-07 06:15:34',\n",
      " '2020-02-07 06:15:23', '2020-02-07 06:14:57', '2020-02-07 06:14:59',\n",
      " '2009-04-07 08:38:50', '2021-10-24 05:47:17', '2020-02-07 06:15:17',\n",
      " '2020-02-07 06:14:52']\n",
      "Length: 472471, dtype: datetime64[ns]' has dtype incompatible with datetime64[ns, UTC], please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D-Tale session launched successfully.\n",
      "Final DataFrame with 472471 rows exported to 'final_df.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 10:54:05,046 - INFO     - Executing shutdown due to inactivity...\n",
      "2025-03-14 10:54:05,177 - INFO     - Executing shutdown...\n",
      "2025-03-14 10:54:05,178 - INFO     - Not running with the Werkzeug Server, exiting by searching gc for BaseWSGIServer\n",
      "/opt/anaconda3/envs/my_mongo_env/lib/python3.9/site-packages/dtale/app.py:445: FutureWarning:\n",
      "\n",
      "`torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json5 as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)  # We want to see all data\n",
    "from statistics import mean, median\n",
    "from time import time\n",
    "import json\n",
    "import dtale\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "\n",
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuetype_information.json') as f:\n",
    "    jira_issuetype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Link Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuelinktype_information.json') as f:\n",
    "    jira_issuelinktype_information = json.load(f)\n",
    "\n",
    "\n",
    "ALL_JIRAS = [jira_name for jira_name in jira_data_sources.keys()]\n",
    "\n",
    "df_jiras = pd.DataFrame(\n",
    "    np.nan,\n",
    "    columns=['Born', 'Issues', 'DIT', 'UIT', 'Links', 'DLT', 'ULT', 'Changes', 'Ch/I', 'UP', 'Comments', 'Co/I'],\n",
    "    index=ALL_JIRAS + ['Sum', 'Median', 'Std Dev']\n",
    ")\n",
    "\n",
    "def parse_date_str(x):\n",
    "    \"\"\"\n",
    "    Parse a date string using dateparser. If the string is \"Missing\", empty, or cannot be parsed, return pd.NaT.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    if s.lower() == \"missing\" or s == \"\":\n",
    "        return pd.NaT\n",
    "    \n",
    "    return x\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using our custom parse_date_str.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_date_str)\n",
    "    return df\n",
    "\n",
    "def drop_invalid_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    Drop rows where any of the specified date columns are NaT.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame.\n",
    "      date_columns (list): List of date column names to check.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with rows dropped if any of the specified date columns are NaT.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=date_columns)\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    (Date-like strings remain as strings.)\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def parse_issuelinks(issuelinks):\n",
    "    \"\"\"\n",
    "    Parse the issuelinks field which is always a string (or null).\n",
    "    Returns a list of link dictionaries if parsing is successful, or an empty list.\n",
    "    \"\"\"\n",
    "    if issuelinks is None:\n",
    "        return []\n",
    "    if isinstance(issuelinks, str):\n",
    "        issuelinks = issuelinks.strip()\n",
    "        if not issuelinks:\n",
    "            return []\n",
    "        try:\n",
    "            parsed = ast.literal_eval(issuelinks)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing issuelinks: {issuelinks}. Error: {e}\")\n",
    "    return []\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' field and return only the total number of links.\n",
    "    \"\"\"\n",
    "    links = parse_issuelinks(issuelinks)\n",
    "    return {\"issuelinks_total\": len(links)}\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \"\"\"\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(emb_array, index=descriptions.index,\n",
    "                          columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])])\n",
    "    return emb_df\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio, batch_size=1200, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository using streaming approach with CSV storage.\n",
    "    \n",
    "    Parameters:\n",
    "        jira_name (str): Name of the Jira repository\n",
    "        db: MongoDB database connection\n",
    "        sample_ratio (float): Used to influence project selection\n",
    "        batch_size (int): Size of batches for processing\n",
    "        target_cap (int): Target number of total issues to sample\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with sampled issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "    # Get total count first (fast operation)\n",
    "    total_issues = db[jira_name].count_documents({})\n",
    "    if total_issues == 0:\n",
    "        print(f\" No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # Define only the fields we actually need\n",
    "    needed_fields = {\n",
    "        # Essential identification fields\n",
    "        \"id\": 1,\n",
    "        \"key\": 1,\n",
    "        \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "        # Issue metadata\n",
    "        \"fields.summary\": 1,\n",
    "        \"fields.description\": 1,\n",
    "        \"fields.created\": 1,\n",
    "        \"fields.updated\": 1,\n",
    "        \"fields.resolutiondate\": 1,\n",
    "        \n",
    "        # Classification fields\n",
    "        \"fields.issuetype.name\": 1,\n",
    "        \"fields.priority.name\": 1,\n",
    "        \"fields.status.name\": 1,\n",
    "        \n",
    "        # People fields\n",
    "        \"fields.assignee.key\": 1,\n",
    "        \"fields.assignee.name\": 1,\n",
    "        \"fields.reporter.key\": 1, \n",
    "        \"fields.reporter.name\": 1,\n",
    "        \"fields.creator.key\": 1,\n",
    "        \"fields.creator.name\": 1,\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\": 1,\n",
    "        \"fields.project.key\": 1, \n",
    "        \"fields.project.name\": 1,\n",
    "        \n",
    "        # Relationships\n",
    "        \"fields.issuelinks\": 1,\n",
    "        \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\": 1,\n",
    "        \"fields.labels\": 1,\n",
    "        \"fields.fixVersions\": 1,\n",
    "        \n",
    "        # Comments\n",
    "        \"fields.comments\": 1\n",
    "    }\n",
    "    \n",
    "    print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "    # --- 1) Get project distribution ---\n",
    "    project_counts = list(db[jira_name].aggregate([\n",
    "        {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "                   \"count\": {\"$sum\": 1},\n",
    "                   \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "    ]))\n",
    "    \n",
    "    projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "               for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "    print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "    # --- 2) Select projects based on size categories ---\n",
    "    adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "    small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "    medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "    large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "    print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "    # Initialize selection\n",
    "    selected_projects = []\n",
    "    total_selected_issues = 0\n",
    "    \n",
    "    # Helper function to find best fit project\n",
    "    def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "        if not project_list:\n",
    "            return None\n",
    "        \n",
    "        sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "        for project in sorted_projects:\n",
    "            if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "                return project\n",
    "        \n",
    "        if not prefer_larger:\n",
    "            return sorted_projects[0]  # smallest\n",
    "        return None\n",
    "    \n",
    "    # Try to select a diverse mix of projects\n",
    "    \n",
    "    # First try to get a large project\n",
    "    if large_projects:\n",
    "        best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "        if best_large and best_large[\"count\"] <= adjusted_target * 1.3:\n",
    "            selected_projects.append(best_large)\n",
    "            total_selected_issues += best_large[\"count\"]\n",
    "            large_projects.remove(best_large)\n",
    "            print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "    # Then add some medium projects\n",
    "    while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "        if best_medium:\n",
    "            selected_projects.append(best_medium)\n",
    "            total_selected_issues += best_medium[\"count\"]\n",
    "            medium_projects.remove(best_medium)\n",
    "            print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Finally fill in with small projects\n",
    "    while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "        if best_small:\n",
    "            selected_projects.append(best_small)\n",
    "            total_selected_issues += best_small[\"count\"]\n",
    "            small_projects.remove(best_small)\n",
    "            print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # If we're still significantly under target, add one more project that best fits\n",
    "    if total_selected_issues < adjusted_target * 0.8:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        remaining_projects = small_projects + medium_projects + large_projects\n",
    "        best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "        if best_remaining:\n",
    "            selected_projects.append(best_remaining)\n",
    "            total_selected_issues += best_remaining[\"count\"]\n",
    "            print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "    # --- 3) Create temporary directory for streaming storage ---\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    batch_files = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # --- 4) Process issues using streaming approach ---\n",
    "        if not selected_projects:\n",
    "            print(\"No projects could be selected. Using default sampling method.\")\n",
    "            # Fall back to random sampling\n",
    "            desired_sample_size = min(500, total_issues)\n",
    "            sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                # Explicitly exclude _id field\n",
    "                cursor = db[jira_name].find({}, {**needed_fields, \"_id\": 0}).skip(idx).limit(1)\n",
    "                batch = list(cursor)\n",
    "                \n",
    "                # Process batch immediately\n",
    "                if batch:\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_random_{idx}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    total_processed += len(batch)\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "        else:\n",
    "            # Process each selected project\n",
    "            project_ids = [p[\"id\"] for p in selected_projects]\n",
    "            \n",
    "            for project_id in project_ids:\n",
    "                # Get count for this project\n",
    "                project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "                \n",
    "                # Fetch in batches\n",
    "                print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "                \n",
    "                batch_count = 0\n",
    "                fetched_count = 0\n",
    "                \n",
    "                while fetched_count < project_issue_count:\n",
    "                    # Create a fresh cursor for each batch with the appropriate skip and limit\n",
    "                    # Explicitly exclude _id field\n",
    "                    cursor = db[jira_name].find(\n",
    "                        {\"fields.project.id\": project_id}, \n",
    "                        {**needed_fields, \"_id\": 0}  # Exclude _id field\n",
    "                    ).skip(batch_count * batch_size).limit(batch_size)\n",
    "                    \n",
    "                    batch = list(cursor)\n",
    "                    if not batch:\n",
    "                        break\n",
    "\n",
    "                    # Store the batch size for printing later\n",
    "                    batch_size_actual = len(batch)\n",
    "                    \n",
    "                    # Process batch immediately\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_{project_id}_{batch_count}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                    fetched_count += batch_size_actual\n",
    "                    total_processed += batch_size_actual\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"  - Processed batch {batch_count} ({batch_size_actual} issues, {fetched_count}/{project_issue_count})\")\n",
    "        \n",
    "        print(f\"Final sample for '{jira_name}': {total_processed} issues from {len(selected_projects)} projects\")\n",
    "        \n",
    "        # --- 5) Combine all saved batches ---\n",
    "        if not batch_files:\n",
    "            print(\"No data collected.\")\n",
    "            return None\n",
    "        \n",
    "        # Read all CSV files and combine\n",
    "        print(f\"Combining {len(batch_files)} processed batches...\")\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                chunk = pd.read_csv(file, low_memory=False)\n",
    "                dfs.append(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading batch file {file}: {e}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"No valid data read from batches.\")\n",
    "            return None\n",
    "        \n",
    "        # Combine all data\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Combined DataFrame has {len(final_df)} rows and {len(final_df.columns)} columns.\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing temp file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(temp_dir):\n",
    "                os.rmdir(temp_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing temp directory {temp_dir}: {e}\")\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \"Missing\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3,\n",
    "                                zero_threshold=0.8, open_dtale=True, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    and drop changelog summary columns dominated by zeros.\n",
    "    \n",
    "    If open_dtale is True, launch a D-Tale session for interactive visualization;\n",
    "    otherwise, simply return the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        selected_jiras (list): List of Jira repositories to process\n",
    "        sample_ratio (float): Ratio of data to sample (1.0 = 100%)\n",
    "        missing_threshold (float): Threshold for dropping columns with missing values\n",
    "        zero_threshold (float): Threshold for dropping columns dominated by zeros\n",
    "        open_dtale (bool): Whether to launch a D-Tale session\n",
    "        target_cap (int): Maximum number of issues to retrieve per repository\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "\n",
    "    # Load Jira data sources configuration\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\" No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    # Process each repository in parallel\n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_repo, jira_name, db, sample_ratio, 1200, target_cap): jira_name for jira_name in all_jiras}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "\n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "\n",
    "\n",
    "    # # Process the 'fields.description' field to generate dense embeddings\n",
    "    # if \"fields.description\" in final_df.columns:\n",
    "    #     desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "    #     final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "\n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "\n",
    "    date_cols = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\"]\n",
    "\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_cols)\n",
    "    final_df = drop_invalid_dates(final_df, date_cols)\n",
    "    final_df['fields.created'] = pd.to_datetime(final_df['fields.created'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.updated'] = pd.to_datetime(final_df['fields.updated'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.resolutiondate'] = pd.to_datetime(final_df['fields.resolutiondate'], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if open_dtale:\n",
    "        print(\"Data processed. Launching D-Tale session...\")\n",
    "        d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "        d.open_browser()\n",
    "        print(\" D-Tale session launched successfully.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def export_clean_df():\n",
    "    \"\"\"\n",
    "    Run the full OverviewAnalysis pipeline and return the final DataFrame with all engineered features.\n",
    "    This version uses sample_ratio=1.0 to get 100% of the dataset and an unlimited target_cap.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The final processed DataFrame with all data.\n",
    "    \"\"\"\n",
    "    final_df = explore_all_fields_in_dtale(\n",
    "        selected_jiras=[\"Mojang\", \"Hyperledger\", \"IntelDAOS\", \"MariaDB\", \"MongoDB\", \"Qt\", \"RedHat\", \"Sakai\", \"Sonatype\", \"Spring\"],\n",
    "        sample_ratio=0.30,  # Set to 1.0 for 100% of data\n",
    "        missing_threshold=0.3,\n",
    "        zero_threshold=0.7,\n",
    "        open_dtale=True,\n",
    "        target_cap=float('inf')  # No upper limit on the number of issues\n",
    "    )\n",
    "    final_df.to_csv(\"final_df.csv\", index=False)\n",
    "    print(f\"Final DataFrame with {len(final_df)} rows exported to 'final_df.csv'.\")\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# For testing purposes, you can run export_clean_df() if executing this module directly.\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_training = export_clean_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8a08e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=500):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using batch processing to prevent connection timeouts.\n",
    "#     Uses a fixed maximum of 500 records per repository and queries only necessary fields.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Original sample ratio parameter (kept for compatibility)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\" No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Processing in batches of {batch_size}...\")\n",
    "    \n",
    "#     # --- 1) Calculate sample size with fixed maximum ---\n",
    "#     MAX_RECORDS = 500  # Fixed maximum number of records\n",
    "#     desired_sample_size = min(MAX_RECORDS, total_issues)\n",
    "#     print(f\"Using fixed maximum of {MAX_RECORDS} records. Will retrieve {desired_sample_size} issues.\")\n",
    "    \n",
    "#     # --- 2) Determine if we need to sample during fetching or after ---\n",
    "#     if desired_sample_size < total_issues / 10:\n",
    "#         # If we want a small sample, use random skip to efficiently get samples\n",
    "#         # This avoids loading all documents when we only need a small fraction\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Fetch documents by their indices using skip/limit, but only retrieve needed fields\n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # For larger samples, process in batches\n",
    "#         sampled_issues = []\n",
    "#         cursor = db[jira_name].find({}, needed_fields)  # Only retrieve needed fields\n",
    "        \n",
    "#         # Process in batches to avoid loading everything at once\n",
    "#         batch_count = 0\n",
    "#         while True:\n",
    "#             batch = list(cursor.limit(batch_size).skip(batch_count * batch_size))\n",
    "#             if not batch:\n",
    "#                 break\n",
    "                \n",
    "#             batch_count += 1\n",
    "#             print(f\"  - Processed batch {batch_count} ({len(batch)} issues)\")\n",
    "#             sampled_issues.extend(batch)\n",
    "            \n",
    "#         # Apply sampling after all batches are collected\n",
    "#         if len(sampled_issues) > desired_sample_size:\n",
    "#             print(f\"  - Sampling {desired_sample_size} issues from {len(sampled_issues)} collected issues\")\n",
    "#             sampled_issues = random.sample(sampled_issues, desired_sample_size)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues (out of {total_issues} total).\")\n",
    "    \n",
    "#     # --- 3) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main\n",
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=500, target_cap=900):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using project-based sampling.\n",
    "#     Samples complete projects to reach approximately target_cap total records.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Used to influence project selection (higher means more projects)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "#         target_cap (int): Target number of total issues to sample across projects\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\" No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "#     # --- 1) Get project distribution ---\n",
    "#     # Get count of issues by project.id\n",
    "#     project_counts = list(db[jira_name].aggregate([\n",
    "#         {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "#                    \"count\": {\"$sum\": 1},\n",
    "#                    \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "#     ]))\n",
    "    \n",
    "#     # Convert to list of dictionaries with project details\n",
    "#     projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "#                for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "#     print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "#     # --- 2) Select projects based on size categories ---\n",
    "#     # Adjust target cap based on sample_ratio if needed\n",
    "#     adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "#     # Group projects by size\n",
    "#     small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "#     medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "#     large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "#     print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "#     # Initialize selection\n",
    "#     selected_projects = []\n",
    "#     total_selected_issues = 0\n",
    "    \n",
    "#     # Helper function to find best fit project\n",
    "#     def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "#         if not project_list:\n",
    "#             return None\n",
    "        \n",
    "#         # Sort by size (descending if prefer_larger, ascending otherwise)\n",
    "#         sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "#         # Find first project that fits\n",
    "#         for project in sorted_projects:\n",
    "#             # Allow slight overage (up to 20%)\n",
    "#             if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "#                 return project\n",
    "        \n",
    "#         # If nothing fits within constraints, take smallest\n",
    "#         if not prefer_larger:\n",
    "#             return sorted_projects[0]  # smallest\n",
    "#         return None\n",
    "    \n",
    "#     # Try to select a diverse mix of projects\n",
    "    \n",
    "#     # First try to get a large project if it fits reasonably\n",
    "#     if large_projects:\n",
    "#         best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "#         if best_large and best_large[\"count\"] <= adjusted_target * 1.3:  # Allow 30% overage for large projects\n",
    "#             selected_projects.append(best_large)\n",
    "#             total_selected_issues += best_large[\"count\"]\n",
    "#             large_projects.remove(best_large)\n",
    "#             print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "#     # Then add some medium projects\n",
    "#     while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "#         if best_medium:\n",
    "#             selected_projects.append(best_medium)\n",
    "#             total_selected_issues += best_medium[\"count\"]\n",
    "#             medium_projects.remove(best_medium)\n",
    "#             print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # Finally fill in with small projects\n",
    "#     while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "#         if best_small:\n",
    "#             selected_projects.append(best_small)\n",
    "#             total_selected_issues += best_small[\"count\"]\n",
    "#             small_projects.remove(best_small)\n",
    "#             print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # If we're still significantly under target, add one more project that best fits\n",
    "#     if total_selected_issues < adjusted_target * 0.8:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         remaining_projects = small_projects + medium_projects + large_projects\n",
    "#         best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "#         if best_remaining:\n",
    "#             selected_projects.append(best_remaining)\n",
    "#             total_selected_issues += best_remaining[\"count\"]\n",
    "#             print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "#     # --- 3) Fetch issues from selected projects ---\n",
    "#     if not selected_projects:\n",
    "#         print(\"No projects could be selected. Using default sampling method.\")\n",
    "#         # Fall back to the original sampling method\n",
    "#         desired_sample_size = min(500, total_issues)\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # Get all issues from selected projects\n",
    "#         project_ids = [p[\"id\"] for p in selected_projects]\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Process each project\n",
    "#         for project_id in project_ids:\n",
    "#             # Get count for this project\n",
    "#             project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "            \n",
    "#             # Fetch in batches\n",
    "#             print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "#             cursor = db[jira_name].find({\"fields.project.id\": project_id}, needed_fields)\n",
    "            \n",
    "#             batch_count = 0\n",
    "#             fetched_count = 0\n",
    "            \n",
    "#             while fetched_count < project_issue_count:\n",
    "#                 batch = list(cursor.limit(batch_size).skip(batch_count * batch_size))\n",
    "#                 if not batch:\n",
    "#                     break\n",
    "                \n",
    "#                 batch_count += 1\n",
    "#                 fetched_count += len(batch)\n",
    "#                 print(f\"  - Fetched batch {batch_count} ({len(batch)} issues, {fetched_count}/{project_issue_count})\")\n",
    "#                 sampled_issues.extend(batch)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues from {len(selected_projects)} projects\")\n",
    "    \n",
    "#     # --- 4) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main\n",
    "\n",
    "# def process_repo(jira_name, db, sample_ratio, batch_size=300, target_cap=90000):\n",
    "#     \"\"\"\n",
    "#     Process a single Jira repository using project-based sampling.\n",
    "#     Samples complete projects to reach approximately target_cap total records.\n",
    "    \n",
    "#     Parameters:\n",
    "#         jira_name (str): Name of the Jira repository\n",
    "#         db: MongoDB database connection\n",
    "#         sample_ratio (float): Used to influence project selection (higher means more projects)\n",
    "#         batch_size (int): Size of batches for processing\n",
    "#         target_cap (int): Target number of total issues to sample across projects\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Processed dataframe with sampled issues\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "#     # Get total count first (fast operation)\n",
    "#     total_issues = db[jira_name].count_documents({})\n",
    "#     if total_issues == 0:\n",
    "#         print(f\" No documents found for '{jira_name}', skipping.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Define only the fields we actually need\n",
    "#     needed_fields = {\n",
    "#         # Essential identification fields\n",
    "#         \"_id\": 1,\n",
    "#         \"id\": 1,\n",
    "#         \"key\": 1,\n",
    "#         \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "#         # Issue metadata\n",
    "#         \"fields.summary\": 1,\n",
    "#         \"fields.description\": 1,\n",
    "#         \"fields.created\": 1,\n",
    "#         \"fields.updated\": 1,\n",
    "#         \"fields.resolutiondate\": 1,\n",
    "        \n",
    "#         # Classification fields\n",
    "#         \"fields.issuetype.name\": 1,\n",
    "#         \"fields.priority.name\": 1,\n",
    "#         \"fields.status.name\": 1,\n",
    "        \n",
    "#         # People fields\n",
    "#         \"fields.assignee.key\": 1,\n",
    "#         \"fields.assignee.name\": 1,\n",
    "#         \"fields.reporter.key\": 1, \n",
    "#         \"fields.reporter.name\": 1,\n",
    "#         \"fields.creator.key\": 1,\n",
    "#         \"fields.creator.name\": 1,\n",
    "        \n",
    "#         # Project context\n",
    "#         \"fields.project.id\": 1,\n",
    "#         \"fields.project.key\": 1, \n",
    "#         \"fields.project.name\": 1,\n",
    "        \n",
    "#         # Relationships\n",
    "#         \"fields.issuelinks\": 1,\n",
    "#         \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "#         # Components and labels\n",
    "#         \"fields.components\": 1,\n",
    "#         \"fields.labels\": 1,\n",
    "#         \"fields.fixVersions\": 1,\n",
    "        \n",
    "#         # Comments\n",
    "#         \"fields.comments\": 1\n",
    "#     }\n",
    "    \n",
    "#     print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "#     # --- 1) Get project distribution ---\n",
    "#     # Get count of issues by project.id\n",
    "#     project_counts = list(db[jira_name].aggregate([\n",
    "#         {\"$group\": {\"_id\": \"$fields.project.id\", \n",
    "#                    \"count\": {\"$sum\": 1},\n",
    "#                    \"name\": {\"$first\": \"$fields.project.name\"}}}\n",
    "#     ]))\n",
    "    \n",
    "#     # Convert to list of dictionaries with project details\n",
    "#     projects = [{\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")} \n",
    "#                for p in project_counts if p[\"_id\"] is not None]\n",
    "    \n",
    "#     print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "#     # --- 2) Select projects based on size categories ---\n",
    "#     # Adjust target cap based on sample_ratio if needed\n",
    "#     adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "#     # Group projects by size\n",
    "#     small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "#     medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "#     large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "#     print(f\"Project distribution: {len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\")\n",
    "    \n",
    "#     # Initialize selection\n",
    "#     selected_projects = []\n",
    "#     total_selected_issues = 0\n",
    "    \n",
    "#     # Helper function to find best fit project\n",
    "#     def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "#         if not project_list:\n",
    "#             return None\n",
    "        \n",
    "#         # Sort by size (descending if prefer_larger, ascending otherwise)\n",
    "#         sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        \n",
    "#         # Find first project that fits\n",
    "#         for project in sorted_projects:\n",
    "#             # Allow slight overage (up to 20%)\n",
    "#             if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "#                 return project\n",
    "        \n",
    "#         # If nothing fits within constraints, take smallest\n",
    "#         if not prefer_larger:\n",
    "#             return sorted_projects[0]  # smallest\n",
    "#         return None\n",
    "    \n",
    "#     # Try to select a diverse mix of projects\n",
    "    \n",
    "#     # First try to get a large project if it fits reasonably\n",
    "#     if large_projects:\n",
    "#         best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "#         if best_large and best_large[\"count\"] <= adjusted_target * 1.3:  # Allow 30% overage for large projects\n",
    "#             selected_projects.append(best_large)\n",
    "#             total_selected_issues += best_large[\"count\"]\n",
    "#             large_projects.remove(best_large)\n",
    "#             print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "#     # Then add some medium projects\n",
    "#     while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "#         if best_medium:\n",
    "#             selected_projects.append(best_medium)\n",
    "#             total_selected_issues += best_medium[\"count\"]\n",
    "#             medium_projects.remove(best_medium)\n",
    "#             print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # Finally fill in with small projects\n",
    "#     while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "#         if best_small:\n",
    "#             selected_projects.append(best_small)\n",
    "#             total_selected_issues += best_small[\"count\"]\n",
    "#             small_projects.remove(best_small)\n",
    "#             print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     # If we're still significantly under target, add one more project that best fits\n",
    "#     if total_selected_issues < adjusted_target * 0.8:\n",
    "#         remaining = adjusted_target - total_selected_issues\n",
    "#         remaining_projects = small_projects + medium_projects + large_projects\n",
    "#         best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "#         if best_remaining:\n",
    "#             selected_projects.append(best_remaining)\n",
    "#             total_selected_issues += best_remaining[\"count\"]\n",
    "#             print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "#     # --- 3) Fetch issues from selected projects ---\n",
    "#     if not selected_projects:\n",
    "#         print(\"No projects could be selected. Using default sampling method.\")\n",
    "#         # Fall back to the original sampling method\n",
    "#         desired_sample_size = min(500, total_issues)\n",
    "#         sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         for idx in sample_indices:\n",
    "#             doc = db[jira_name].find({}, needed_fields).skip(idx).limit(1)\n",
    "#             sampled_issues.extend(list(doc))\n",
    "#     else:\n",
    "#         # Get all issues from selected projects\n",
    "#         project_ids = [p[\"id\"] for p in selected_projects]\n",
    "#         sampled_issues = []\n",
    "        \n",
    "#         # Process each project\n",
    "#         for project_id in project_ids:\n",
    "#             # Get count for this project\n",
    "#             project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "            \n",
    "#             # Fetch in batches\n",
    "#             print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "            \n",
    "#             # Fixed: Create a new cursor for each batch instead of reusing the same cursor\n",
    "#             batch_count = 0\n",
    "#             fetched_count = 0\n",
    "            \n",
    "#             while fetched_count < project_issue_count:\n",
    "#                 # Create a fresh cursor for each batch with the appropriate skip and limit\n",
    "#                 cursor = db[jira_name].find(\n",
    "#                     {\"fields.project.id\": project_id}, \n",
    "#                     needed_fields\n",
    "#                 ).skip(batch_count * batch_size).limit(batch_size)\n",
    "                \n",
    "#                 batch = list(cursor)\n",
    "#                 if not batch:\n",
    "#                     break\n",
    "                \n",
    "#                 batch_count += 1\n",
    "#                 fetched_count += len(batch)\n",
    "#                 print(f\"  - Fetched batch {batch_count} ({len(batch)} issues, {fetched_count}/{project_issue_count})\")\n",
    "#                 sampled_issues.extend(batch)\n",
    "    \n",
    "#     print(f\"Final sample for '{jira_name}': {len(sampled_issues)} issues from {len(selected_projects)} projects\")\n",
    "    \n",
    "#     # --- 4) Process the sample as before ---\n",
    "#     if not sampled_issues:\n",
    "#         return None\n",
    "        \n",
    "#     # Convert to DataFrame and apply the pipeline\n",
    "#     df_main = pd.json_normalize(sampled_issues, sep='.')\n",
    "#     df_main = fix_data_types(df_main)\n",
    "    \n",
    "#     # Add repository name for traceability\n",
    "#     df_main['repository'] = jira_name\n",
    "    \n",
    "#     return df_main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
