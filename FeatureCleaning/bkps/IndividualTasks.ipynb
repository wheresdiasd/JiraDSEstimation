{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029fad86-86c9-43bc-a9db-9d8124c3a830",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing repository: Apache ...\n",
      "\n",
      "Processing repository: Hyperledger ...\n",
      "\n",
      "Processing repository: IntelDAOS ...\n",
      "\n",
      "Processing repository: JFrog ...\n",
      "\n",
      "Processing repository: Jira ...\n",
      "\n",
      "Processing repository: JiraEcosystem ...\n",
      "\n",
      "Processing repository: MariaDB ...\n",
      "\n",
      "Processing repository: Mindville ...\n",
      "\n",
      "Processing repository: Mojang ...\n",
      "\n",
      "Processing repository: MongoDB ...\n",
      "\n",
      "Processing repository: Qt ...\n",
      "\n",
      "Processing repository: RedHat ...\n",
      "Loaded ['Core Server', 'Sawtooth', 'Fabric', 'Python Driver', 'Ruby Driver', 'Java Driver', 'Tools (JBoss Tools)', 'EJB 3.0', 'Application Server 3  4  5 and 6', 'C Driver', 'C# Driver', 'jBPM', 'JGroups', 'Seam 2', 'Blockchain Explorer', 'Shared Access Layer', 'Application Links', 'Cello', 'Minecraft (Bedrock codebase)', 'Atlassian Plugins', 'Jira Server and Data Center', 'Atlassian REST', 'Indy Node', 'Iroha', 'Atlassian OAuth Plugin', 'Universal Plugin Manager', 'Documentation', 'Atlassian Maven Plugin Suite', 'Minecraft: Java Edition', 'Indy SDK', 'Activity Streams', 'Confluence Server and Data Center', 'Fabric SDK Node', 'Fabric CA', 'Fabric SDK Go', 'Node.js Driver', 'Bamboo', 'Minecraft Launcher', 'Atlassian Marketplace', 'Bedrock Dedicated Server', 'Crucible', 'Jira Software Server and Data Center', 'Atlassian Gadgets', 'Atlassian Reference Application', 'MongoDB Database Tools', 'Atlassian User Interface', 'WiredTiger', 'Evergreen', 'ActiveObjects', 'Atlassian Selenium', 'Mongoid', 'Compass ', 'Bitbucket Server', 'Atlassian Plugins Webresources', 'JIRA REST Java Client Library', 'Atlassian Connector for Eclipse (discontinued)', 'Atlassian Connector for IntelliJ IDE (discontinued)', 'Atlassian Connector for Visual Studio (discontinued)', 'Atlassian Troubleshooting & Support Tools Plugin', 'JIRA Toolkit Plugin', 'Subversion JIRA plugin', 'Atlassian Connect', 'Sourcetree for Windows', 'Atlassian Cloud', 'Jira Service Management Server and Data Center', 'Maven JGit Flow', 'Atlassian Connect in Jira Cloud', 'Confluence Ecosystem (Moved: go.atlassian.com/ce-move)', 'Atlassian Accessibility', 'Jira Software Cloud', 'Jira Service Management Cloud', 'Confluence Cloud', 'Jira Cloud', 'Atlassian Connect JavaScript API', 'Bitbucket Cloud', 'Jira Performance Testing Tools', 'Forge', 'Server Platform Frontend', 'JBoss Web Services', 'JBoss ESB', 'JBoss Transaction Manager', 'JBRULES', 'PicketBox ', 'JBoss Enterprise Application Platform 4 and 5', 'jboss.org', 'JBoss Enterprise SOA Platform', 'Red Hat CodeReady Studio (devstudio)', 'RESTEasy', 'IronJacamar', 'Teiid', 'Teiid Designer', 'Visual Design', 'Infinispan', 'Arquillian', 'Errai', 'Weld', 'ModeShape', 'SwitchYard', 'Application Server 7', 'AeroGear', 'JBoss Developer Materials', 'Undertow', 'Drools', 'OptaPlanner', 'JBoss Enterprise Application Platform', 'Red Hat Developer Website', 'WildFly', 'WINDUP - Red Hat Application Migration Toolkit', 'Keycloak', 'HAL', 'apiman (API Management)', 'AppFormer', 'Red Hat Fuse', 'Fuse Tooling', 'JBoss A-MQ', 'FUSE Mediation Router', 'WildFly Elytron', 'WildFly Core', 'FeedHenry', 'Hawkular', 'JBoss Web Server', 'AMQ Interconnect', 'Cloud Enablement', 'RH-SSO', 'WINDUPRULE - Red Hat Application Migration Toolkit rules', 'BxMS Documentation', 'AMQ Documentation', 'Thorntail', 'Red Hat Data Grid', 'Debezium', 'JBoss BRMS Platform', 'JBoss BPMS Platform', 'Docs for Red Hat Developers', 'Red Hat 3scale API Management', 'AMQ Broker', 'A-MQ Messaging-as-a-Service', 'Spring Boot & Cloud', 'AMQ Streams', 'Red Hat Decision Manager', 'Red Hat Process Automation Manager', 'Kiali QE', 'Distributed Tracing', 'Maistra', 'Red Hat CodeReady Workspaces', 'Kogito', 'Quarkus', 'App Dev UXD (Archived)', 'Project Quay', 'OpenShift Top Level Product Epics', 'OpenShift Logging', 'OpenShift Dev Console', 'OpenShift SDN', 'Multiple Architecture Enablement', 'OpenShift Request For Enhancement', 'Cost Management', 'Openshift sandboxed containers', 'Automation Hub', 'CentOS Stream'] project IDs from CSV.\n",
      "Loaded 158 project IDs from CSV.===============================\n",
      "Using project IDs filter: ['Core Server', 'Sawtooth', 'Fabric', 'Python Driver', 'Ruby Driver', 'Java Driver', 'Tools (JBoss Tools)', 'EJB 3.0', 'Application Server 3  4  5 and 6', 'C Driver', 'C# Driver', 'jBPM', 'JGroups', 'Seam 2', 'Blockchain Explorer', 'Shared Access Layer', 'Application Links', 'Cello', 'Minecraft (Bedrock codebase)', 'Atlassian Plugins', 'Jira Server and Data Center', 'Atlassian REST', 'Indy Node', 'Iroha', 'Atlassian OAuth Plugin', 'Universal Plugin Manager', 'Documentation', 'Atlassian Maven Plugin Suite', 'Minecraft: Java Edition', 'Indy SDK', 'Activity Streams', 'Confluence Server and Data Center', 'Fabric SDK Node', 'Fabric CA', 'Fabric SDK Go', 'Node.js Driver', 'Bamboo', 'Minecraft Launcher', 'Atlassian Marketplace', 'Bedrock Dedicated Server', 'Crucible', 'Jira Software Server and Data Center', 'Atlassian Gadgets', 'Atlassian Reference Application', 'MongoDB Database Tools', 'Atlassian User Interface', 'WiredTiger', 'Evergreen', 'ActiveObjects', 'Atlassian Selenium', 'Mongoid', 'Compass ', 'Bitbucket Server', 'Atlassian Plugins Webresources', 'JIRA REST Java Client Library', 'Atlassian Connector for Eclipse (discontinued)', 'Atlassian Connector for IntelliJ IDE (discontinued)', 'Atlassian Connector for Visual Studio (discontinued)', 'Atlassian Troubleshooting & Support Tools Plugin', 'JIRA Toolkit Plugin', 'Subversion JIRA plugin', 'Atlassian Connect', 'Sourcetree for Windows', 'Atlassian Cloud', 'Jira Service Management Server and Data Center', 'Maven JGit Flow', 'Atlassian Connect in Jira Cloud', 'Confluence Ecosystem (Moved: go.atlassian.com/ce-move)', 'Atlassian Accessibility', 'Jira Software Cloud', 'Jira Service Management Cloud', 'Confluence Cloud', 'Jira Cloud', 'Atlassian Connect JavaScript API', 'Bitbucket Cloud', 'Jira Performance Testing Tools', 'Forge', 'Server Platform Frontend', 'JBoss Web Services', 'JBoss ESB', 'JBoss Transaction Manager', 'JBRULES', 'PicketBox ', 'JBoss Enterprise Application Platform 4 and 5', 'jboss.org', 'JBoss Enterprise SOA Platform', 'Red Hat CodeReady Studio (devstudio)', 'RESTEasy', 'IronJacamar', 'Teiid', 'Teiid Designer', 'Visual Design', 'Infinispan', 'Arquillian', 'Errai', 'Weld', 'ModeShape', 'SwitchYard', 'Application Server 7', 'AeroGear', 'JBoss Developer Materials', 'Undertow', 'Drools', 'OptaPlanner', 'JBoss Enterprise Application Platform', 'Red Hat Developer Website', 'WildFly', 'WINDUP - Red Hat Application Migration Toolkit', 'Keycloak', 'HAL', 'apiman (API Management)', 'AppFormer', 'Red Hat Fuse', 'Fuse Tooling', 'JBoss A-MQ', 'FUSE Mediation Router', 'WildFly Elytron', 'WildFly Core', 'FeedHenry', 'Hawkular', 'JBoss Web Server', 'AMQ Interconnect', 'Cloud Enablement', 'RH-SSO', 'WINDUPRULE - Red Hat Application Migration Toolkit rules', 'BxMS Documentation', 'AMQ Documentation', 'Thorntail', 'Red Hat Data Grid', 'Debezium', 'JBoss BRMS Platform', 'JBoss BPMS Platform', 'Docs for Red Hat Developers', 'Red Hat 3scale API Management', 'AMQ Broker', 'A-MQ Messaging-as-a-Service', 'Spring Boot & Cloud', 'AMQ Streams', 'Red Hat Decision Manager', 'Red Hat Process Automation Manager', 'Kiali QE', 'Distributed Tracing', 'Maistra', 'Red Hat CodeReady Workspaces', 'Kogito', 'Quarkus', 'App Dev UXD (Archived)', 'Project Quay', 'OpenShift Top Level Product Epics', 'OpenShift Logging', 'OpenShift Dev Console', 'OpenShift SDN', 'Multiple Architecture Enablement', 'OpenShift Request For Enhancement', 'Cost Management', 'Openshift sandboxed containers', 'Automation Hub', 'CentOS Stream']\n",
      "Found 2134 total issues in 'Mindville'. Analyzing projects...\n",
      "Found 7 projects in repository.\n",
      "Project distribution: 6 small, 0 medium, 1 large\n",
      "Selected large project: Insight Core - Server with 1847 issues\n",
      "Selected small project: Insight Discovery with 87 issues\n",
      "Fetching 1847 issues for project ID 10000...\n",
      "Fetching 87 issues for project ID 10001...\n",
      "Final sample for 'Mindville': 0 issues from 2 projects\n",
      "No data collected.\n",
      "\n",
      "Processing repository: Sakai ...\n"
     ]
    }
   ],
   "source": [
    "def get_project_name_from_csv(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    if 'project_name' not in df.columns:\n",
    "        raise ValueError(\"CSV file does not contain 'project_name' column.\")\n",
    "    \n",
    "    project_ids = df['project_name'].unique().tolist()\n",
    "    print(f\"Loaded {project_ids} project IDs from CSV.\")\n",
    "    return project_ids\n",
    "\n",
    "def save_csvs_by_project_id(final_df, output_folder=\"projects_output\"):\n",
    "    \"\"\"\n",
    "    Saves one CSV per unique project ID in `final_df`, placing them into `output_folder`.\n",
    "    The DataFrame is grouped by 'fields.project.name'.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists (create it if necessary)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Group by project ID\n",
    "    grouped = final_df.groupby(\"fields.project.name\")\n",
    "\n",
    "    for project_name, df_subset in grouped:\n",
    "        # Make the project ID safe for filenames (handle slashes, etc.)\n",
    "        project_name_str = str(project_name).replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "\n",
    "        # Build the path for this project's CSV file\n",
    "        output_file = os.path.join(output_folder, f\"project_{project_name_str}.csv\")\n",
    "\n",
    "        # Write the CSV\n",
    "        df_subset.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(df_subset)} records to {output_file}\")\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import json5 as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)  # We want to see all data\n",
    "from statistics import mean, median\n",
    "from time import time\n",
    "import json\n",
    "import dtale\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tempfile\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Connect to the database\n",
    "client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "db = client['JiraRepos']\n",
    "\n",
    "# Load the Jira Data Sources JSON\n",
    "with open('../0. DataDefinition/jira_data_sources.json') as f:\n",
    "    jira_data_sources = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuetype_information.json') as f:\n",
    "    jira_issuetype_information = json.load(f)\n",
    "\n",
    "# Load the Jira Issue Link Types Information (Downloaded using the DataDownload script)\n",
    "with open('../0. DataDefinition/jira_issuelinktype_information.json') as f:\n",
    "    jira_issuelinktype_information = json.load(f)\n",
    "\n",
    "\n",
    "ALL_JIRAS = [jira_name for jira_name in jira_data_sources.keys()]\n",
    "\n",
    "df_jiras = pd.DataFrame(\n",
    "    np.nan,\n",
    "    columns=['Born', 'Issues', 'DIT', 'UIT', 'Links', 'DLT', 'ULT', 'Changes', 'Ch/I', 'UP', 'Comments', 'Co/I'],\n",
    "    index=ALL_JIRAS + ['Sum', 'Median', 'Std Dev']\n",
    ")\n",
    "\n",
    "\n",
    "def parse_date_str(x):\n",
    "    \"\"\"\n",
    "    Parse a date string using dateparser. If the string is \"Missing\", empty, or cannot be parsed, return pd.NaT.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    if s.lower() == \"missing\" or s == \"\":\n",
    "        return pd.NaT\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def convert_date_columns_dateparser(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert the specified date columns from string to datetime using our custom parse_date_str.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame containing date strings.\n",
    "      date_columns (list): List of column names to convert.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: The DataFrame with specified columns converted to datetime objects.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(parse_date_str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_invalid_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    Drop rows where any of the specified date columns are NaT.\n",
    "    \n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input DataFrame.\n",
    "      date_columns (list): List of date column names to check.\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame with rows dropped if any of the specified date columns are NaT.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=date_columns)\n",
    "\n",
    "\n",
    "def fix_data_types(df, numeric_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Convert DataFrame columns (stored as strings) to appropriate data types,\n",
    "    excluding any date formatting.\n",
    "\n",
    "    For each column that is not list-like:\n",
    "      - If at least `numeric_threshold` fraction of values can be converted to numeric,\n",
    "        the column is converted to a numeric dtype.\n",
    "      - Otherwise, the column is cast to 'category' dtype.\n",
    "    (Date-like strings remain as strings.)\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        # Skip list-like columns\n",
    "        if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            continue\n",
    "        numeric_series = pd.to_numeric(df[col], errors='coerce')\n",
    "        if numeric_series.notnull().mean() >= numeric_threshold:\n",
    "            df[col] = numeric_series\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def flatten_histories(histories):\n",
    "    \"\"\"\n",
    "    Flatten a list of changelog history entries into a DataFrame.\n",
    "    Each row represents a single change item.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for history in histories:\n",
    "        history_id = history.get(\"id\")\n",
    "        author = history.get(\"author\", {}).get(\"name\")\n",
    "        created = history.get(\"created\")\n",
    "        items = history.get(\"items\", [])\n",
    "        for item in items:\n",
    "            rows.append({\n",
    "                \"changelog.history_id\": history_id,\n",
    "                \"changelog.author\": author,\n",
    "                \"changelog.created\": created,\n",
    "                \"changelog.field\": item.get(\"field\"),\n",
    "                \"changelog.fieldtype\": item.get(\"fieldtype\"),\n",
    "                \"changelog.from\": item.get(\"from\"),\n",
    "                \"changelog.fromString\": item.get(\"fromString\"),\n",
    "                \"changelog.to\": item.get(\"to\"),\n",
    "                \"changelog.toString\": item.get(\"toString\")\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def process_issue_histories(issue):\n",
    "    \"\"\"\n",
    "    Process a single issue's changelog histories:\n",
    "      - Flatten the histories.\n",
    "      - Apply type conversion.\n",
    "      - Add an 'issue_key' (using 'key' if available, else 'id') for merging.\n",
    "    \"\"\"\n",
    "    if \"changelog\" in issue and \"histories\" in issue[\"changelog\"]:\n",
    "        histories = issue[\"changelog\"][\"histories\"]\n",
    "        df_history = flatten_histories(histories)\n",
    "        df_history = fix_data_types(df_history)\n",
    "        df_history[\"issue_key\"] = issue.get(\"key\", issue.get(\"id\"))\n",
    "        return df_history\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_and_flatten_histories(issues):\n",
    "    \"\"\"\n",
    "    Extract and flatten changelog histories from a list of issues using parallel processing.\n",
    "    \"\"\"\n",
    "    flattened_histories = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_issue_histories, issue): issue for issue in issues}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None and not result.empty:\n",
    "                flattened_histories.append(result)\n",
    "    if flattened_histories:\n",
    "        return pd.concat(flattened_histories, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def summarize_changelog_histories(df_histories):\n",
    "    \"\"\"\n",
    "    Summarize flattened changelog histories by counting the number of changes per field.\n",
    "    Returns a DataFrame with one row per issue (keyed by 'issue_key').\n",
    "    \"\"\"\n",
    "    summary = df_histories.groupby('issue_key')['changelog.field'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    summary = summary.rename(columns=lambda x: f'changelog_count_{x}' if x != 'issue_key' else x)\n",
    "    return summary\n",
    "\n",
    "\n",
    "def process_issue_links(issuelinks):\n",
    "    \"\"\"\n",
    "    Process the 'fields.issuelinks' JSON array and extract features:\n",
    "      - Total number of links.\n",
    "      - Count and binary flag for each link type.\n",
    "    \"\"\"\n",
    "    features = {\"issuelinks_total\": 0}\n",
    "    link_types = {}\n",
    "    if isinstance(issuelinks, list):\n",
    "        features[\"issuelinks_total\"] = len(issuelinks)\n",
    "        for link in issuelinks:\n",
    "            lt = link.get(\"type\", {}).get(\"name\", \"Unknown\")\n",
    "            link_types[lt] = link_types.get(lt, 0) + 1\n",
    "    else:\n",
    "        features[\"issuelinks_total\"] = 0\n",
    "    for lt, count in link_types.items():\n",
    "        features[f\"issuelinks_{lt.lower()}_count\"] = count\n",
    "        features[f\"has_issuelinks_{lt.lower()}\"] = 1 if count > 0 else 0\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_comments(comments):\n",
    "    \"\"\"\n",
    "    Process the 'fields.comments' JSON array and extract summary features:\n",
    "      - Total number of comments.\n",
    "      - Average and maximum comment length.\n",
    "      - Number of unique authors.\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        \"comment_count\": 0,\n",
    "        \"avg_comment_length\": 0,\n",
    "        \"max_comment_length\": 0,\n",
    "        \"unique_authors_count\": 0\n",
    "    }\n",
    "    if not isinstance(comments, list) or len(comments) == 0:\n",
    "        return features\n",
    "    comment_bodies = [c.get('body', '') for c in comments if isinstance(c, dict)]\n",
    "    authors = [c.get('author', {}).get('name') for c in comments if isinstance(c, dict)]\n",
    "    features[\"comment_count\"] = len(comment_bodies)\n",
    "    lengths = [len(body) for body in comment_bodies]\n",
    "    if lengths:\n",
    "        features[\"avg_comment_length\"] = sum(lengths) / len(lengths)\n",
    "        features[\"max_comment_length\"] = max(lengths)\n",
    "    unique_authors = {a for a in authors if a is not None}\n",
    "    features[\"unique_authors_count\"] = len(unique_authors)\n",
    "    return features\n",
    "\n",
    "\n",
    "def process_description_field(descriptions):\n",
    "    \"\"\"\n",
    "    Process the 'fields.description' field by generating dense embeddings\n",
    "    using a pre-trained Sentence Transformer. The resulting embedding vector\n",
    "    is expanded into multiple columns (one per dimension).\n",
    "    \n",
    "    NOTE: In order for this code to work, define desc_model outside this function, e.g.:\n",
    "        desc_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \"\"\"\n",
    "    descriptions = descriptions.fillna(\"\").astype(str)\n",
    "    embeddings = descriptions.apply(lambda x: desc_model.encode(x, show_progress_bar=False))\n",
    "    emb_array = np.vstack(embeddings.values)\n",
    "    emb_df = pd.DataFrame(\n",
    "        emb_array, \n",
    "        index=descriptions.index,\n",
    "        columns=[f\"desc_emb_{i}\" for i in range(emb_array.shape[1])]\n",
    "    )\n",
    "    return emb_df\n",
    "\n",
    "\n",
    "def process_repo(jira_name, db, sample_ratio, batch_size=300, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Process a single Jira repository using streaming approach with CSV storage.\n",
    "    \n",
    "    Parameters:\n",
    "        jira_name (str): Name of the Jira repository\n",
    "        db: MongoDB database connection\n",
    "        sample_ratio (float): Used to influence project selection\n",
    "        batch_size (int): Size of batches for processing\n",
    "        target_cap (int): Target number of total issues to sample\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with sampled issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing repository: {jira_name} ...\")\n",
    "    \n",
    "    # 1) Get total count first (fast operation)\n",
    "    total_issues = db[jira_name].count_documents({})\n",
    "    if total_issues == 0:\n",
    "        print(f\"⚠️ No documents found for '{jira_name}', skipping.\")\n",
    "        return None\n",
    "    \n",
    "    # 2) Load the CSV-based filter at the start (so it applies in both branches)\n",
    "    project_ids_filter = get_project_name_from_csv('../OverallProjectEstimation/DataSets/data_export_1741699774916.csv')\n",
    "    print(f\"Loaded {len(project_ids_filter)} project IDs from CSV.===============================\")\n",
    "    base_filter = {}\n",
    "    if project_ids_filter:\n",
    "        base_filter = {\"fields.project.name\": {\"$in\": project_ids_filter}}\n",
    "        print(f\"Using project IDs filter: {project_ids_filter}\")\n",
    "    \n",
    "    # Define only the fields we actually need\n",
    "    needed_fields = {\n",
    "        # Essential identification fields\n",
    "        \"id\": 1,\n",
    "        \"key\": 1,\n",
    "        \"changelog\": 1,  # Needed for changelog histories\n",
    "        \n",
    "        # Issue metadata\n",
    "        \"fields.summary\": 1,\n",
    "        \"fields.description\": 1,\n",
    "        \"fields.created\": 1,\n",
    "        \"fields.updated\": 1,\n",
    "        \"fields.resolutiondate\": 1,\n",
    "        \n",
    "        # Classification fields\n",
    "        \"fields.issuetype.name\": 1,\n",
    "        \"fields.priority.name\": 1,\n",
    "        \"fields.status.name\": 1,\n",
    "        \n",
    "        # People fields\n",
    "        \"fields.assignee.key\": 1,\n",
    "        \"fields.assignee.name\": 1,\n",
    "        \"fields.reporter.key\": 1,\n",
    "        \"fields.reporter.name\": 1,\n",
    "        \"fields.creator.key\": 1,\n",
    "        \"fields.creator.name\": 1,\n",
    "        \n",
    "        # Project context\n",
    "        \"fields.project.id\": 1,\n",
    "        \"fields.project.key\": 1,\n",
    "        \"fields.project.name\": 1,\n",
    "        \n",
    "        # Relationships\n",
    "        \"fields.issuelinks\": 1,\n",
    "        \"fields.customfield_10557\": 1,  # Sprint field\n",
    "        \n",
    "        # Components and labels\n",
    "        \"fields.components\": 1,\n",
    "        \"fields.labels\": 1,\n",
    "        \"fields.fixVersions\": 1,\n",
    "        \n",
    "        # Comments\n",
    "        \"fields.comments\": 1\n",
    "    }\n",
    "    \n",
    "    print(f\"Found {total_issues} total issues in '{jira_name}'. Analyzing projects...\")\n",
    "    \n",
    "    # --- 3) Get project distribution ---\n",
    "    # Note: We haven't used base_filter here in the aggregation; \n",
    "    # you can add a $match stage if you only want to see counts for the filtered set\n",
    "    project_counts = list(db[jira_name].aggregate([\n",
    "        {\n",
    "            \"$group\": {\n",
    "                \"_id\": \"$fields.project.id\",\n",
    "                \"count\": {\"$sum\": 1},\n",
    "                \"name\": {\"$first\": \"$fields.project.name\"}\n",
    "            }\n",
    "        }\n",
    "    ]))\n",
    "    \n",
    "    projects = [\n",
    "        {\"id\": p[\"_id\"], \"count\": p[\"count\"], \"name\": p.get(\"name\", \"Unknown\")}\n",
    "        for p in project_counts\n",
    "        if p[\"_id\"] is not None\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(projects)} projects in repository.\")\n",
    "    \n",
    "    # --- 4) Select projects based on size categories ---\n",
    "    adjusted_target = min(target_cap, int(total_issues * sample_ratio))\n",
    "    \n",
    "    small_projects = [p for p in projects if p[\"count\"] < adjusted_target * 0.2]\n",
    "    medium_projects = [p for p in projects if adjusted_target * 0.2 <= p[\"count\"] < adjusted_target * 0.6]\n",
    "    large_projects = [p for p in projects if p[\"count\"] >= adjusted_target * 0.6]\n",
    "    \n",
    "    print(\n",
    "        f\"Project distribution: \"\n",
    "        f\"{len(small_projects)} small, {len(medium_projects)} medium, {len(large_projects)} large\"\n",
    "    )\n",
    "    \n",
    "    # Initialize selection\n",
    "    selected_projects = []\n",
    "    total_selected_issues = 0\n",
    "    \n",
    "    # Helper function to find best fit project\n",
    "    def find_best_fit(project_list, remaining_capacity, prefer_larger=True):\n",
    "        if not project_list:\n",
    "            return None\n",
    "        sorted_projects = sorted(project_list, key=lambda p: p[\"count\"], reverse=prefer_larger)\n",
    "        for project in sorted_projects:\n",
    "            if project[\"count\"] <= remaining_capacity * 1.2:\n",
    "                return project\n",
    "        if not prefer_larger:\n",
    "            return sorted_projects[0]  # Return the smallest if we can't find one\n",
    "        return None\n",
    "    \n",
    "    # Try to select a diverse mix of projects\n",
    "    \n",
    "    # First try to get a large project\n",
    "    if large_projects:\n",
    "        best_large = find_best_fit(large_projects, adjusted_target, prefer_larger=True)\n",
    "        if best_large and best_large[\"count\"] <= adjusted_target * 1.3:\n",
    "            selected_projects.append(best_large)\n",
    "            total_selected_issues += best_large[\"count\"]\n",
    "            large_projects.remove(best_large)\n",
    "            print(f\"Selected large project: {best_large['name']} with {best_large['count']} issues\")\n",
    "    \n",
    "    # Then add some medium projects\n",
    "    while medium_projects and total_selected_issues < adjusted_target * 0.7:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_medium = find_best_fit(medium_projects, remaining, prefer_larger=False)\n",
    "        if best_medium:\n",
    "            selected_projects.append(best_medium)\n",
    "            total_selected_issues += best_medium[\"count\"]\n",
    "            medium_projects.remove(best_medium)\n",
    "            print(f\"Selected medium project: {best_medium['name']} with {best_medium['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Finally fill in with small projects\n",
    "    while small_projects and total_selected_issues < adjusted_target * 0.9:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        best_small = find_best_fit(small_projects, remaining, prefer_larger=True)\n",
    "        if best_small:\n",
    "            selected_projects.append(best_small)\n",
    "            total_selected_issues += best_small[\"count\"]\n",
    "            small_projects.remove(best_small)\n",
    "            print(f\"Selected small project: {best_small['name']} with {best_small['count']} issues\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # If we're still significantly under target, add one more project that best fits\n",
    "    if total_selected_issues < adjusted_target * 0.8:\n",
    "        remaining = adjusted_target - total_selected_issues\n",
    "        remaining_projects = small_projects + medium_projects + large_projects\n",
    "        best_remaining = find_best_fit(remaining_projects, remaining * 1.3, prefer_larger=False)\n",
    "        if best_remaining:\n",
    "            selected_projects.append(best_remaining)\n",
    "            total_selected_issues += best_remaining[\"count\"]\n",
    "            print(f\"Added additional project: {best_remaining['name']} with {best_remaining['count']} issues\")\n",
    "    \n",
    "    # --- 5) Create temporary directory for streaming storage ---\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    batch_files = []\n",
    "    total_processed = 0\n",
    "    \n",
    "    try:\n",
    "        # --- 6) Process issues using streaming approach ---\n",
    "        if not selected_projects:\n",
    "            # Fall back to random sampling\n",
    "            print(\"No projects could be selected. Using default sampling method.\")\n",
    "            desired_sample_size = min(500, total_issues)\n",
    "            sample_indices = sorted(random.sample(range(total_issues), desired_sample_size))\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                # Use base_filter in random sampling\n",
    "                cursor = db[jira_name].find(base_filter, {**needed_fields, \"_id\": 0}).skip(idx).limit(1)\n",
    "                batch = list(cursor)\n",
    "                \n",
    "                # Process batch immediately\n",
    "                if batch:\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_random_{idx}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    total_processed += len(batch)\n",
    "                    \n",
    "                    # Free memory\n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "        else:\n",
    "            # Process each selected project\n",
    "            project_ids = [p[\"id\"] for p in selected_projects]\n",
    "            \n",
    "            for project_id in project_ids:\n",
    "                project_issue_count = next((p[\"count\"] for p in selected_projects if p[\"id\"] == project_id), 0)\n",
    "                print(f\"Fetching {project_issue_count} issues for project ID {project_id}...\")\n",
    "                \n",
    "                batch_count = 0\n",
    "                fetched_count = 0\n",
    "                \n",
    "                while fetched_count < project_issue_count:\n",
    "                    # Combine the per-project filter with CSV-based filter\n",
    "                    project_filter = {\"fields.project.id\": project_id}\n",
    "                    if base_filter:\n",
    "                        project_filter = {\"$and\": [project_filter, base_filter]}\n",
    "                    \n",
    "                    cursor = db[jira_name].find(\n",
    "                        project_filter,\n",
    "                        {**needed_fields, \"_id\": 0}\n",
    "                    ).skip(batch_count * batch_size).limit(batch_size)\n",
    "                    \n",
    "                    batch = list(cursor)\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    \n",
    "                    batch_size_actual = len(batch)\n",
    "                    \n",
    "                    # Process batch\n",
    "                    batch_df = pd.json_normalize(batch, sep='.')\n",
    "                    batch_df = fix_data_types(batch_df)\n",
    "                    batch_df['repository'] = jira_name\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    batch_file = os.path.join(temp_dir, f\"{jira_name}_{project_id}_{batch_count}.csv\")\n",
    "                    batch_df.to_csv(batch_file, index=False)\n",
    "                    batch_files.append(batch_file)\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                    fetched_count += batch_size_actual\n",
    "                    total_processed += batch_size_actual\n",
    "                    \n",
    "                    del batch\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"  - Processed batch {batch_count} ({batch_size_actual} issues, {fetched_count}/{project_issue_count})\")\n",
    "        \n",
    "        print(f\"Final sample for '{jira_name}': {total_processed} issues from {len(selected_projects)} projects\")\n",
    "        \n",
    "        # --- 7) Combine all saved batches ---\n",
    "        if not batch_files:\n",
    "            print(\"No data collected.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Combining {len(batch_files)} processed batches...\")\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                chunk = pd.read_csv(file, low_memory=False)\n",
    "                dfs.append(chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading batch file {file}: {e}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"No valid data read from batches.\")\n",
    "            return None\n",
    "        \n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"Combined DataFrame has {len(final_df)} rows and {len(final_df.columns)} columns.\")\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                if os.path.exists(file):\n",
    "                    os.remove(file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error removing temp file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(temp_dir):\n",
    "                os.rmdir(temp_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing temp directory {temp_dir}: {e}\")\n",
    "\n",
    "\n",
    "def drop_high_missing_columns(df, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Drop columns from the DataFrame where the fraction of missing values exceeds the threshold.\n",
    "    \"\"\"\n",
    "    return df.loc[:, df.isnull().mean() <= threshold]\n",
    "\n",
    "\n",
    "def impute_missing_values(df, numeric_strategy='median', categorical_strategy='constant', fill_value='Missing'):\n",
    "    \"\"\"\n",
    "    Impute missing values using scikit-learn's SimpleImputer.\n",
    "      - Numeric columns: impute with the specified strategy (default: median).\n",
    "      - Categorical columns: impute with a constant value (default: \\\"Missing\\\").\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        num_imputer = SimpleImputer(strategy=numeric_strategy)\n",
    "        df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        cat_imputer = SimpleImputer(strategy=categorical_strategy, fill_value=fill_value)\n",
    "        df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_all_fields_in_dtale(selected_jiras=None, sample_ratio=0.2, missing_threshold=0.3,\n",
    "                                zero_threshold=0.8, open_dtale=True, target_cap=900000):\n",
    "    \"\"\"\n",
    "    Connect to the MongoDB 'JiraRepos' database, sample issues from selected repositories,\n",
    "    process and flatten changelog histories (summarizing them without from/to transitions),\n",
    "    process JSON array fields (issuelinks, comments) into engineered features,\n",
    "    process the 'fields.description' field into dense embedding features,\n",
    "    drop columns with excessive missing data, impute missing values,\n",
    "    and drop changelog summary columns dominated by zeros.\n",
    "    \n",
    "    If open_dtale is True, launch a D-Tale session for interactive visualization;\n",
    "    otherwise, simply return the final DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        selected_jiras (list): List of Jira repositories to process\n",
    "        sample_ratio (float): Ratio of data to sample (1.0 = 100%)\n",
    "        missing_threshold (float): Threshold for dropping columns with missing values\n",
    "        zero_threshold (float): Threshold for dropping columns dominated by zeros\n",
    "        open_dtale (bool): Whether to launch a D-Tale session\n",
    "        target_cap (int): Maximum number of issues to retrieve per repository\n",
    "    \"\"\"\n",
    "    # Connect to MongoDB\n",
    "    client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "    db = client[\"JiraRepos\"]\n",
    "\n",
    "    # Load Jira data sources configuration\n",
    "    with open(\"../0. DataDefinition/jira_data_sources.json\") as f:\n",
    "        jira_data_sources = json.load(f)\n",
    "\n",
    "    all_jiras = list(jira_data_sources.keys())\n",
    "    \n",
    "    # If selected_jiras is provided and is not empty, check if the user wants 'all'.\n",
    "    # If 'all' is present (case-insensitive), do not filter. Otherwise, filter.\n",
    "    if selected_jiras and len(selected_jiras) > 0:\n",
    "        all_jiras = [j for j in all_jiras if j in selected_jiras]\n",
    "        if not all_jiras:\n",
    "            print(f\"⚠️ No valid Jira repositories found for {selected_jiras}.\")\n",
    "            return\n",
    "\n",
    "    # Process each repository in parallel\n",
    "    merged_dfs = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_repo, jira_name, db, sample_ratio, 300, target_cap): jira_name\n",
    "            for jira_name in all_jiras\n",
    "        }\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                merged_dfs.append(result)\n",
    "\n",
    "    if merged_dfs:\n",
    "        final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to display.\")\n",
    "        return\n",
    "\n",
    "    # Drop columns with high missing ratios\n",
    "    final_df = drop_high_missing_columns(final_df, threshold=missing_threshold)\n",
    "\n",
    "    # Process JSON array field for issuelinks\n",
    "    if \"fields.issuelinks\" in final_df.columns:\n",
    "        issuelinks_features = final_df[\"fields.issuelinks\"].apply(process_issue_links)\n",
    "        issuelinks_df = pd.json_normalize(issuelinks_features)\n",
    "        final_df = pd.concat([final_df.drop(columns=[\"fields.issuelinks\"]), issuelinks_df], axis=1)\n",
    "\n",
    "    # Example: If you want to generate embeddings for description, uncomment below:\n",
    "    # if \"fields.description\" in final_df.columns:\n",
    "    #     desc_embeddings = process_description_field(final_df[\"fields.description\"])\n",
    "    #     final_df = pd.concat([final_df.drop(columns=[\"fields.description\"]), desc_embeddings], axis=1)\n",
    "\n",
    "    # Impute missing values\n",
    "    final_df = impute_missing_values(final_df)\n",
    "\n",
    "    date_cols = [\"fields.created\", \"fields.updated\", \"fields.resolutiondate\"]\n",
    "    final_df = convert_date_columns_dateparser(final_df, date_cols)\n",
    "    final_df = drop_invalid_dates(final_df, date_cols)\n",
    "    final_df['fields.created'] = pd.to_datetime(final_df['fields.created'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.updated'] = pd.to_datetime(final_df['fields.updated'], errors=\"coerce\", utc=True)\n",
    "    final_df['fields.resolutiondate'] = pd.to_datetime(final_df['fields.resolutiondate'], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if open_dtale:\n",
    "        print(\"Data processed. Launching D-Tale session...\")\n",
    "        d = dtale.show(final_df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "        d.open_browser()\n",
    "        print(\"✅ D-Tale session launched successfully.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def export_clean_df():\n",
    "    \"\"\"\n",
    "    Run the full pipeline and return the final DataFrame with all engineered features.\n",
    "    This version uses sample_ratio=1.0 to get 100% of the dataset and no upper limit on issues.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The final processed DataFrame with all data.\n",
    "    \"\"\"\n",
    "    final_df = explore_all_fields_in_dtale(\n",
    "        selected_jiras=ALL_JIRAS,\n",
    "        sample_ratio=1.0,   # 100% of data\n",
    "        missing_threshold=0.3,\n",
    "        zero_threshold=0.8,\n",
    "        open_dtale=True,\n",
    "        target_cap=float('inf')  # No upper limit on the number of issues\n",
    "    )\n",
    "    # final_df is your combined DataFrame with all records\n",
    "    # grouped = final_df.groupby(\"project_name\")\n",
    "    # final_df is your combined DataFrame with all records\n",
    "    save_csvs_by_project_id(final_df, output_folder=\"projects_output\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# For testing purposes, you can run export_clean_df() if executing this module directly.\n",
    "if __name__ == \"__main__\":\n",
    "    df_for_training = export_clean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6762e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dtale\n",
    "\n",
    "# Replace this path with the location of your CSV file\n",
    "csv_file_path = \"./jira_extracted_data/RedHat_20250317.csv\"\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Start a D-Tale session and open it in the browser\n",
    "d = dtale.show(df, ignore_duplicate=True, allow_cell_edits=False)\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f76db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_mongo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
